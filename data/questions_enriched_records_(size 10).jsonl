{"document_index":13,"context":"GPT from Scratch with MLX\nDefine and train GPT-2 on your MacBook\nPranav Jadhav\nTowards Data Science\nPranav Jadhav\n\n\u00b7\nFollow\n\nPublished in\nTowards Data Science\n\n31 min read\n\u00b7\n2 days ago\n\nTable of Contents\nPreparing the data\nCoding GPT-2\nInput Embeddings\nPositional Embeddings\nSelf Attention\nKeys, Queries, and Values\nMulti-Head Attention\nMLP\nBlock\nLayernorms and Skip Connections\nForward Pass\nSampling\nInitialization\nTraining Loop\nReferences\nPreparing the data\nInstall mlx and run the following imports.\n\nimport mlx.core as mx\nimport mlx.nn as nn\nimport mlx.optimizers as optim\nimport mlx.utils as utils\nimport numpy as np\nimport math\nThe first step to training an LLM is collecting a large corpus of text data and then tokenizing it. Tokenization is the process of mapping text to integers, which can be fed into the LLM. Our training corpus for this model will be the works of Shakespeare concatenated into one file. This is roughly 1 million characters and looks like this:\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n...\nFirst, we read the file as a single long string into the text variable. Then we use the set() function to get all the unique characters in the text which will be our vocabulary. By printing vocab you can see all the characters in our vocabulary as one string, and we have a total of 65 characters which till be our tokens.\n\n# Creating the vocabulary\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nvocab = sorted(list(set(text)))\nvocab_size = len(vocab)\n\nprint(''.join(vocab))\n# !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nprint(vocab_size)\n# 65\nProduction models will use tokenization algorithms like byte-pair encoding to generate a larger vocabulary of sub-word chunks. Since our focus today is on the architecture, we will continue with character-level tokenization. Next, we will map our vocabulary to integers known as token IDs. Then we can encode our text into tokens and decode them back to a string.\n\n# Create mapping from vocab to integers\nitos = {i:c for i,c in enumerate(vocab)} # int to string\nstoi = {c:i for i,c in enumerate(vocab)} # string to int\nencode = lambda x: [stoi[c] for c in x]\ndecode = lambda x: ''.join([itos[i] for i in x])\n\nprint(encode(\"hello world\"))\n# [46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\nprint(decode(encode(\"hello world\")))\n# hello world\nWe use theenumerate() function to iterate over all characters and their index in the vocabulary and create a dictionary itos which maps integers to characters and stoi which maps strings to integers. Then we use these mappings to create our encode and decode functions. Now we can encode the entire text and split training and validation data.\n\ndata = encode(text)\nsplit = int(0.9 * len(data))\ntrain_data = data[:split]\nval_data = data[split:]\nCurrently, our training data is just a very long string of tokens. However, we are trying to train our model to predict the next token some given previous tokens. Therefore our dataset should be comprised of examples where the input is some string of tokens and the label is the correct next token. We need to define a model parameter called context length which is the maximum number of tokens used to predict the next token. Our training examples will be the length of our context length.\n\nLet\u2019s look at the first ctx_len+1 tokens.\n\nctx_len = 8\nprint(train_data[:ctx_len + 1])\n# [18, 47, 56, 57, 58,  1, 15, 47, 58]\n# x: [18, 47, 56, 57, 58,  1, 15, 47] | y: 58\nThis is one training example where the input is \u201c18, 47, 56, 57, 58, 1, 15, 47\u201d and the desired output is \u201c58\u201d. This is 8 tokens of context. However, we also want to train the model to predict the next token given only 7, 6, 5 \u2026 0 tokens as context which is needed during generation. Therefore we also consider the 8 sub examples packed into this example:\n\nctx_len = 8\nprint(train_data[:ctx_len + 1])\n# [18, 47, 56, 57, 58,  1, 15, 47, 58]\n# 8 sub examples\n# [18] --> 47\n# [18, 47] --> 56\n# [18, 47, 56] --> 57\n# [18, 47, 56, 57] --> 58\n# [18, 47, 56, 57, 58] --> 1\n# [18, 47, 56, 57, 58, 1] --> 15\n# [18, 47, 56, 57, 58, 1, 15] --> 47\n# [18, 47, 56, 57, 58, 1, 15, 47] --> 58\nNotice that the labels are simply the inputs shifted left.\n\nprint(\"inputs: \", train_data[:ctx_len])\nprint(\"labels: \", train_data[1:ctx_len+1]) # labels = inputs indexed 1 higher\n# inputs: [18, 47, 56, 57, 58,  1, 15, 47]\n# labels: [47, 56, 57, 58,  1, 15, 47, 58]\nAt index 0 the input is 18 and the label is 47. At index 1 the input is everything before and including index 1 which is [18, 47] and the label is 56, etc. Now that we understand that the labels are simply the input sequence indexed one higher we can build our datasets.\n\n# Creating training and validation datasets\nctx_len = 8\nX_train = mx.array([train_data[i:i+ctx_len] for i in range(0, len(train_data) - ctx_len, ctx_len)])\ny_train = mx.array([train_data[i+1:i+ctx_len+1] for i in range(0, len(train_data) - ctx_len, ctx_len)])\nX_val = mx.array([val_data[i:i+ctx_len] for i in range(0, len(val_data) - ctx_len, ctx_len)])\ny_val = mx.array([val_data[i+1:i+ctx_len+1] for i in range(0, len(val_data) - ctx_len, ctx_len)])\nWe loop through the data and take chunks of size ctx_len as the inputs (X) and then take the same chunks but at 1 higher index as the labels (y). Then we take these Python lists and create mlx array objects from them. The model internals will be written with mlx so we want our inputs to be mlx arrays.\n\nOne more thing. During training we don\u2019t want to feed the model one example at a time, we want to feed it multiple examples in parallel for efficiency. This group of examples is called our batch, and the number of examples in a group is our batch size. Thus we define a function to generate batches for training.\n\ndef get_batches(X, y, b_size, shuffle=True):\n    if shuffle:\n        ix = np.arange(X.shape[0])\n        np.random.shuffle(ix)\n        ix = mx.array(ix)\n        X = X[ix]\n        y = y[ix]\n    for i in range(0, X.shape[0], b_size):\n        input = X[i:i+b_size]\n        label = y[i:i+b_size]\n        yield input, label\nIf shuffle=True, we shuffle the data by indexing it with a randomly shuffled index. Then we loop through our dataset and return batch-size chunks from input and label datasets. These chunks are known as mini-batches and are just stacked examples that we process in parallel. These mini-batches will be our input to the model during training.\n\nHere\u2019s an example of a minibatch of 4 examples with context length 8.\n\n\nA single minibatch (image by author)\nThis minibatch packs 32 next-token prediction problems. The model will predict the next token for each token in the input and the labels will be used to calculate the loss. Notice that the labels contain the next token for each index of the inputs.\n\nYou\u2019ll want to keep this picture in your mind because the shapes of these tensors will get hairy. For now, just remember that we will input a tensor of shape (batch_size, ctx_len) to the model.\n\nCoding GPT-2\nLet\u2019s look at the GPT-2 architecture to get an overview of what we are trying to implement.\n\n\nGPT-2 Architecture (image by author)\nDon\u2019t worry if this looks confusing. We will implement it step by step from bottom to top. Let\u2019s start by implementing the input embeddings.\n\nInput Embeddings\nThe purpose of the input embedding layer is to map token IDs to vectors. Each token will be mapped to a vector which will be its representation as it is forwarded through the model. The vectors for each token will accumulate and exchange information as they pass through the model and eventually be used to predict the next token. These vectors are called embeddings.\n\nThe simplest way to map token IDs to vectors is through a lookup table. We create a matrix of size (vocab_size, n_emb) where each row is the embedding vector for the corresponding token. This matrix is known as the embedding weights.\n\n\nEmbedding Layer (image by author)\nThe diagram shows an example embedding layer of size (65, 6). This means there are 65 tokens in the vocabulary and each one will be represented by a length 6 embedding vector. The inputted sequence will be used to index the embedding weights to get the vector corresponding to each token. Remember the minibatches we input into the model? Originally the minibatch is size (batch_size, ctx_len). After passing through the embedding layer it is size (batch_size, ctx_len, n_emb). Instead of each token being a single integer, each token is now a vector of length n_emb.\n\nLet\u2019s define the embedding layer in code now.\n\nn_emb = 6 # You can add these hyperparams at the top of your file\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb)\nWe will define a class to organize our implementation. We subclass nn.Module to take advantage of mlx\u2019s features. Then in the init function, we call the superclass constructor and initialize our token embedding layer called wte .\n\nPositional Embeddings\nNext up is the positional embeddings. The purpose of positional embeddings is to encode information about the position of each token in the sequence. This can be added to our input embeddings to get a complete representation of each token that contains information about the token\u2019s position in the sequence.\n\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb) # token embeddings\n        self.wpe = nn.Embedding(ctx_len, n_emb) # position embeddings\nThe position embeddings work the same as token embeddings, except instead of having a row for each token we have a row for each possible position index. This means our embedding weights will be of shape (ctx_len, n_emb). Now we implement the __call__ function in our GPT class. This function will contain the forward pass of the model.\n\n# Tensor shapes commented\ndef __call__(self, x):\n    B, T = x.shape # (B = batch_size, T = ctx_len)\n    tok_emb = self.wte(x) # (B, T, n_emb)\n    pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)\n    x = tok_emb + pos_emb # (B, T, n_emb)\nFirst, we break out the dimensions of our input into variables B and T for easy handling. In sequence modeling contexts B and T are usually used as shorthand for \u201cbatch\u201d and \u201ctime\u201d dimensions. In this case, the \u201ctime\u201d dimension of our sequence is the context length.\n\nNext, we calculate token and position embeddings. Notice that for the position embeddings, our input is mx.arange(T) . This will output an array of consecutive integers from 0 to T-1 which is exactly what we want because those are the positions we want to embed. After passing that through the embedding layer we will have a tensor of shape (T, n_emb) because the embedding layer plucks out the n_emb length vector for each of the T positions. Note that even though pos_emb is not the same shape as tok_emb we can add the two because mlx will broadcast, or replicate pos_emb across the batch dimension to allow elementwise addition. Finally, we perform the addition to get the new representations of the tokens with positional information.\n\nSelf-Attention\nSo far the representation vectors for each token have been calculated independently. They have not had the opportunity to exchange any information. This is intuitively bad in language modeling because the meaning and usage of words depend on the surrounding context. Self-attention is how we incorporate information from previous tokens into a given token.\n\nFirst, let\u2019s consider a naive approach. What if we simply represented each token as the average of its representation vector and the vectors of all the tokens before it? This achieves our goal of packing information from previous tokens into the representation for a given token. Here\u2019s what it would look like.\n\n\nimage by author\nBut self-attention doesn\u2019t involve writing a for-loop. The key insight is we can achieve this previous token averaging with matrix multiplication!\n\n\nimage by author\nBy multiplying our input sequence on the left by a special matrix we get the desired result. This matrix is known as the attention weights. Notice that each row of the attention weight matrix specificies \u201chow much\u201d of each other token goes into the representation for any given token. For example in row two, we have [0.5, 0.5, 0, 0]. This means that row two of the result will be 0.5*token1 + 0.5*token2 + 0*token3 + 0*token4 , or the average of token1 and token2. Note that the attention weights are a lower-triangular matrix (zeros in upper right entries). This ensures that future tokens will not be included in the representation of a given token. This ensures that tokens can only communicate with the previous tokens because during generation the model will only have access to previous tokens.\n\nLet\u2019s look at how we can construct the attention weight matrix.\n\n\nimage by author\nNotice that if we create an array of zeros with -inf in the upper right entries and then perform row-wise softmax we get the desired attention weights. A good exercise is to step through the softmax calculation for a row to see how this works. The takeaway is that we can take some array of size (ctx_len, ctx_len) and softmax each row to get attention weights that sum to one.\n\nNow we can leave the realm of naive self-attention. Instead of simply averaging previous tokens, we use arbitrary weighted sums over previous tokens. Notice what happens when we do row-wise softmax of an arbitrary matrix.\n\n\nimage by author\nWe still get weights that sum to one on each row. During training, we can learn the numbers in the matrix on the left which will specify how much each token goes into the representation for another token. This is how tokens pay \u201cattention\u201d to each other. But we still haven\u2019t understood where this matrix on the left came from. These pre-softmax attention weights are calculated from the tokens themselves, but indirectly through three linear projections.\n\nKeys, Queries, and Values\n\nimage by author\nEach token in our sequence emits 3 new vectors. These vectors are called keys, queries, and values. We use the dot product of the query vector of one token and the key vector of another token to quantify the \u201caffinity\u201d those two tokens have. We want to calculate the pairwise affinities of each token with every other token, therefore we multiply the query vector (4x3) with the key vector transposed (3x4) to get the raw attention weights (4x4). Due to the way matrix multiplication works the (i,j) entry in the raw attention weights will be the query of token i dot the key of token j or the \u201caffinity\u201d between the two. Thus we have calculated interactions between every token. However, we don\u2019t want past tokens interacting with future tokens so we apply a mask of -inf to the upper right entries to ensure they will zero out after softmax. Then we perform row-wise softmax to get the final attention weights. Instead of multiplying these weights directly with the input, we multiply them with the value projection. This results in the new representations.\n\nNow that we understand attention conceptually, let\u2019s implement it.\n\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n\nWe start by defining the key, query, and value projection layers. Note that instead of going from n_emb to n_emb, we project from n_emb to head_size. This doesn\u2019t change anything, it just means the new representations calculated by attention will be dimension head_size.\n\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n    def __call__(self, x): # shapes commented\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\nThe forward pass begins by calculating the key, query, and value projections. We also break out the input shape into the variables B, T, and C for future convenience.\n\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) \/ math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\nNext, we calculate the attention weights. We only want to transpose the last two dimensions of the key tensor, because the batch dimension is just there so we can forward multiple training examples in parallel. The mlx transpose function expects the new order of the dimensions as input, so we pass it [0, 2, 1] to transpose the last two dimensions. One more thing: we scale the attention weights by the inverse square root of head_size. This is known as scaled attention and the purpose is to ensure that when Q and K are unit variance, attn_weights will be unit variance. If the variance of attn_weights is high, then the softmax will map these small and large values to 0 or 1which results in less complex representations.\n\nThe next step is to apply the mask to ensure we are doing causal language modeling i.e. ensuring tokens cannot attend to future tokens.\n\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) \/ math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\nWe create the mask with a clever broadcasting trick. Let\u2019s say our ctx_len=4 like in the diagrams above. First, we use mx.arange(4) to set the indices variable to [0, 1, 2, 3].\n\n\nimage by author\nThen we can index like so indices[:, None] to generate a column vector with the values of indices. Similarly, we can get a row vector using indices[None]. Then when we do the < comparison, mlx broadcasts the vectors because they have mismatching shapes so they can\u2019t be compared elementwise. Broadcasting means mlx will replicate the vectors along the lacking dimension. This results in an elementwise comparison of two (4, 4) matrices which makes sense. Side note: I recommend familiarizing yourself with the details of broadcasting by reading this, it comes up all the time when dealing with tensors.\n\nAfter the elementwise comparison, we are left with the following tensor:\n\n[[False,  True,  True,  True],\n [False, False,  True,  True],\n [False, False, False,  True],\n [False, False, False, False]]\nMultiplying this tensor by -1e9, we get:\n\n[[-0e+00, -1e+09, -1e+09, -1e+09],\n [-0e+00, -0e+00, -1e+09, -1e+09],\n [-0e+00, -0e+00, -0e+00, -1e+09],\n [-0e+00, -0e+00, -0e+00, -0e+00]]\nNow we have an additive mask. We can add this matrix to our attention weights to make all the upper right entries very large negative numbers. This will cause them to be zeroed out after the softmax operation. Also, note that we add \u201c_\u201d as a prefix to the attribute name _causal_mask which marks it as a private variable. This signals to mlx that it is not a parameter and should not be updated during training.\n\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) \/ math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n        attn_weights = attn_weights + self._causal_mask\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        o = (attn_weights @ V) # (B, T, head_size)\nNow we can softmax row-wise to get the final attention weights and multiply these weights by the values to get our output. Note we pass axis=-1 to softmax which specifies that we want to softmax across the last dimension which are the rows.\n\nThe final step is output linear projection and dropout.\n\ndropout = 0.1 # add this with hyperparams at top of file\nclass Attention(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.head_size = head_size\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # output projection\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        attn_weights = (Q @ K.transpose([0, 2, 1])) \/ math.sqrt(self.head_size)\n        # attn_weights.shape = (B, T, T)\n        attn_weights = attn_weights + self._causal_mask\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        o = (attn_weights @ V) # (B, T, head_size)\n        o = self.c_proj(self.resid_dropout(o))\n        return o\nWe add two new layers, c_proj and resid_dropout which are the output projection and residual dropout. The output projection is to return the vectors to their original dimension n_emb. The dropout is added for regularization and training stability which is important as we start layering the transformer blocks to get a deep network. And that\u2019s it for implementing one attention head!\n\nMulti-Head Attention\nInstead of having just one attention head LLMs often use multiple attention heads in parallel and concatenate their outputs to create the final representation. For example, let\u2019s say we had one attention head with head_size=64 so the vector it produced for each token was 64 dimensional. We could achieve the same thing with 4 parallel attention heads each with head_size=16 by concatenating their outputs to produce a 16x4 = 64 dimensional output. Multi-head attention allows the model to learn more complex representations because each head learns different projections and attention weights.\n\nn_heads = 4\nclass MultiHeadAttention(nn.Module): # naive implementation\n    def __init__(self):\n        super().__init__()\n        self.heads = [Attention(head_size \/\/ n_heads) for _ in range(n_heads)]\n    def __call__(self, x):\n        return mx.concatenate([head(x) for head in self.heads], axis=-1)\nThe straightforward implementation is to create a list of n_heads attention heads where each one has size equal to our final head size divided by n_heads. Then we concatenate the output of each head over the last axis. However, this implementation is inefficient and does not take advantage of the speed of tensors. Let\u2019s implement multi-head attention with the power of tensors.\n\nhead_size = 64 # put at top of file\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # output projection\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\nWe start with our single-head attention implementation. The __init__() function has not changed. The forward pass begins as normal with the creation of the key, query, and value projections.\n\nhead_size = 64 # put at top of file\nn_heads = 8 # put at top of file\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # output projection\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        mha_shape = (B, T, n_heads, head_size\/\/n_heads)\n        K = mx.as_strided(K, (mha_shape)) # (B, T, n_heads, head_size\/\/n_heads)\n        Q = mx.as_strided(Q, (mha_shape)) # (B, T, n_heads, head_size\/\/n_heads)\n        V = mx.as_strided(V, (mha_shape)) # (B, T, n_heads, head_size\/\/n_heads)\nThe next thing we need to do is introduce a new dimension for the number of heads n_heads . In the naive implementation, we had separate attention objects each with their own key, query, and value tensors but now we have them all in one tensor, therefore we need a dimension for the heads. We define the new shape we want in mha_shape . Then we use mx.as_strided() to reshape each tensor to have the head dimension. This function is equivalent to view from pytorch and tells mlx to treat this array as a different shape. But we still have a problem. Notice that we if try to multiply Q @ K_t (where K_t is K transposed over it\u2019s last 2 dims) to compute attention weights as we did before, we will be multiplying the following shapes:\n\n(B, T, n_heads, head_size\/\/n_heads) @ (B, T, head_size\/\/n_heads, n_heads)\nResult shape: (B, T, n_heads, n_heads)\nThis would result in a tensor of shape (B, T, n_heads, n_heads) which is incorrect. With one head our attention weights were shape (B, T, T) which makes sense because it gives us the interaction between each pair of tokens. So now our shape should be the same but with a heads dimension: (B, n_heads, T, T) . We achieve this by transposing the dimensions of keys, queries, and values after we reshape them to make n_heads dimension 1 instead of 2.\n\nhead_size = 64 # put at top of file\nn_heads = 8 # put at top of file\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # output projection\n        self.attn_dropout = nn.Dropout(dropout)\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        mha_shape = (B, T, n_heads, head_size\/\/n_heads)\n        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size\/\/n_heads)\n        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size\/\/n_heads)\n        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size\/\/n_heads)\n        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) \/ math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)\n        attn_weights = attn_weights + self._causal_mask[:T, :T]\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        attn_weights = self.attn_dropout(attn_weights)\n        o = (attn_weights @ V) # (B, n_heads, T, head_size\/\/n_heads)\n\nNow we can calculate the correction attention weights. Notice that we scale the attention weights by the size of an individual attention head rather than head_size which would be the size after concatenation. We also apply dropout to the attention weights.\n\nFinally, we perform the concatenation and apply the output projection and dropout.\n\nhead_size = 64 # put at top of file\nn_heads = 8 # put at top of file\nclass MultiHeadAttention(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n        indices = mx.arange(ctx_len)\n        mask = indices[:, None] < indices[None] # broadcasting trick\n        self._causal_mask = mask * -1e9\n        self.c_proj = nn.Linear(head_size, n_emb) # output projection\n        self.attn_dropout = nn.Dropout(dropout)\n        self.resid_dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\n        K = self.k_proj(x) # (B, T, head_size)\n        Q = self.q_proj(x) # (B, T, head_size)\n        V = self.v_proj(x) # (B, T, head_size)\n        mha_shape = (B, T, n_heads, head_size\/\/n_heads)\n        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size\/\/n_heads)\n        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size\/\/n_heads)\n        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size\/\/n_heads)\n        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) \/ math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)\n        attn_weights = attn_weights + self._causal_mask[:T, :T]\n        attn_weights = mx.softmax(attn_weights, axis=-1)\n        attn_weights = self.attn_dropout(attn_weights)\n        o = (attn_weights @ V) # (B, n_heads, T, head_size\/\/n_heads)\n        o = o.transpose([0, 2, 1, 3]).reshape((B, T, head_size)) # concat heads\n        o = self.c_proj(self.resid_dropout(o))\n        return o\nSince we have everything in one tensor, we can do some shape manipulation to do the concatenation. First, we move n_heads back to the second to last dimension with the transpose function. Then we reshape back to the original size to undo the splitting into heads we performed earlier. This is the same as concatenating the final vectors from each head. And that\u2019s it for multi-head attention! We\u2019ve gotten through the most intense part of our implementation.\n\nMLP\nThe next part of the architecture is the multilayer perception or MLP. This is a fancy way of saying 2 stacked linear layers. There\u2019s not much to be said here, it is a standard neural network.\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.c_fc = nn.Linear(n_emb, 4 * n_emb)\n        self.gelu = nn.GELU()\n        self.c_proj = nn.Linear(4 * n_emb, n_emb)\n        self.dropout = nn.Dropout(dropout)\n    def __call__(self, x):\n        x = self.gelu(self.c_fc(x))\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\nWe take the input and project it to a higher dimension with c_fc . Then we apply gelu nonlinearity and project it back down to the embedding dimension with c_proj . Finally, we apply dropout and return. The purpose of the MLP is to allow for some computation after the vectors have communicated during attention. We will stack these communication layers (attention) and computation layers (mlp) into a block.\n\nBlock\nA GPT block consists of attention followed by an MLP. These blocks will be repeated to make the architecture deep.\n\nclass Block(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mlp = MLP()\n        self.mha = MultiHeadAttention()\n    def __call__(self, x):\n        x = self.mha(x)\n        x = self.mlp(x)\n        return x\nNow, we need to add two more features to improve training stability. Let\u2019s take a look at the architecture diagram again.\n\nLayernorms and Skip Connections\n\nimage by author\nWe still need to implement the components highlighted in red. The arrows are skip connections. Instead of the input being transformed directly, the effect of the attention and MLP layers is additive. Their result is added to the input instead of directly replacing it. This is good for the training stability of deep networks since in the backward pass, the operands of an addition operation will receive the same gradient as their sum. Gradients can thus flow backwards freely which prevents issues like vanishing\/exploding gradients that plague deep networks. Layernorm also helps with training stability by ensuring activations are normally distributed. Here is the final implementation.\n\nclass Block(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mlp = MLP()\n        self.mha = MultiHeadAttention()\n        self.ln_1 = nn.LayerNorm(dims=n_emb)\n        self.ln_2 = nn.LayerNorm(dims=n_emb)\n    def __call__(self, x):\n        x = x + self.mha(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\nLayernorm is applied before multi-head attention and MLP. The skip connections are added with x = x + ... making the operations additive.\n\nForward Pass\nWith the Block defined, we can finish the full GPT-2 forward pass.\n\nn_layers = 3 # put at top of file\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb) # token embeddings\n        self.wpe = nn.Embedding(ctx_len, n_emb) # position embeddings\n        self.blocks = nn.Sequential(\n            *[Block() for _ in range(n_layers)],\n        ) # transformer blocks\n        self.ln_f = nn.LayerNorm(dims=n_emb) # final layernorm\n        self.lm_head = nn.Linear(n_emb, vocab_size) # output projection\n    # Tensor shapes commented\n    def __call__(self, x):\n        B, T = x.shape # (B = batch_size, T = ctx_len)\n        tok_emb = self.wte(x) # (B, T, n_emb)\n        pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)\n        x = tok_emb + pos_emb # (B, T, n_emb)\n        x = self.blocks(x) # (B, T, n_emb)\n        x = self.ln_f(x) # (B, T, b_emb)\n        logits = self.lm_head(x) # (B, T, vocab_size)\n        return logits\nWe create a container for the blocks using nn.Sequential which takes any input and passes it sequentially through the contained layers. Then we can apply all the blocks with self.blocks(x) . Finally, we apply a layer norm and then the lm_head. The lm_head or language modeling head is just a linear layer that maps from the embedding dimension to the vocab size. The model will output a vector containing some value for each word in our vocabulary, or the logits. We can softmax the logits to get a probability distribution over the vocabulary which we can sample from to get the next token. We will also use the logits to calculate the loss during training. There are just two more things we need to implement before we begin training.\n\nSampling\nWe need to write a generate function to sample from the model once training is complete. The idea is that we start with some sequence of our choice, then we predict the next token and append this to our sequence. Then we feed the new sequence in and predict the next token again. This continues until we decide to stop.\n\n# method of GPT class\ndef generate(self, max_new_tokens):\n  ctx = mx.zeros((1, 1), dtype=mx.int32)\nWe prompt the model with a single token, zero. Zero is the newline character so it is a natural place to start the generation since we just want to see how Shakespeare-like our model can get. Note that we initialize the shape to (1, 1) to simulate a single batch with a sequence length of one.\n\n# method of GPT class\ndef generate(self, max_new_tokens):\n  ctx = mx.zeros((1, 1), dtype=mx.int32)\n  for _ in range(max_new_tokens):\n    logits = self(ctx[:, -ctx_len:]) # pass in last ctx_len characters\n    logits = logits[:, -1, :] # get logits for the next token\n    next_tok = mx.random.categorical(logits, num_samples=1)\n    ctx = mx.concatenate((ctx, next_tok), axis=1)\nreturn ctx\nThen we get the logits for the next token by passing in the last ctx_len characters to the model. However, our model output is of shape (B, T, vocab_size) since it predicts the next token logits for each token in the input. We use all of that during training, but now we only want the logits for the last token because we can use this to sample a new token. Therefore we index the logits to get the last element in the first dimension which is the sequence dimension. Then we sample the next token using the mx.random.categorical() function which takes the logits and the number of samples we want as input. This function will softmax the logits to turn them into a probability distribution and then randomly sample a token according to the probabilities. Finally, we concatenate the new token to the context and repeat the process max_new_tokens number of times.\n\nInitialization\nThe last thing to do is handle weight initialization which is important for training dynamics.\n\n# method of GPT\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 \/ math.sqrt(2 * n_layers)))\nFirst, we define two different nn.init.normal functions. The first one is for initializing all linear and embedding layers. The second one is for initializing linear layers that are specifically residual projections i.e. the last linear layer inside multi-head attention and MLP. The reason for this special initialization is that it checks accumulation along the residual path as model depth increases according to the GPT-2 paper [2].\n\nIn mlx we can change the parameters of the model using the mx.update() function. Checking the docs, it expects a complete or partial dictionary of the new model parameters. We can see what this dictionary looks like by printing out self.parameters() inside the GPT class.\n\n{'wte': {'weight': array([[-0.025084, -0.0197523, -0.0341617, ..., -0.0979123, -0.0830218, -0.0784692],\n       [-0.00777913, -0.117002, -0.0310708, ..., 0.0128591, 0.122941, 0.000414443],\n       [0.0240044, -0.0859084, 0.0253116, ..., 0.108967, 0.0767123, 0.0221565],\n       ...,\n       [0.050729, -0.04578, 0.0685943, ..., -0.0496998, -0.00350879, -0.00631825],\n       [0.00518804, 0.0499818, 0.0330045, ..., 0.0300661, 0.0431054, 0.000958906],\n       [-0.0323007, 0.0132046, 0.0208218, ..., -0.0785159, 0.00436121, -0.00726994]], dtype=float32)}, 'wpe': {'weight': array([[0.000797923, -0.0396898, -0.029047, ..., -0.0132273, 0.00684483, -0.0067624],\n       [-0.0247021, -0.0274349, 0.0310587, ..., -0.100099, 0.0301566, -0.0178732],\n       [0.0929172, -0.0468649, 0.0101506, ..., -0.0341086, -0.0516283, 0.0447596],\n       ...,\n       [-0.0508172, 0.0892201, -0.00183612, ..., -0.00341944, 0.023437, 0.0296461],\n       [0.0105829, 0.0688093, 0.146744, ..., -0.0836337, 0.0206679, 0.0184166],\n       [-0.00578717, -0.0606196, -0.0917056, ..., -0.0641549, -0.0490424, 0.0998114]], dtype=float32)}, 'blocks': {'layers': [{'mlp': {'c_fc': {'weight': array([[0.0169199, 0.00264431, 0.0316978, ..., -0.0596867, -0.0153549, 0.0176386],\n       ...\nIt\u2019s a nested dictionary containing each model weight as an mx.array. So to initialize the parameters of our model we need to build up a dictionary like this with our new params and pass them to self.update() . We can achieve this as follows:\n\n# method of GPT\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 \/ math.sqrt(2 * n_layers)))\n    new_params = []\n    for name, module in self.named_modules():\n        if isinstance(module, nn.layers.linear.Linear):\n            new_params.append((name + '.weight', normal_init(module.weight)))\n        elif isinstance(module, nn.layers.embedding.Embedding):\n            new_params.append((name + '.weight', normal_init(module.weight)\nWe maintain a list of tuples called new_params which will contain tuples of (parameter_name, new_value). Next, we loop through each nn.Module object in our model with self.named_modules() which returns tuples of (name, module). If we print out the module names within the loop we see that they look like this:\n\nlm_head\nblocks\nblocks.layers.4\nblocks.layers.3\nblocks.layers.3.ln_2\nblocks.layers.3.ln_1\nblocks.layers.3.mha\nblocks.layers.3.mha.resid_dropout\nblocks.layers.3.mha.c_proj\nblocks.layers.3.mha.attn_dropout\nblocks.layers.3.mha.c_attn\n...\nblocks.layers.0.mlp.dropout\nblocks.layers.0.mlp.c_proj\nblocks.layers.0.mlp.gelu\nblocks.layers.0.mlp.c_fc\nwpe\nwte\nWe use the isinstance() function to find the linear and embedding layers and then add them to our list. For example, say we are looping and reach \u201cblocks.layers.0.mlp.c_fc\u201d which is the first linear layer in the MLP. This would trigger the first if statement, and the tuple (\"block.layers.0.mlp.c_fc.weight\", [<normally initialized weight here>]) would be added to our list. We have to add \u201c.weight\u201d to the name because we specifically want to initialize the weight in this way, not the bias. Now we need to handle the residual projection initialization.\n\n# method of GPT\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 \/ math.sqrt(2 * n_layers)))\n    new_params = []\n    for name, module in self.named_modules():\n        if isinstance(module, nn.layers.linear.Linear):\n            if 'c_proj' in name: # residual projection\n                new_params.append((name + '.weight', residual_init(module.weight)))\n            else:\n                new_params.append((name + '.weight', normal_init(module.weight)))\n        elif isinstance(module, nn.layers.embedding.Embedding):\n            new_params.append((name + '.weight', normal_init(module.weight)))\nAfter checking if the module is a linear layer, we check if \u201cc_proj\u201d is in the name because that\u2019s how we named the residual projections. Then we can apply the special initialization. Finally, we need to initialize the biases to be zero.\n\n# method of GPT\ndef _init_parameters(self):\n    normal_init = nn.init.normal(mean=0.0, std=0.02)\n    residual_init = nn.init.normal(mean=0.0, std=(0.02 \/ math.sqrt(2 * n_layers)))\n    new_params = []\n    for name, module in self.named_modules():\n        if isinstance(module, nn.layers.linear.Linear):\n            if 'c_proj' in name:\n                new_params.append((name + '.weight', residual_init(module.weight)))\n            else:\n                new_params.append((name + '.weight', normal_init(module.weight)))\n            if 'bias' in module:\n                new_params.append((name + '.bias', mx.zeros(module.bias.shape)))\n        elif isinstance(module, nn.layers.embedding.Embedding):\n            new_params.append((name + '.weight', normal_init(module.weight)))\n    self = self.update(utils.tree_unflatten(new_params))\nWe add another if statement under our linear branch to check if the nn.Module object has a bias attribute. If it does, we add it to the list initialized to zeros. Finally, we need to transform our list of tuples into a nested dictionary. Luckily mlx has some functions implemented for dealing with parameter dictionaries, and we can use util.tree_unflatten() to convert this list of tuples to a nested parameter dictionary. This is passed into the update method to initialize the parameters. Now we can call _init_parameters() in the constructor.\n\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.wte = nn.Embedding(vocab_size, n_emb) # token embeddings\n        self.wpe = nn.Embedding(ctx_len, n_emb) # position embeddings\n        self.blocks = nn.Sequential(\n            *[Block() for _ in range(n_layers)],\n        ) # transformer blocks\n        self.ln_f = nn.LayerNorm(dims=n_emb) # final layernorm\n        self.lm_head = nn.Linear(n_emb, vocab_size) # output projection\n        self._init_parameters() # <-- initialize params\n        # print total number of params on initialization\n        total_params = sum([p.size for n,p in utils.tree_flatten(self.parameters())])\n        print(f\"Total params: {(total_params \/ 1e6):.3f}M\")\n    # Tensor shapes commented\n    def __call__(self, x):\n        B, T = x.shape # (B = batch_size, T = ctx_len)\n        tok_emb = self.wte(x) # (B, T, n_emb)\n        pos_emb = self.wpe(mx.arange(T)) # (T, n_emb)\n        x = tok_emb + pos_emb # (B, T, n_emb)\n        x = self.blocks(x) # (B, T, n_emb)\n        x = self.ln_f(x) # (B, T, b_emb)\n        logits = self.lm_head(x) # (B, T, vocab_size)\n        return logits\n    def generate(self, max_new_tokens):\n        ctx = mx.zeros((1, 1), dtype=mx.int32)\n        for _ in range(max_new_tokens):\n          logits = self(ctx[:, -ctx_len:]) # pass in last ctx_len characters\n          logits = logits[:, -1, :] # get logits for the next token\n          next_tok = mx.random.categorical(logits, num_samples=1)\n          ctx = mx.concatenate((ctx, next_tok), axis=1)\n        return ctx\n    def _init_parameters(self):\n        normal_init = nn.init.normal(mean=0.0, std=0.02)\n        residual_init = nn.init.normal(mean=0.0, std=(0.02 \/ math.sqrt(2 * n_layers)))\n        new_params = []\n        for name, module in self.named_modules():\n            if isinstance(module, nn.layers.linear.Linear):\n                if 'c_proj' in name:\n                    new_params.append((name + '.weight', residual_init(module.weight)))\n                else:\n                    new_params.append((name + '.weight', normal_init(module.weight)))\n                if 'bias' in module:\n                    new_params.append((name + '.bias', mx.zeros(module.bias.shape)))\n            elif isinstance(module, nn.layers.embedding.Embedding):\n                new_params.append((name + '.weight', normal_init(module.weight)))\n        self = self.update(utils.tree_unflatten(new_params))\n\nWe also add 2 lines of code in the constructor to print the total number of params. Finally, we are ready to build the training loop.\n\nTraining Loop\nTo train the model we need a loss function. Since we are predicting classes (next token) we use cross-entropy loss.\n\ndef loss_fn(model, x, y):\n    logits = model(x)\n    B, T, C = logits.shape # (batch_size, seq_len, vocab_size)\n    logits = logits.reshape(B*T, C)\n    y = y.reshape(B*T)\n    loss = nn.losses.cross_entropy(logits, y, reduction='mean')\n    return loss\nFirst, we get the logits from the model. Then we reshape logits to make a list of vocab_size length arrays. We also reshape y, the correct token ids, to have the same length. Then we use the built-in cross-entropy loss function to calculate the loss for each example and average them to get a single value.\n\nmodel = GPT()\nmx.eval(model.parameters()) # Create the model params (mlx is lazy evaluation)\nloss_and_grad = nn.value_and_grad(model, loss_fn)\nlr = 0.1\noptimizer = optim.AdamW(learning_rate=lr)\nNext, we instantiate the model, but since mlx is lazy evaluation it won\u2019t allocate and create the parameters. We need to call mx.eval on the parameters to ensure they get created. Then we can use nn.value_and_grad() to get a function that returns the loss and gradient of model parameters w.r.t the loss. This is all we need to optimize. Finally, we initialize an AdamW optimizer.\n\nA quick note on nn.value_and_grad(). If you are used to PyTorch you might expect us to use loss.backward() which goes through the computation graph and updates the .grad attribute of each tensor in our model. However, mlx automatic differentiation works on functions instead of computation graphs [3]. Therefore, mlx has built-ins that take in a function and return the gradient function such as nn.value_and_grad() .\n\nNow we define the training loop.\n\nnum_epochs=20\nbatch_size=32\nfor epoch in range(num_epochs):\n    model.train(True)\n    running_loss = 0\n    batch_cnt = 0\n    for input, label in get_batches(X_train, y_train, batch_size):\n        batch_cnt += 1\n        loss, grads = loss_and_grad(model, input, label)\n        optimizer.update(model, grads)\n        running_loss += loss.item()\n        # compute new parameters and optimizer state\n        mx.eval(model.parameters(), optimizer.state)\n    avg_train_loss = running_loss \/ batch_cnt\n    model.train(False) # set eval mode\n    running_loss = 0\n    batch_cnt = 0\n    for input, label in get_batches(X_val, y_val, batch_size):\n        batch_cnt += 1\n        loss = loss_fn(model, input, label)\n        running_loss += loss.item()\n    avg_val_loss = running_loss \/ batch_cnt\n    print(f\"Epoch {epoch:2} | train = {avg_train_loss:.4f} | val = {avg_val_loss:.4f}\")\nThe outer loop runs through the epochs. We first set the model to training mode because some modules have different behaviors during training and testing such as dropout. Then we use our get_batches function from earlier to loop through batches of the training data. We get the loss over the batch and the gradient using loss_and_grad . Then we pass the model and gradients to the optimizer to update the model parameters. Finally we call mx.eval (remember mlx does lazy evaluation) to ensure the parameters and optimizer state get updated. Then we calculate the average train loss over the data to print later. This is one pass through the training data. Similarly, we calculate the validation loss and then print the average train and val loss over the epoch.\n\ncompletion = decode(model.generate(1000)[0].tolist())\nprint(completion)\nwith open('completions.txt', 'w') as f:\n    f.write(completion)\nFinally, we add some code to generate from our model. Since the generation output is still in the (B, T) shape we have to index it at 0 to make it 1D and then convert it from an mlx array to a Python list. Then we can pass it to our decode function from earlier, and write it to a file.\n\nThese are the parameters we will use for training (you can play around with this):\n\nctx_len = 128\nn_emb = 128\ndropout = 0.1\nhead_size = 128\nn_heads = 4\nn_layers = 3\nnum_epochs = 20\nbatch_size = 64\nlr = 1e-3\nNow we can run the file to start training. With the settings above training took around 10 minutes on my m2 MacBook. I achieved the following training loss last epoch.\n\nEpoch 19 | train = 1.6961 | val = 1.8143\nLet\u2019s look at some output.\n\nGLOUCESTER:\nBut accomes mo move it.\n\nKING EDWARD:\nWhere our that proclaim that I curse, or I sprithe.\n\nCORIOLANUS:\nNot want:\nHis bops to thy father\nAt with hath folk; by son and fproathead:\nThe good nor may prosperson like it not,\nWhat, the beggares\nMore hath, when that made a,\nYour vainst Citizen:\nLet here are go in queen me and knife\nTo my deserved me you promise: not a fettimes,\nThat one the will not.\n\nCORIOLANUS:\nAnd been of queens,\nThou to do we best!\n\nJULIET:\nNot, brother recourable this doth our accuse\nInto fight!\nNot bad for just 10 minutes of training with a tiny model that is predicting characters! It clearly has the form of Shakespeare, although it is nonsense. The only difference between our model and the real GPT-2 now is scale! Now I encourage you to experiment \u2014 try out different settings, maybe tinker with the architecture, and see how low of a loss you can achieve.\n\nReferences\n[1] Karpathy A (2015).Tiny Shakespeare [Data set]. https:\/\/github.com\/karpathy\/char-rnn (MIT license)\n\n[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language Models are Unsupervised Multitask Learners (2019), OpenAI\n\n[3] Automatic Differentiation \u2014 mlx docs","query":"How can I freeze a variable during training in MLX?","ground_truth":"To freeze an attribute during training, you can add '_' as a prefix to the attribute name, which signals to mlx that it is not a parameter and should not be updated during training.","response_correct":"Add '_' before the variable name to mark it as non-trainable in MLX.","response_wrong":"Use a 'freeze' function in MLX to prevent a variable from being updated during training."}
{"document_index":12,"context":"Gleam for Python users\nHello productive pragmatic Pythonistas!\n\na soft wavey boundary between two sections of the website\nComments\nVariables\nMatch operator\nVariables type annotations\nFunctions\nExporting functions\nFunction type annotations\nReferencing functions\nLabelled arguments\nOperators\nConstants\nBlocks\nData types\nStrings\nTuples\nLists\nDicts\nFlow control\nCase\nTry\nType aliases\nCustom types\nRecords\nUnions\nOpaque custom types\nModules\nImports\nNamed imports\nUnqualified imports\nComments\nPython\nIn Python, comments are written with a # prefix.\n\n# Hello, Joe!\nA docstring (matching \u201c\u201d\u201d) that occurs as the first statement in a module, function, class, or method definition will become the __doc__ attribute of that object.\n\ndef a_function():\n    \"\"\"Return some important data.\"\"\"\n    pass\nGleam\nIn Gleam, comments are written with a \/\/ prefix.\n\n\/\/ Hello, Joe!\nComments starting with \/\/\/ are used to document the following statement. Comments starting with \/\/\/\/ are used to document the current module.\n\n\/\/\/\/ This module is very important.\n\n\/\/\/ The answer to life, the universe, and everything.\nconst answer: Int = 42\nVariables\nYou can reassign variables in both languages.\n\nPython\nsize = 50\nsize = size + 100\nsize = 1\nPython has no specific variable keyword. You choose a name and that\u2019s it!\n\nGleam\nGleam has the let keyword before its variable names.\n\nlet size = 50\nlet size = size + 100\nlet size = 1\nMatch operator\nPython\nPython supports basic, one directional destructuring (also called unpacking). Tuple of values can be unpacked and inner values can be assigned to left-hand variable names.\n\n(a, b) = (1, 2)\n# a == 1\n# b == 2\n\n# works also for for-loops\nfor key, value in enumerate(a_dict):\n    print(key, value)\nGleam\nIn Gleam, let and = can be used for pattern matching, but you\u2019ll get compile errors if there\u2019s a type mismatch, and a runtime error if there\u2019s a value mismatch. For assertions, the equivalent let assert keyword is preferred.\n\nlet #(x, _) = #(1, 2)\nlet assert [] = [1] \/\/ runtime error\nlet assert [y] = \"Hello\" \/\/ compile error, type mismatch\nVariables type annotations\nPython\nPython is a dynamically typed language. Types are only checked at runtime and a variable can have different types in its lifetime.\n\nType hints (Python 3+) are optional annotations that document the code with type information. These annotations are accessible at runtime via the __annotations__ module-level variable.\n\nThese hints will mainly be used to inform static analysis tools like IDEs, linters\u2026\n\nsome_list: list[int] = [1, 2, 3]\nGleam\nIn Gleam type annotations can optionally be given when binding variables.\n\nlet some_list: List(Int) = [1, 2, 3]\nGleam will check the type annotation to ensure that it matches the type of the assigned value. It does not need annotations to type check your code, but you may find it useful to annotate variables to hint to the compiler that you want a specific type to be inferred.\n\nFunctions\nPython\nIn Python, you can define functions with the def keyword. In that case, the return keyword is mandatory.\n\ndef sum(x, y):\n    return x + y\nAnonymous functions returning a single expression can also be defined with the lambda keyword and be assigned into variables.\n\nmul = lambda x, y: x * y\nmul(1, 2)\nGleam\nGleam\u2019s functions are declared using a syntax similar to Rust or JavaScript. Gleam\u2019s anonymous functions have a similar syntax and don\u2019t need a . when called.\n\npub fn sum(x, y) {\n  x + y\n}\n\nlet mul = fn(x, y) { x * y }\nmul(1, 2)\nExporting functions\nPython\nIn Python, top level functions are exported by default. There is no notion of private module-level functions.\n\nGleam\nIn Gleam, functions are private by default and need the pub keyword to be public.\n\n\/\/ this is public\npub fn sum(x, y) {\n  x + y\n}\n\n\/\/ this is private\nfn mul(x, y) {\n  x * y\n}\nFunction type annotations\nPython\nType hints can be used to optionally annotate function arguments and return types.\n\nDiscrepancies between type hints and actual values at runtime do not prevent interpretation of the code.\n\nStatic code analysers (IDE tooling, type checkers like mypy) will be required to detect those errors.\n\ndef sum(x: int, y: int) -> int:\n    return x + y\n\ndef mul(x: int, y: int) -> bool:\n    # no errors from the interpreter.\n    return x * y\nGleam\nFunctions can optionally have their argument and return types annotated in Gleam. These type annotations will always be checked by the compiler and throw a compilation error if not valid. The compiler will still type check your program using type inference if annotations are omitted.\n\npub fn add(x: Int, y: Int) -> Int {\n  x + y\n}\n\npub fn mul(x: Int, y: Int) -> Bool { \/\/ compile error, type mismatch\n  x * y\n}\nReferencing functions\nPython\nAs long as functions are in scope they can be assigned to a new variable. There is no special syntax to assign a module function to a variable.\n\nGleam\nGleam has a single namespace for value and functions within a module, so there is no need for a special syntax to assign a module function to a variable.\n\nfn identity(x) {\n  x\n}\n\nfn main() {\n  let func = identity\n  func(100)\n}\nLabelled arguments\nBoth Python and Gleam have ways to give arguments names and in any order.\n\nPython\nKeyword arguments are evaluated once at function definition time, and there is no evidence showing a noticeable performance penalty when using named arguments.\n\nWhen calling a function, arguments can be passed\n\npositionally, in the same order of the function declaration\nby name, in any order\ndef replace(inside: str, each: str, with_string: str):\n    pass\n\n# equivalent calls\nreplace('hello world', 'world', 'you')\nreplace(each='world', inside='hello world',  with_string='you')\nGleam\nIn Gleam arguments can be given a label as well as an internal name. Contrary to Python, the name used at the call-site does not have to match the name used for the variable inside the function.\n\npub fn replace(inside string, each pattern, with replacement) {\n  go(string, pattern, replacement)\n}\nreplace(each: \",\", with: \" \", inside: \"A,B,C\")\nThere is no performance cost to Gleam\u2019s labelled arguments as they are optimised to regular function calls at compile time, and all the arguments are fully type checked.\n\nOperators\nOperator\tPython\tGleam\tNotes\nEqual\t==\t==\tIn Gleam both values must be of the same type\nStrictly equal to\t==\t==\tComparison in Gleam is always strict. (see note for Python)\nReference equality\tis\t \tTrue only if the two objects have the same reference\nNot equal\t!=\t!=\tIn Gleam both values must be of the same type\nGreater than\t>\t>\tIn Gleam both values must be ints\nGreater than\t>\t>.\tIn Gleam both values must be floats\nGreater or equal\t>=\t>=\tIn Gleam both values must be ints\nGreater or equal\t>=\t>=.\tIn Gleam both values must be floats\nLess than\t<\t<\tIn Gleam both values must be ints\nLess than\t<\t<.\tIn Gleam both values must be floats\nLess or equal\t<=\t<=\tIn Gleam both values must be ints\nLess or equal\t<=\t<=.\tIn Gleam both values must be floats\nBoolean and\tand\t&&\tIn Gleam both values must be bools\nLogical and\tand\t \tNot available in Gleam\nBoolean or\tor\t||\tIn Gleam both values must be bools\nLogical or\tor\t \tNot available in Gleam\nAdd\t+\t+\tIn Gleam both values must be ints\nAdd\t+\t+.\tIn Gleam both values must be floats\nSubtract\t-\t-\tIn Gleam both values must be ints\nSubtract\t-\t-.\tIn Gleam both values must be floats\nMultiply\t*\t*\tIn Gleam both values must be ints\nMultiply\t*\t*.\tIn Gleam both values must be floats\nDivide\t\/\t\/\tIn Gleam both values must be ints\nDivide\t\/\t\/.\tIn Gleam both values must be floats\nRemainder\t%\t%\tIn Gleam both values must be ints, in Gleam negative values behave differently: Use int.modulo to mimick Python\u2019s behavior.\nConcatenate\t+\t<>\tIn Gleam both values must be strings\nPipe\t \t|>\tGleam\u2019s pipe can pipe into anonymous functions. This operator does not exist in python\nSome notes for Python:\n\n== is by default comparing by value:\n\nscalars will have their value compared\nthe only type cast will be for 0 and 1 that will be coerced to False and True respectively\nvariables that point to the same object will be equal with ==\ntwo objects with the same members values won\u2019t be equal:\n\nno structural equality, unless the __eq__ operator is redefined.\nPython operators are short-circuiting as in Gleam.\nPython operators can be overloaded and be applied to any types with potential custom behaviors\nConstants\nPython\nIn Python, top-level declarations are in the global\/module scope is the highest possible scope. Any variables and functions defined will be accessible from anywhere in the code.\n\nThere is no notion of constant variables in Python.\n\n# in the global scope\nTHE_ANSWER = 42\nGleam\nIn Gleam constants can be created using the const keyword.\n\nconst the_answer = 42\n\npub fn main() {\n  the_answer\n}\nBlocks\nPython\nPython blocks are always associated with a function \/ conditional \/ class declarations\u2026 There is no way to create multi-line expressions blocks like in Gleam.\n\nBlocks are declared via indentation.\n\ndef a_func():\n    # A block here\n    pass\nGleam\nIn Gleam braces { } are used to group expressions.\n\npub fn main() {\n  let x = {\n    some_function(1)\n    2\n  }\n  let y = x * {x + 10} \/\/ braces are used to change arithmetic operations order\n  y\n}\nData types\nStrings\nIn Python, strings are stored as unicode code-points sequence. Strings can be encoded or decoded to\/from a specific encoding.\n\nIn Gleam all strings are UTF-8 encoded binaries.\n\nPython\n\"Hell\u00f8, world!\"\nGleam\n\"Hell\u00f8, world!\"\nTuples\nTuples are very useful in Gleam as they\u2019re the only collection data type that allows mixed types in the collection.\n\nPython\nPython tuples are immutable, fixed-size lists that can contain mixed value types. Unpacking can be used to bind a name to a specific value of the tuple.\n\nmy_tuple = (\"username\", \"password\", 10)\n_, password, _ = my_tuple\nGleam\nlet my_tuple = #(\"username\", \"password\", 10)\nlet #(_, password, _) = my_tuple\nLists\nLists in Python are allowed to have values of mixed types, but not in Gleam.\n\nPython\nPython can emulate the cons operator of Gleam using the * operator and unpacking:\n\nlist = [2, 3, 4]\n[head, *tail] = list\n# head == 2\n# tail == [3, 4]\nGleam\nGleam has a cons operator that works for lists destructuring and pattern matching. In Gleam lists are immutable so adding and removing elements from the start of a list is highly efficient.\n\nlet list = [2, 3, 4]\nlet list = [1, ..list]\nlet [1, second_element, ..] = list\n[1.0, ..list] \/\/ compile error, type mismatch\nDictionaries\nIn Python, dictionaries can have keys of any type as long as:\n\nthe key type is hashable, such as integers, strings, tuples (due to their immutable values), functions\u2026 and custom mutable objects implementing the __hash__ method.\nthe key is unique in the dictionary. and values of any type.\nIn Gleam, dicts can have keys and values of any type, but all keys must be of the same type in a given dict and all values must be of the same type in a given dict.\n\nThere is no dict literal syntax in Gleam, and you cannot pattern match on a dict. Dicts are generally not used much in Gleam, custom types are more common.\n\nPython\n{\"key1\": \"value1\", \"key2\": \"value2\"}\n{\"key1\":  \"1\", \"key2\": 2}\nGleam\nimport gleam\/dict\n\ndict.from_list([#(\"key1\", \"value1\"), #(\"key2\", \"value2\")])\ndict.from_list([#(\"key1\", \"value1\"), #(\"key2\", 2)]) \/\/ Type error!\nFlow control\nCase\nCase is one of the most used control flows in Gleam. It can be seen as a switch statement on steroids. It provides a terse way to match a value type to an expression. Gleam\u2019s case expression is fairly similar to Python\u2019s match statement.\n\nPython\nMatching on primitive types:\n\ndef http_error(status):\n    match status:\n        case 400:\n            return \"Bad request\"\n        case 404:\n            return \"Not found\"\n        case 418:\n            return \"I'm a teapot\"\nMatching on tuples with variable capturing:\n\nmatch point:\n    case (0, 0):\n        print(\"Origin\")\n    case (0, y):\n        print(f\"Y={y}\")\n    case (x, 0):\n        print(f\"X={x}\")\n    case (x, y):\n        print(f\"\"X={x}, Y={y}\"\")\n    case _:\n        raise ValueError(\"Not a point\")\nMatching on type constructors:\n\nmatch point:\n    case Point(x=0, y=0):\n        print(\"Origin is the point's location.\")\n    case Point(x=0, y=y):\n        print(f\"Y={y} and the point is on the y-axis.\")\n    case Point(x=x, y=0):\n        print(f\"X={x} and the point is on the x-axis.\")\n    case Point():\n        print(\"The point is located somewhere else on the plane.\")\n    case _:\n        print(\"Not a point\")\nThe match expression supports guards, similar to Gleam:\n\nmatch point:\n    case Point(x, y) if x == y:\n        print(f\"The point is located on the diagonal Y=X at {x}.\")\n    case Point(x, y):\n        print(f\"Point is not on the diagonal.\")\nGleam\nThe case operator is a top level construct in Gleam:\n\ncase some_number {\n  0 -> \"Zero\"\n  1 -> \"One\"\n  2 -> \"Two\"\n  n -> \"Some other number\" \/\/ This matches anything\n}\nThe case operator especially coupled with destructuring to provide native pattern matching:\n\ncase xs {\n  [] -> \"This list is empty\"\n  [a] -> \"This list has 1 element\"\n  [a, b] -> \"This list has 2 elements\"\n  _other -> \"This list has more than 2 elements\"\n}\nThe case operator supports guards:\n\ncase xs {\n  [a, b, c] if a >. b && a <=. c -> \"ok\"\n  _other -> \"ko\"\n}\nand disjoint union matching:\n\ncase number {\n  2 | 4 | 6 | 8 -> \"This is an even number\"\n  1 | 3 | 5 | 7 -> \"This is an odd number\"\n  _ -> \"I'm not sure\"\n}\nTry\nError management is approached differently in Python and Gleam.\n\nPython\nPython uses the notion of exceptions to interrupt the current code flow and pop up the error to the caller.\n\nAn exception is raised using the keyword raise.\n\ndef a_function_that_fails():\n    raise Exception(\"an error\")\nThe callee block will be able to capture any exception raised in the block using a try\/except set of blocks:\n\ntry:\n    print(\"executed\")\n    a_function_that_fails()\n    print(\"not_executed\")\nexcept Exception as e:\n    print(\"doing something with the exception\", e)\n\nGleam\nIn contrast in Gleam, errors are just containers with an associated value.\n\nA common container to model an operation result is Result(ReturnType, ErrorType).\n\nA Result is either:\n\nan Error(ErrorValue)\nor an Ok(Data) record\nHandling errors actually means to match the return value against those two scenarios, using a case for instance:\n\ncase int.parse(\"123\") {\n  Error(e) -> io.println(\"That wasn't an Int\")\n  Ok(i) -> io.println(\"We parsed the Int\")\n}\nIn order to simplify this construct, we can use the use expression with the try function from the gleam\/result module.\n\nbind a value to the providing name if Ok(Something) is matched\ninterrupt the flow and return Error(Something)\nlet a_number = \"1\"\nlet an_error = \"ouch\"\nlet another_number = \"3\"\n\nuse int_a_number <- try(parse_int(a_number))\nuse attempt_int <- try(parse_int(an_error)) \/\/ Error will be returned\nuse int_another_number <- try(parse_int(another_number)) \/\/ never gets executed\n\nOk(int_a_number + attempt_int + int_another_number) \/\/ never gets executed\nType aliases\nType aliases allow for easy referencing of arbitrary complex types. Even though their type systems does not serve the same function, both Python and Gleam provide this feature.\n\nPython\nA simple variable can store the result of a compound set of types.\n\ntype Headers = list[tuple[str, str]]\n\n# can now be used to annotate a variable\nheaders: Headers = [(\"Content-Type\", \"application\/json\")]\nGleam\nThe type keyword can be used to create aliases:\n\npub type Headers =\n  List(#(String, String))\n\nlet headers: Headers = [#(\"Content-Type\", \"application\/json\")]\nCustom types\nRecords\nCustom type allows you to define a collection data type with a fixed number of named fields, and the values in those fields can be of differing types.\n\nPython\nPython uses classes to define user-defined, record-like types. Properties are defined as class members and initial values are generally set in the constructor.\n\nBy default the constructor does not provide base initializers in the constructor so some boilerplate is needed:\n\nclass Person:\n    name: str\n    age: int\n\n    def __init__(name: str, age: int) -> None:\n        self.name = name\n        self.age = age\n\nperson = Person(name=\"Jake\", age=20)\n# or with positional arguments Person(\"Jake\", 20)\nname = person.name\nMore recent alternatives are to use dataclasses or to leverage the NamedTuple base type to generate a constructor with initializers.\n\nBy default a class created with the dataclass decorator is mutable (although you can pass options to the dataclass decorator to change the behavior):\n\nfrom dataclasses import dataclasses\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n\nperson = Person(name=\"Jake\", age=20)\nname = person.name\nperson.name = \"John\"  # The name is now \"John\"\nNamedTuples on the other hand are immutable:\n\nfrom typing import NamedTuple\n\nclass Person(NamedTuple):\n    name: str\n    age: int\n\nperson = Person(name=\"Jake\", age=20)\nname = person.name\n\n# cannot reassign a value\nperson.name = \"John\"  # error\nGleam\nGleam\u2019s custom types can be used in much the same way that structs are used in Elixir. At runtime, they have a tuple representation and are compatible with Erlang records.\n\ntype Person {\n  Person(name: String, age: Int)\n}\n\nlet person = Person(name: \"Jake\", age: 35)\nlet name = person.name\nAn important difference to note is there is no OOP in Gleam. Methods can not be added to types.\n\nUnions\nIn Python unions can be declared with the | operator.\n\nIn Gleam functions must always take and receive one type. To have a union of two different types they must be wrapped in a new custom type.\n\nPython\ndef int_or_float(x: int | float) -> str:\n    if isinstance(x, int):\n        return f\"It's an integer: {x}\"\n    else:\n        return f\"It's a float: {x}\"\nGleam\ntype IntOrFloat {\n  AnInt(Int)\n  AFloat(Float)\n}\n\nfn int_or_float(x) {\n  case x {\n    AnInt(1) -> \"It's an integer: 1\"\n    AFloat(1.0) -> \"It's a float: 1.0\"\n  }\n}\nOpaque custom types\nIn Python, constructors cannot be marked as private. Opaque types can be imperfectly emulated using a class method and some magic property that only updates via the class factory method.\n\nIn Gleam, custom types can be defined as being opaque, which causes the constructors for the custom type not to be exported from the module. Without any constructors to import other modules can only interact with opaque types using the intended API.\n\nPython\nclass OnlyCreatable:\n\n    __create_key = object()\n\n    @classmethod\n    def create(cls, value):\n        return OnlyCreatable(cls.__create_key, value)\n\n    def __init__(self, create_key, value):\n        assert(create_key == OnlyCreatable.__create_key), \\\n            \"OnlyCreatable objects must be created using OnlyCreatable.create\"\n        self.value = value\nGleam\npub opaque type Identifier {\n  Identifier(Int)\n}\n\npub fn get_id() {\n  Identifier(100)\n}\nModules\nPython\nThere is no special syntax to define modules as files are modules in Python\n\nGleam\nGleam\u2019s file is a module and named by the file name (and its directory path). Since there is no special syntax to create a module, there can be only one module in a file.\n\n\/\/ in file foo.gleam\npub fn identity(x) {\n  x\n}\n\/\/ in file main.gleam\nimport foo \/\/ if foo was in a folder called `lib` the import would be `lib\/foo`\npub fn main() {\n  foo.identity(1)\n}\nImports\nPython\n# inside module src\/nasa\/moon_base.py\n# imports module src\/nasa\/rocket_ship.py\nfrom nasa import rocket_ship\n\ndef explore_space():\n    rocket_ship.launch()\nGleam\nImports are relative to the root src folder.\n\nModules in the same directory will need to reference the entire path from src for the target module, even if the target module is in the same folder.\n\n\/\/ inside module src\/nasa\/moon_base.gleam\n\/\/ imports module src\/nasa\/rocket_ship.gleam\nimport nasa\/rocket_ship\n\npub fn explore_space() {\n  rocket_ship.launch()\n}\nNamed imports\nPython\nimport unix.cat as kitty\nGleam\nimport unix\/cat as kitty\nUnqualified imports\nPython\nfrom animal.cat import Cat, stroke\n\ndef main():\n    kitty = Cat(name=\"Nubi\")\n    stroke(kitty)\nGleam\nimport animal\/cat.{Cat, stroke}\n\npub fn main() {\n  let kitty = Cat(name: \"Nubi\")\n  stroke(kitty)\n}","query":"What is the syntax for pipe","ground_truth":"The operator for Pipe in Gleam is '|>'.","response_correct":"In Gleam, the pipe operator is written as '|>'.","response_wrong":"The operator for Pipe in Gleam is '|'. This helps to pass results directly into another function."}
{"document_index":14,"context":"My eyes felt like galaxies\u2014holding the swirling glow of countless memories\u2014as I took in our childhood home. Its siding looked like remnants of driftwood after a bonfire. I swore I smelled the smoky char of pine creep into my nostrils. It\u2019s wild how the past stays with you like that. It can feel more visceral and real than the tangible things right in front of you.\n\n\u201cJesus, it feels like just yesterday.\u201d I placed a trembling hand over my heart, struggling to steady my breath.\n\nMy brother, Perry, pulled me into a tight embrace, his strength grounding me like an anchor.\n\n\u201cThe house hasn\u2019t changed much,\u201d he said, his voice steady and comforting. \u201cBut we have.\u201d His certainty made me question, Have I really changed?\n\nBetween the two of us, Perry was as solid and stoic as a mountain range. Good thing, because I was like the wind\u2014flighty and unpredictable. Over the years, Perry had learned to handle even my harshest hurricanes.\n\nBeing his older sister\u2014even if only by four minutes\u2014I always wished I\u2019d been his protector rather than the other way around. But that demon burning deep in my belly also flashed a crooked smile, knowing that Perry would never abandon me, especially since I got sober.\n\nI hadn\u2019t had a drink in exactly seven hundred and thirty days, and although it remained unsaid, I knew Perry was terrified of leaving me to my own devices in fear I would relapse.\n\nOur sibling bond was iron-clad. After we lost our parents in the fire (my mother didn\u2019t properly butt out her 2:00 am cigarette and well, the rest is history), all Perry and I had was each other. But let\u2019s call a spade a spade; we were also as fucked up and as co-dependent as it gets. Who mutually decides to visit the catalyst of your alcohol addiction on the anniversary of your sobriety?\n\nThe house\u2019s dilapidated front door creaked as Perry gently pushed it open. The rusted metal hinges were holding it up by a thread.\n\n\u201cAfter you.\u201d Perry gestured me in, squinting from the sunlight. He was a gentleman, even in such obscurity.\n\nAs he held the door open, the shallow scar on his right cheek taunted me like some kind of schoolyard bully. His wound often pulled me in like that. Some days, I was sure I would dive right into it and drown. Other days, I prayed to God and the Devil himself to just let me fucking drown, already.\n\nThat mark became permanently etched on Perry\u2019s face on the day I quit drinking, exactly seven hundred and thirty days ago. That was the day Perry screamed bloody murder at me from the passenger seat, \u201cJackie! Stop the fucking car!\u201d But my bloodstream was far too poisoned with Bacardi Limon to listen. All I remember next was my vehicle being wrapped around a tree. I could have died that day, but what truly disturbed me in the middle of the night was the fact that I almost killed Perry.\n\nA lot can happen in seven hundred and thirty days. But I assure you, forgiving yourself isn\u2019t one of them.\n\n\u201cWell? You coming in?\u201d Perry was still holding the door ajar.\n\nI shook it off and gave my brother a knowing glance. I swear, even though we were fraternal, we had twin telepathy. I exhaled and walked in.\n\n\u201cWatch your step,\u201d I warned, my forehead tense.\n\nI imagined the rickety floorboards collapsing, crashing us into what had once been our dad\u2019s \u201cman cave\u201d. That\u2019s where he was passed out, the night of the fire.\n\n\u201cKids, stay here. Do not move,\u201d our mother demanded after getting us out of the house safely. I remember the black soot on her face and the spiderweb veins in her eyes. She shook us firmly by the forearms. \u201cI\u2019m getting your father.\u201d\n\nPerry and I held each other, shaking. The heat from the inferno felt like standing in a volcano. We never saw our parents again.\n\nTwo decades later, there we were\u2014Perry and I\u2014-making our way through the wreckage of our home. It was midday, yet the inside of the house screamed a tone of pale blue midnight. My shoulders were up to my ears, as though we were walking through a haunted house attraction.\n\nI coughed into my forearm. The ocean of dust was thick like butter. As I cleared my lungs, Perry called out from behind me.\n\n\u201cJacks, look at this! The fireplace,\u201d Perry's voice was filled with awe.\n\n\u201cUnbelievable. It\u2019s still here,\u201d I whispered, a lump forming in my throat.\n\nIt was as though a Fairy Godmother breezed by and brought the decaying living room to life with her magical paintbrush. Kind of like in \u201cTitanic\u201d, when they showed the sunken ship underwater, versus in its prestige as it sailed across the Atlantic.\n\nWe made our way over to the fireplace and sat cross-legged on the floor.\n\n\u201cThis was our favorite spot, remember?\u201d I avoided his gaze, overwhelmed by the flood of memories.\n\n\u201cYeah,\u201d Perry murmured, his eyes softening with nostalgia.\n\nFor a moment, the taste of crispy, fire roasted marshmallows superseded the saturated stench of mildew.\n\n\u201cRemember our s\u2019mores nights?\u201d I asked.\n\n\u201cDuh. What about all our fireplace movie nights?\u201d Perry proceeded to do his best, nasally, childhood impersonation of me, \u201cBut mom! I want to watch Beauty and the Beast!! Perry always gets to pick the movie!!\u201d\n\nI punched him in the arm, \u201cFirst of all, I never sounded like that. And second. So what? I knew what I wanted.\u201d\n\nThe corners of Perry\u2019s mouth lifted. He had such a sincere sparkle about him, as though a storm cloud of confetti followed him overhead wherever he went, \u201cYou really did, kiddo.\u201d\n\nMy chest went heavy. How could Perry love me after everything I had done? After all the relationships I\u2019d ruined? All the jobs I\u2019d lost? All of his relationships I\u2019d ruined? How could he still choose me, when so often I had chosen a forty of Jack Daniels over him?\n\nHow could Perry still love me after I almost fucking killed him?\n\nPerry\u2019s gaze widened, \u201cHey! Remember when Mom would bring out those hot drinks she always made?\u201d He paused, almost as if he was searching for the right term. \u201cApple\u2026 something? Apple ssshhh\u2026\u201d\n\nI snapped my fingers, \u201cApple-Schnapple!\u201d\n\n\u201cYes!\u201d\n\n\u201cI mean, looking back it was basically just hot apple cider, but damn it was good.\u201d And it really was.\n\nOur laughs echoed throughout the abandoned asylum we once called home.\n\nPerry leaned back, holding himself up with his hands. \u201cI loved our fireplace hangs. Especially our movie nights down here. But nothing beats our movies up in the projector room.\u201d\n\nI tilted my head, \u201cWe never had a projector room.\u201d\n\nPerry playfully \u201cshoo\u2019d\u201d me away.\n\n\u201cNo. Perry. I would remember us having a projector room. Our movie nights together were our favorite thing. You even just said it yourself.\u201d\n\nThe house suddenly became pin-drop silent as Perry leaned in. \"Memories are quite something, aren\u2019t they?\" The slight shift in his tone made my skin crawl. Perry was always wistful, but this felt different, almost\u2026 clinical, \"We often remember things in ways that are\u2026 easier for us to digest.\"\n\nI was fidgety. \u201cGees Perry. You sound like Dr. Lasko.\u201d\n\nHe seemed to enjoy my little joke.\n\nDr. Lasko had been my therapist since the accident, and I would go out on a limb and say that he would not have approved of this self-inflicted exposure therapy I was subjecting myself to by visiting the house.\n\nPerry seemingly snapped out of his little therapist moment and went right back to being his sparkly confetti self. As I watched his amicable face scan the mantle above the fireplace, I felt a sickening uneasiness. Imagine you had actually fucking killed him.\n\n\u201cMan, I can still picture all our family photos up there,\u201d Perry\u2019s childlike wonder destroyed me.\n\nMy face went flush. I could feel the water rising in my eyes like the tides. How pathetic and infuriating was it that after everything I\u2019d done, I was still somehow the victim.\n\n\u201cHey.\u201d He took my hand.\n\n\u201cOh Perry,\u201d  I threw myself into him. \u201cI\u2019m so sorry.\u201d\n\nMy brother held me with his usual care that I didn\u2019t deserve.\n\n\u201cJacks, it\u2019s ok. I\u2019m still here. We\u2019re both still here.\u201d\n\nAs my chin rested on my brother\u2019s shoulder, I looked ahead of me at the remains of the house. Something felt off, and it wasn\u2019t just the overall unsettling environment. My brow furrowed. \u201cWasn\u2019t the doorway to the kitchen on the other side of the living room?\u201d\n\nI felt Perry shrug, \u201cI don\u2019t think so.\u201d\n\nI was staring into the out of place doorway like I was trying to pull a recollection towards me. And that\u2019s when I saw them in the kitchen: the translucent memory of mom and dad. Mom was getting our Apple-Schnapples ready. She was pacing, unlike Dad, who was sitting motionless at the table. His face was planted on its mahogany surface. His glass of Apple-Schnapple was empty, and so was the bottle of Jim Beam beside it.\n\nMom floated into the living room, our warm beverages in hand and a cigarette in her mouth, \u201cKids, your father\u2019s not feeling well. Let\u2019s have our Apple-Schnapples in here.\u201d\n\nOh my God. The bruise on her face.\n\nPerry jarred me back to reality with the gut punch of what he had to say next, \u201cYou\u2019re remembering the truth about mom and dad, aren\u2019t you?\u201d\n\nI pushed myself off and away from him. \u201cHow did you\u2026\u201d\n\nMy brother looked down, delicately tracing the floor with his finger, \u201cWe always put them on a pedestal after they died.\u201d\n\nI felt a shiver run down my spine, \u201cWhat are you talking about?\u201d\n\nAs Perry continued to speak, his words grew even more detached. \"Do you remember that first drink Dad ever gave you?\"\n\nMy eyes darted around the room as my jugular pulsed in my neck. As much as I tried to escape what Perry had just said, I did remember.\n\nI could hear my dad\u2019s slurring words of encouragement, \u201cCome on, Jackie. Just one drink. It\u2019ll be our special time, just you and me.\u201d\n\nThe bitterness of that first sip of beer made me squirm, but sharing a \u201cspecial time\u201d with my dad\u2014and the desperate yearning that maybe he did love me, afterall\u2014was the overwhelm of the full moon swallowing me whole. I was only a child, and much like how my mom turned a blind eye to my father\u2019s drinking, she did the exact same when it came to her daughter.\n\nI\u2019d used the death of my parents as the excuse for my alcoholism for so long, because admitting that they helped create the monster I would eventually become was like a knife to the heart. And knowing I had been too weak to conquer the addiction from my own volition just made the weapon twist in my chest.\n\nThe room was spinning. My face was blistering hot like the night of the fire. Or was that the warm heat from the fireplace when we were kids? The childhood fireplace memories ran through my mind, frame by frame, until\u2026..they eventually vaporized to nothing. I crouched over, thinking I might vomit.\n\n\u201cWe never had a fireplace.\u201d Perry was nodding, very matter-of-factly.\n\nMy fingernails dug into my thighs when I looked at the fireplace and: it was gone. Only a blank wall of faded, forest green wallpaper remained. Our house was once on fire, yes, but that was it. There was never a happy childhood fireplace. Ever.\n\nMy hands were cold and clammy. I fell back onto the wall behind me. \u201cPerry. Where are we?\u201d\n\nHe stood up and glided towards the staircase. One hand on the banister, his footsteps creaked, one by one, as he made his way to the second floor.\n\nMy mouth was bone dry, \u201cPerry!\u201d\n\nHe stopped and turned towards me, \u201cCome to the projector room. We love watching movies together, don\u2019t we? There\u2019s a movie I\u2019d like to show you.\u201d\n\nAs my brother disappeared from sight, I did what any classic horror movie trope would tell you to do: I went upstairs.\n\nI found Perry standing at the end of the ominous hallway. Large, empty picture frames lined the oxblood walls leading up to him. Through the doorway where Perry stood, particles in the air danced in the projector\u2019s cone-shaped light. That telltale winding of a film reel was the only sound in the deafening quiet of this house that I no longer recognized.\n\nHalf of Perry\u2019s face\u2014the one with the scar\u2014was perfectly illuminated, as though he was wearing the mask from \u201cThe Phantom of the Opera\u201d. \u201cI think you\u2019re ready to see how this movie ends, Jackie. This is the most progress you\u2019ve made since we\u2019ve been coming here.\u201d\n\nI gripped my thumbs in the palms of my hands, \u201cPerry, you\u2019re freaking the fuck out of me!\u201d\n\nI thought my knees might buckle as my brother\u2019s face glitched, like a flash of static snow on a television set. As his face settled back to normal to a deadpan gaze, he disappeared in the innocuous room. I followed, running on nothing but fumes.\n\nClutching the doorway, my mouth fell agape. Perry was gone. I darted to the middle of the room.\n\nAs I frantically searched for my brother, I shielded my eyes with the back of my hand from the projector\u2019s light. And that\u2019s when, from behind me, I heard five words that made my blood run cold, \u201cJackie! Stop the fucking car!\u201d\n\nI was convulsing yet paralyzed. Moving as slowly as cold molasses, I rotated on the spot towards my worst nightmare, shown on 35 mm. On the projector screen was Perry and me in my car, exactly seven hundred and thirty days ago, the day I almost kill\u2014\n\nOh my God.\n\nMy head pounded as fragmented memories surged. The reality of what happened began to crystallize, unrelenting.\n\nMy joints ached and my stomach churned. Clamping a hand over my mouth to stifle a scream, I stumbled down the hallway as it began caving in on itself. The picture frames were sucked into the walls. The floorboards cracked into distorted peaks and valleys. Wooden beams swung down from the ceiling like pendulums. I tried to spit out the chalky grit of disintegrating drywall that made the hallway look like a winter squall.\n\nPanting heavily, I stopped dead in my tracks at the stained glass window. My body trembled with an all-too-familiar dread. Each time I faced this, I wondered if this fall would be the one that would finally end it all.\n\nMaybe it\u2019d be better if it did.\n\nHolding my breath, I threw myself through the glass, my hands clawing the air for dear life. Free falling two stories feels like an eternity when you\u2019re watching the memory of your childhood home fall apart before your very eyes. But when the weight of my body finally made contact with the earth I\u2014\n\nI gasped. The cold air of the hospital room shocked my lungs. I sat up, ripping the suction cups from my face and body. My gown was clinging to me, soaked in sweat. Medical equipment beeped all around me like a metronome.\n\nDr. Lasko, my therapist since the accident, sat across the stark white room, sighing as he rubbed his forehead. He, too, was connected to a plethora of suction cups and wires. He looked a little worn out in the fluorescent overhead lighting. Ever since I was hospitalized and later incarcerated, Dr. Lasko had been helping me delve into my memories, namely the ones that were too excruciating for me to face. And as such, Dr. Lasko had been appearing in the simulations as my brother Perry, the love of my life who died in the car crash, seven hundred and thirty days prior.\n\nDisoriented, I blinked rapidly, the vividness of the memory contrasting sharply with the sterile, geometric ceiling tiles above me.\n\n\u201cI don\u2019t ever want to do that again!\u201d I was venomous.\n\n\u201cJackie,\u201d Dr. Lasko started.\n\n\u201cDon\u2019t start,\u201d I pulled up four fingers for air quotes, \u201c\u2018Jackie, don\u2019t give up. This was the closest you\u2019ve ever come to facing the truth.\u201d\n\nAs the initial burst of adrenal and cortisol left my body, I fell back on my pillow. I was depleted. Quiet rivers flowed down my cheeks.\n\nRemoving his own suction cups, Dr. Lasko approached my bedside and took a seat. He treaded lightly. \u201cJackie, I understand how challenging this is for you, but you did an incredible job today. If we continue making progress like this, there's a real possibility you'll gain your freedom sooner.\u201d\n\nI looked at the well-meaning doctor, but all I could see was Perry. Multicolored confetti fell softly around him like that first November snow. His face was the sun. His eyes reflected the whole world back to me.\n\nPerry.\n\nWith a weak grip, I took Dr. Lasko\u2019s hand. My vocal cords were like sandpaper. \u201cI\u2019ll leave this place one day, doc.\u201d A single tear dripped from my chin onto my collarbone. \u201cBut I\u2019m not sure if I\u2019ll ever be free.\u201d\n\nDr. Lasko didn\u2019t say a word, but I felt him squeeze my hand, just a little bit tighter.\n\nI licked the cracks on my lips as my eyes closed shut, imagining the oaky comfort of bourbon on my tongue. I felt myself drift, and good thing, because I needed the rest. Dr. Lasko and I would be delving into my memories again the following day.\n\nNo matter how masochistic it felt, I vowed to keep showing up for the simulations. Even if I never forgave myself for what I did, at least in my memories, I got to see Perry.","query":"How is the narrator related to Perry","ground_truth":"Perry is the narrator's brother.","response_correct":"Perry is Jackie\u2019s brother.","response_wrong":"Perry is Jackie\u2019s cousin."}
{"document_index":7,"context":"The Emperor is a mind flayer who appears in Baldur's Gate 3. It[note 1] plays a key role in the main story, but its identity is intentionally obscured until later parts of the game, allowing the player to ultimately decide for themselves if they want to know more about it, and whether or not it is trustworthy.\n\n\nContents\nOverview\nIdentity\nPersonal quest\nRecruitment\nRomance\nHistory\nEvents of Baldur's Gate 3\nAct Two finale\nAct Three\nElfsong Tavern\nThe Wyrmway\nEndings\nList of interactions\nConversation scenes\nIdentity revealed\nRegarding Duke Stelmane\nOn conclusion of Visit the Emperor's Old Hideout\nRomance\nAchievements\nGallery\nNotes\nFootnotes\nReferences\nOverview\nIdentity\nThe Emperor plays a key role in the main story of Baldur's Gate 3, and as part of this role its identity and personal background are kept obfuscated for much of the game. It very carefully divulges information that it deems necessary, sometimes arguing that the player is not ready for the answer yet, or that it will reveal specific information in the future.\n\nDuring Acts One and Two, the Emperor only \"meets\" with the player as the Dream Guardian. At the beginning of Act Three, the player finally meets the Emperor face to face, an event which reveals that it is a mind flayer.\n\nThrough all three Acts, the Emperor generally serves as a guide, and unlikely ally to the party, having the means to protect their minds from the influence of the Absolute, through the use of the prisoner within the Astral Prism.\n\n\"Don't let my form deceive you. I am the one that's been protecting you. I am the one that came to you in your dreams. Help me.\n\u2014 The Emperor, during Act 3\nPersonal quest\nAfter reaching the Elfsong Tavern in Act Three, the Emperor will initiate the quest Visit the Emperor's Old Hideout, in which the player can better get to know the Emperor. It discloses some of its past, during its time in the city and from before it became illithid.\n\nRecruitment\nThe Emperor can appear in multiple combat encounters as a controllable ally, a neutral ally, or an enemy. It cannot, however, become a full member of the player's party or camp.\n\nRomance\nThe Emperor can have a romance with the player during Act Three. See Romance.\n\nHistory\nDetails about the Emperor's personal history are intentionally obfuscated during most of the game, but the player has the opportunity to learn more about it through conversations, interactions with other characters, reading books, and completing specific side quests.\n\nIco knownSpells lvl 03.png Act 3 Spoilers! This section reveals details about the story of Baldur's Gate 3.\n\n\nAn Adventurer, I came from Baldur's Gate, though I was never one to be constrained by circumstance. I longed for more.\n\nThat longing brought me to Moonrise Towers on a search for treasure. To a colony of mind flayers who caught me and changed me.\nThe Emperor was once Balduran, an adventurer who founded a coastal village called Grey Harbour. After securing enough money to fund the building of the Wall that led to Baldur's Gate being founded, he felt the call of the sea once more. On the voyage, and following a shipwreck, Balduran made his way to Moonrise Towers in search of fortune. There, he found a coven of mind flayers who infected him with an illithid tadpole. As a record of his interrogation by Enver Gortash during the planning phases of the Absolute Hoax states, he spent ten years under the thrall of the Moonrise Elder Brain.\n\nAfter Balduran was reborn as an illithid and broke free from the Elder Brain the Absolute, it returned to Baldur's Gate, living in the shadows and feeding on the brains of criminals. Initially struggling with its identity as a mind flayer, Balduran eventually embraced its new form.\n\nBalduran's new acceptance of its illithid form caused a wedge to form between it and its close companion, the dragon Ansur. Ansur attempted to kill Balduran as it slept, believing this would be a merciful death. The Emperor sensed the attempt, and in its struggle to protect itself from being murdered, it killed Ansur in self-defence. [1]\n\nAfter Ansur's death, Balduran came to be called the Emperor as it used its newfound psychic influence to rule Baldur's Gate from the shadows. For the next four centuries, it made its haven under the Elfsong tavern, keeping various sentimental knick knacks from its time as Balduran.\n\n\nI had the fortune of meeting Duke Stelmane. We formed a partnership\nDuring those four centuries, it also came to be associated with the Knights of the Shield, a lawful and neutral evil conglomerate of politicians and merchants manipulating events behind the scenes. Duke Stelmane was a major figure of this secret society, acting as the Emperor's envoy while it secretly kept her enthralled. [note 2]\n\nSometime before the events of the game, Enver Gortash and the Dark Urge captured the Emperor, and brought it back under the thrall of the Moonrise Elder Brain, who was now wearing the Crown of Karsus and had become the Netherbrain masquerading as the Absolute. The Netherbrain, sought to have all three Chosen of the Dead Three killed, and specifically picked the Emperor, unbeknown to it, to lead a team of illithids on a nautiloid to search for and steal from the Githyanki the Astral Prism containing their prince, Orpheus.[2]\nEvents of Baldur's Gate 3\nAct Two finale\nMain article: Help Your Protector\nOn the way to Baldur's Gate, the party will be ambushed by a group of Gish'ra warriors while resting at Wyrm's Lookout. Entering the portal to the Astral Prism, the party will hear their Dream Guardian calling out for help. However, when the party reaches them, it is only to discover that the true identity of their visitor is the illithid known as the Emperor.\n\nAfter defending the Emperor, it will explain how it used the power of the Prism and Orpheus to protect the party from the Absolute, and recite to the party its history as an adventurer and finding freedom from the Absolute. The Emperor will offer the party an Astral Touched Tadpole, which causes the user to transform into a partial-illithid. It insists the path of the mind flayer is preferable, regardless of the player's view on them.\n\nThough this may seen contradictory to its previous promise as the Dream Guardian; to ensure the party do not become mind flayers, this promise refers to the player becoming a mind flayer unwillingly because of the Elder Brain. The Emperor is in favour of the player becoming a mind flayer of their own volition and without the influence of the Elder Brain.\n\nAct Three\nElfsong Tavern\nMain article: Visit the Emperor's Old Hideout\nAs the party nears the Elfsong, the Emperor will remark that the tavern is the location of its old hideout. The hideout proper is in the basement, past the Knights of the Shield's hideout. In it, the player will find various sentimental knick knacks from the Emperor's previous life, before becoming an illithid.\n\nAround the room is its old dog Rascal's collar, its favourite recipe (fiddlehead soup), its first adventuring sword, and part of a cutlery set from its mother; the butter knife having been lost during its last shipwreck on the Isle of Balduran, inside the wreck of the Wandering Eye ship.\n\nThere are also some more illithid-adequate items such as chains for its preferred prey - allegedly criminals and lawbreakers - and jars for brains.\n\nThe Wyrmway\nSee also: Wyrmway and The Blade of Frontiers\nOnce the party completes the Wyrmway trials, they will find the corpse of Ansur the Dragon. Interacting with his body will awaken Ansur's spirit, which briefly possesses the player in order to communicate. As Ansur's introduction concludes, he will detect the Emperor within the Astral Prism.\n\nIco knownSpells lvl 03.png Act 3 Spoilers! This section reveals details about the story of Baldur's Gate 3.\n\nAnsur will reveal that the Emperor in fact was formerly Balduran, the founder of Baldur's Gate. Furthermore, he explains that while the Emperor initially did not want to become a mind flayer, it eventually fully embraced its new form, and its comfort with this caused a rift between the Emperor and Ansur. After \"exhausting all possibility of reversing (the Emperor's) condition\", Ansur was agonizing and the Emperor (as seen in the letter on Ansur's body) attempted to convince him to leave. Ansur then attempted to murder the Emperor during its sleep as a mercy killing, and the Emperor killed Ansur in self-defense.\n\nThis development is somewhat foreshadowed when the player first meets The Emperor in their true form, as the song that plays during the encounter is a variation of The Elf Song, which prominently features Balduran in its lyrics.\nEndings\nIco knownSpells lvl 03.png Act 3 Spoilers! This section reveals details about the story of Baldur's Gate 3.\n\nLet the Emperor use the Netherstones\nThe Emperor unless convinced otherwise is mostly concerned with its survival and prosperity. Should the player allow it to wield the Netherstones, it will follow through on destroying the Elder Brain, at the cost of letting it \"assimilate\" with Orpheus.\nIf the player suggests to the Emperor to take control of the Netherbrain, it will mention that the thought of becoming the Absolute did cross its mind. But unless otherwise persuaded, it will refuse, claiming that whoever becomes the leader of the Cult of the Absolute will be in an open war with the Githyanki, which is a war it is not certain it will survive. The Emperor will destroy the Netherbrain, and the parasites within its control in this ending.\nThe Emperor controls the Netherbrain\nIt is also possible, after suggesting it to take control of the Netherbrain, to persuade it. In this scenario, it does not free the player or their party, instead making them mindless thralls and assuming absolute control of them, continuing the Grand Design.\nOrpheus is freed\nIf the player frees Orpheus, the Emperor will abandon the party, and side with the Netherbrain for the sake of its own survival, as it believes that Orpheus will kill it.\nAttack the Emperor\nThe Emperor can be attacked and killed when it first reveals itself to be a mind flayer. This will result in the influence of the Netherbrain taking over control of the party, ending the game.\nList of interactions\nSee Dream Guardian to read about its previous conversations with the player when it was in disguise.\nCharm Person Icon.png Romance Spoilers This section reveals details about romance and may contain mature themes.\n\nPlayers have a limited number of opportunities to interact with the Emperor, and as such, opportunities for conversation are much more limited compared to that of companions.\n\nConversation scenes are available, but only occur during Act 3, after its \"true\" identity is revealed to the player, and all scenes require a long rest to trigger. The Emperor will occasionally also talk to the player as they walk through different locations in Baldur's Gate.\n\nConversation scenes\nKnown conversation opportunities with the Emperor currently include the following cases, but each scene appears to have multiple outcomes that affect the tone of all subsequent conversations.\n\nDepending on the player's choices, the Emperor's behaviour has many possible states. The more the player treats the Emperor like a \"person\", the more it will act as such, compared to other illithids. The more the player treats The Emperor like a monstrosity with hostile intent, the more it will respond to the player with threatening language and visions of it acting like a hostile illithid.\n\nIdentity revealed\nDuring Help Your Protector at the start of Act 3, a conversation is automatically triggered when the player ventures far enough into the Astral Plane. A combat encounter in some form is inevitable from this conversation, and then another set of conversation options are available after the combat resolves. The Emperor will have nothing further to say when this conversation ends, even if the player tries to interact with it further.\n\nRegarding Duke Stelmane\nWhen the player first explores the Rivington area, being in proximity to certain characters or objects will \"inform\" the player about the recent death of Duke Belynne Stelmane. This will trigger a line of ambient commentary from The Emperor. The next time a Long Rest is triggered, the player may trigger a scene discussing The Emperor's reactions in more depth. Certain dialogue choices made during earlier conversations seem to disqualify the player from this scene. If the player does not long rest before completing the quest Visit the Emperor's Old Hideout, this scene will be skipped entirely.\n\nOn conclusion of Visit the Emperor's Old Hideout\nThis scene may be available to trigger (by long resting) after the player completes the quest Visit the Emperor's Old Hideout.\n\nPossible states for this scene appear to vary heavily depending on the player's choices in prior conversation scenes, with the general differentiating factor being the \"attitude\" the player appears to express towards illithids, and towards the Emperor, through their selected options in these prior scenes.\n\nIf the player tried to kill the Emperor in Act One, by choosing the dialogue option \"You do a great impression of a human. But you're not fooling me.\" , the Emperor offers to share memories through a vision. This vision shows Stelmane paralysed in pain, being brainwashed, and turning into the Emperor's puppet. Her face emotionless, and the Emperor puppeteering her gestures to get a sense of company. Such was its true relationship with Duke Stelmane. [note 2]\n\nThe Emperor uses this memory to frighten the player. It gives them orders, and threatens to make them half-illithid even if they refuse.\n\nRomance\nIn terms of game mechanics, it is technically possible to romance the Emperor. [note 3]\n\nIf the player chooses to reject its advances, the Emperor's attitude in conversation will change in a way that appears to be reactively appropriate to the way it was treated. For example, if the option \"Absolutely not, you freak!\" is chosen at any opportunity, the Emperor's treatment of the player takes a much more hostile tone in all future interactions.\n\nPlayers have a limited number of opportunities to interact with the Emperor, and as such, opportunities for romantically-styled interactions are much more limited compared to the other primary companions.\n\nIf the player visits Cr\u00e8che Y'llek prior to the start of Act 3, killing the Dream Guardian will subsequently lock the player out of romancing the Emperor, and from interacting with it in general.\n\nThere are many possible ways to interact with the Emperor in the available conversation scenes. It currently seems that the primary way to unlock \"romantic\" options is by choosing dialogue that generally treats the Emperor more like \"any other person\", and does not show explicit hostility towards its actions, or its illithid characteristics.\n\nThe player does not need to accept the powers of the Astral-Touched Tadpole to unlock this option. The Emperor seems to take offence to destroying the tadpole, but more testing is needed to determine if this has any effect on the available scenes.\n\nThe scene that occurs after completing Visit the Emperor's Old Hideout is generally regarded as the \"primary\" romantic scene. As long as the player is receptive to the Emperor's advances, conclusions to this scene will allow the player to engage in more intimate activities with it.\n\nConversation options that acknowledge this romance (after the primary scene has concluded) appear to exist in a limited number of places. For example, it is possible to tell Raphael \"I don't want any part of this \u2014 the Emperor is my lover.\" during a specific conversation, if initiated after the romance scene has happened.\n\nEngaging in the primary scene has no effect on other ongoing romances, even when romancing Lae'zel, who is generally hostile to illithids.\n\nAchievements\nA-Mind Blown.jpg\nMind Blown\nRomance the Emperor.\n\n\nGallery\nThey called me The Emperor\nThey called me The Emperor\n\n\nThe Emperor feeding on criminals\nThe Emperor feeding on criminals\n\n\nCharacter portrait by Edward Vanderghote\nCharacter portrait by Edward Vanderghote\n\n\nThe Emperor's model\nThe Emperor's model\n\nNotes\nThe Emperor's existence confirms the Dream Guardian as being an illithid influence, albeit in a different way.\nIn Early Access, the Dream Guardian (known then as Dream Visitor) was implied to be a mental manifestation of the player's tadpole, as it eased them towards using their powers more, as well as showing them a future of domination and control.\nIn the Full Release, the Emperor plays a similar role, in the sense that it also encourages the player to expand their potential through using the tadpole's power, but it is much more passive. In addition, its interests seem to be aligned against the Absolute.\nFootnotes\n The Emperor, like other mind flayers, is addressed using the \"it\" pronoun. It is incidentally referred to as \"he\" in-game, and \"they\" in the game's files, possibly due to an oversight, or characters conflating its current and previous identities.\n The Emperor's vision of its control over Belynne Stelmane is corroborated by the 5e module,  Baldur's Gate: Descent into Avernus. In it, Stelmane is described as having a secret, mental battle against a mind flayer. This mind flayer is very likely the Emperor itself, and as a result, puts its entire \"alliance\" with Stelmane into question. It is very possible the Emperor and Stelmane did not have a proper alliance at all, and rather, the Emperor enthralled her for its needs. Whether this was always the case, or if they had a genuine alliance beforehand, isn't fully clear.\n This romance behaves somewhat differently from that with companions, as the Emperor generally cannot be interacted with outside of cutscenes, and romantic progression is limited to the final act of the game.\nReferences\n Dialogue with Ansur.\n The Netherbrain's dialogue to the player at the Morphic Pool.","query":"Who ambushes the party at Wyrm's lookout?","ground_truth":"On the way to Baldur's Gate, the party will be ambushed by a group of Gish'ra warriors while resting at Wyrm's Lookout.","response_correct":"The party is ambushed by Gish'ra warriors at Wyrm's Lookout.","response_wrong":"The party is caught off guard by a band of ill-tempered adventurers at Wyrm's Lookout."}
{"document_index":3,"context":"llmware\n\nBuilding Enterprise RAG Pipelines with Small, Specialized Models\nllmware provides a unified framework for building LLM-based applications (e.g, RAG, Agents), using small, specialized models that can be deployed privately, integrated with enterprise knowledge sources safely and securely, and cost-effectively tuned and adapted for any business process.\n\nllmware has two main components:\n\nRAG Pipeline - integrated components for the full lifecycle of connecting knowledge sources to generative AI models; and\n\n50+ small, specialized models fine-tuned for key tasks in enterprise process automation, including fact-based question-answering, classification, summarization, and extraction.\n\nBy bringing together both of these components, along with integrating leading open source models and underlying technologies, llmware offers a comprehensive set of tools to rapidly build knowledge-based enterprise LLM applications.\n\nMost of our examples can be run without a GPU server - get started right away on your laptop.\n\nJoin us on Discord | Watch Youtube Tutorials | Explore our Model Families on Huggingface\n\nNew to RAG? Check out the Fast Start video series\n\nMulti-Model Agents with SLIM Models - Intro-Video\n\nIntro to SLIM Function Call Models\nCan't wait? Get SLIMs right away:\n\nfrom llmware.models import ModelCatalog\n\nModelCatalog().get_llm_toolkit()  # get all SLIM models, delivered as small, fast quantized tools\nModelCatalog().tool_test_run(\"slim-sentiment-tool\") # see the model in action with test script included\nKey features\nWriting code withllmware is based on a few main concepts:\n\nModel Catalog: Access all models the same way with easy lookup, regardless of underlying implementation.\nLibrary: ingest, organize and index a collection of knowledge at scale - Parse, Text Chunk and Embed.\nQuery: query libraries with mix of text, semantic, hybrid, metadata, and custom filters.\nPrompt with Sources: the easiest way to combine knowledge retrieval with a LLM inference.\nRAG-Optimized Models - 1-7B parameter models designed for RAG workflow integration and running locally.\nSimple-to-Scale Database Options - integrated data stores from laptop to parallelized cluster.\nAgents with Function Calls and SLIM Models\nStart coding - Quick Start for RAG\nWhat's New?\n-Best New Small RAG Model - BLING finetune of Phi-3 - \"bling-phi-3-gguf\" - see the video\n\n-Web Services with Agent Calls for Financial Research - end-to-end scenario - video and example\n\n-Voice Transcription with WhisperCPP - getting_started, using_sample_files, and analysis_use_case with great_speeches_video\n\n-Phi-3 GGUF Streaming Local Chatbot with UI - setup your own Phi-3-gguf chatbot on your laptop in minutes - example with video\n\n-Small, specialized, function-calling Extract Model - introducing slim-extract - video and example\n\n-LLM to Answer Yes\/No questions - introducing slim-boolean model - video and example\n\n-Natural Language Query to CSV End to End example - using slim-sql model - video and example and now using Custom Tables on Postgres example\n\n-Multi-Model Agents with SLIM models - multi-step Agents with SLIMs on CPU - video - example\n\n-OCR Embedded Document Images Example - systematically extract text from images embedded in documents example\n\n-Enhanced Parser Functions for PDF, Word, Powerpoint and Excel - new text-chunking controls and strategies, extract tables, images, header text - example\n\n-Agent Inference Server - set up multi-model Agents over Inference Server example\n\n-GGUF - Getting Started - check out examples - GGUF (example) and Videos video\n\n-Optimizing Accuracy of RAG Prompts - check out example and videos - part I and part II\n\nGetting Started\nStep 1 - Install llmware - pip3 install llmware or pip3 install 'llmware[full]'\n\nnote: starting with v0.3.0, we provide options for a core install (minimal set of dependencies) or full install (adds to the core with wider set of related python libraries).\nStep 2- Go to Examples - Get Started Fast with 100+ 'Cut-and-Paste' Recipes\nStep 3 - Tutorial Videos - check out our Youtube channel for high-impact 5-10 minute tutorials on the latest examples.\n\u270dWorking with the llmware Github repository\nThe llmware repo can be pulled locally to get access to all the examples, or to work directly with the latest version of the llmware code.\n\ngit clone git@github.com:llmware-ai\/llmware.git\nWe have provided a welcome_to_llmware automation script in the root of the repository folder. After cloning:\n\nOn Windows command line: .\\welcome_to_llmware_windows.sh\nOn Mac \/ Linux command line: sh .\/welcome_to_llmware.sh\nAlternatively, if you prefer to complete setup without the welcome automation script, then the next steps include:\n\ninstall requirements.txt - inside the \/llmware path - e.g., pip3 install -r llmware\/requirements.txt\n\ninstall requirements_extras.txt - inside the \/llmware path - e.g., pip3 install -r llmware\/requirements_extras.txt (Depending upon your use case, you may not need all or any of these installs, but some of these will be used in the examples.)\n\nrun examples - copy one or more of the example .py files into the root project path. (We have seen several IDEs that will attempt to run interactively from the nested \/example path, and then not have access to the \/llmware module - the easy fix is to just copy the example you want to run into the root path).\n\ninstall vector db - no-install vector db options include milvus lite, chromadb, faiss and lancedb - which do not require a server install, but do require that you install the python sdk library for that vector db, e.g., pip3 install pymilvus, or pip3 install chromadb. If you look in examples\/Embedding, you will see examples for getting started with various vector DB, and in the root of the repo, you will see easy-to-get-started docker compose scripts for installing milvus, postgres\/pgvector, mongo, qdrant, neo4j, and redis.\n\nNote: we have seen recently issues with Pytorch==2.3 on some platforms - if you run into any issues, we have seen that uninstalling Pytorch and downleveling to Pytorch==2.1 usually solves the problem.\n\nData Store Options\nFast Start: use SQLite3 and ChromaDB (File-based) out-of-the-box - no install required\nSpeed + Scale: use MongoDB (text collection) and Milvus (vector db) - install with Docker Compose\nPostgres: use Postgres for both text collection and vector DB - install with Docker Compose\nMix-and-Match: LLMWare supports 3 text collection databases (Mongo, Postgres, SQLite) and 10 vector databases (Milvus, PGVector-Postgres, Neo4j, Redis, Mongo-Atlas, Qdrant, Faiss, LanceDB, ChromaDB and Pinecone)\nMeet our Models\nSLIM model series: small, specialized models fine-tuned for function calling and multi-step, multi-model Agent workflows.\nDRAGON model series: Production-grade RAG-optimized 6-7B parameter models - \"Delivering RAG on ...\" the leading foundation base models.\nBLING model series: Small CPU-based RAG-optimized, instruct-following 1B-3B parameter models.\nIndustry BERT models: out-of-the-box custom trained sentence transformer embedding models fine-tuned for the following industries: Insurance, Contracts, Asset Management, SEC.\nGGUF Quantization: we provide 'gguf' and 'tool' versions of many SLIM, DRAGON and BLING models, optimized for CPU deployment.\nUsing LLMs and setting-up API keys & secrets\nLLMWare is an open platform and supports a wide range of open source and proprietary models. To use LLMWare, you do not need to use any proprietary LLM - we would encourage you to experiment with SLIM, BLING, DRAGON, Industry-BERT, the GGUF examples, along with bringing in your favorite models from HuggingFace and Sentence Transformers.\n\nIf you would like to use a proprietary model, you will need to provide your own API Keys. API keys and secrets for models, aws, and pinecone can be set-up for use in environment variables or passed directly to method calls.\n\nRoadmap - Where are we going ...\nInterested in contributing to llmware? Information on ways to participate can be found in our Contributors Guide. As with all aspects of this project, contributing is governed by our Code of Conduct.\n\nQuestions and discussions are welcome in our github discussions.\n\nRelease notes and Change Log\nSee also additional deployment\/install release notes in wheel_archives\n\nThursday, June 6 - v0.3.1-WIP\n\nAdded module 3 to Fast Start example series examples 7-9 on Agents & Function Calls\nAdded reranker Jina model for in-memory semantic similarity RAG - see example\nChanges merged into main branch - expected next pypi release at end of week\nTuesday, June 4 - v0.3.0\n\nAdded support for new Milvus Lite embedded 'no-install' database - see example.\nAdded two new SLIM models to catalog and agent processes - 'q-gen' and 'qa-gen'\nUpdated model class instantiation to provide more extensibility to add new classes in different modules\nNew welcome_to_llmware.sh and welcome_to_llmware_windows.sh fast install scripts\nEnhanced Model class base with new configurable post_init and register methods\nCreated InferenceHistory to track global state of all inferences completed\nMultiple improvements and updates to logging at module level\nNote: starting with v0.3.0, pip install provides two options - a base minimal install pip3 install llmware which will support most use cases, and a larger install pip3 install 'llmware[full]' with other commonly-used libraries.\nWednesday, May 22 - v0.2.15\n\nImprovements in Model class handling of Pytorch and Transformers dependencies (just-in-time loading, if needed)\nExpanding API endpoint options and inference server functionality - see new client access options and server_launch\nSaturday, May 18 - v0.2.14\n\nNew OCR image parsing methods with example\nAdding first part of logging improvements (WIP) in Configs and Models.\nNew embedding model added to catalog - industry-bert-loans.\nUpdates to model import methods and configurations.\nSunday, May 12 - v0.2.13\n\nNew GGUF streaming method with basic example and phi3 local chatbot\nSignificant cleanups in ancillary imports and dependencies to reduce install complexity - note: the updated requirements.txt and setup.py files.\nDefensive code to provide informative warning of any missing dependencies in specialized parts of the code, e.g., OCR, Web Parser.\nUpdates of tests, notice and documentation.\nOpenAIConfigs created to support Azure OpenAI.\nSunday, May 5 - v0.2.12 Update\n\nLaunched \"bling-phi-3\" and \"bling-phi-3-gguf\" in ModelCatalog - newest and most accurate BLING\/DRAGON model\nNew long document summarization method using slim-summary-tool example\nNew Office (Powerpoint, Word, Excel) sample files example\nAdded support for Python 3.12\nDeprecated faiss and replaced with 'no-install' chromadb in Fast Start examples\nRefactored Datasets, Graph and Web Services classes\nUpdated Voice parsing with WhisperCPP into Library\nMonday, April 29 - v0.2.11 Update\n\nUpdates to gguf libs for Phi-3 and Llama-3\nAdded Phi-3 example and Llama-3 example and Quantized Versions to Model Catalog\nIntegrated WhisperCPP Model class and prebuilt shared libraries - getting-started-example\nNew voice sample files for testing - example\nImproved CUDA detection on Windows and safety checks for older Mac OS versions\nMonday, April 22 - v0.2.10 Update\n\nUpdates to Agent class to support Natural Language queries of Custom Tables on Postgres example\nNew Agent API endpoint implemented with LLMWare Inference Server and new Agent capabilities example\nTuesday, April 16 - v0.2.9 Update\n\nNew CustomTable class to rapidly create custom DB tables in conjunction with LLM-based workflows.\nEnhanced methods for converting CSV and JSON\/JSONL files into DB tables.\nSee new examples Creating Custom Table example\nTuesday, April 9 - v0.2.8 Update\n\nOffice Parser (Word Docx, Powerpoint PPTX, and Excel XLSX) - multiple improvements - new libs + Python method.\nIncludes: several fixes, improved text chunking controls, header text extraction and configuration options.\nGenerally, new office parser options conform with the new PDF parser options.\nPlease see Office Parsing Configs example\nWednesday, April 3 - v0.2.7 Update\n\nPDF Parser - multiple improvements - new libs + Python methods.\nIncludes: UTF-8 encoding for European languages.\nIncludes: Better text chunking controls, header text extraction and configuration options.\nPlease see PDF Parsing Configs example for more details.\nNote: deprecating support for aarch64-linux (will use 0.2.6 parsers). Full support going forward for Linux Ubuntu20+ on x86_64 + with CUDA.\nFriday, March 22 - v0.2.6 Update\n\nNew SLIM models: summary, extract, xsum, boolean, tags-3b, and combo sentiment-ner.\nNew logit and sampling analytics.\nNew SLIM examples showing how to use the new models.\nThursday, March 14 - v0.2.5 Update\n\nImproved support for GGUF on CUDA (Windows and Linux), with new prebuilt binaries and exception handling.\nEnhanced model configuration options (sampling, temperature, top logit capture).\nAdded full back-level support for Ubuntu 20+ with parsers and GGUF engine.\nSupport for new Anthropic Claude 3 models.\nNew retrieval methods: document_lookup and aggregate_text.\nNew model: bling-stablelm-3b-tool - fast, accurate 3b quantized question-answering model - one of our new favorites.\nWednesday, February 28 - v0.2.4 Update\n\nMajor upgrade of GGUF Generative Model class - support for Stable-LM-3B, CUDA build options, and better control over sampling strategies.\nNote: new GGUF llama.cpp built libs packaged with build starting in v0.2.4.\nImproved GPU support for HF Embedding Models.\nFriday, February 16 - v0.2.3 Update\n\nAdded 10+ embedding models to ModelCatalog - nomic, jina, bge, gte, ember and uae-large.\nUpdated OpenAI support >=1.0 and new text-3 embedding models.\nSLIM model keys and output_values now accessible in ModelCatalog.\nUpdating encodings to 'utf-8-sig' to better handle txt\/csv files with bom.\nSupported Operating Systems: MacOS (Metal and x86), Linux (x86 and aarch64), Windows\n\nnote on Linux: we test most extensively on Ubuntu 22 and now Ubuntu 20 and recommend where possible\nif you need another Linux version, please raise an issue - we will prioritize testing and ensure support.\nSupported Vector Databases: Milvus, Postgres (PGVector), Neo4j, Redis, LanceDB, ChromaDB, Qdrant, FAISS, Pinecone, Mongo Atlas Vector Search\n\nSupported Text Index Databases: MongoDB, Postgres, SQLite\n\nOptional\nDocker\n\nTo enable the OCR parsing capabilities, install Tesseract v5.3.3 and Poppler v23.10.0 native packages.\n\nChange Log\nLatest Updates - 19 Jan 2024 - llmware v0.2.0\n\nAdded new database integration options - Postgres and SQlite\nImproved status update and parser event logging options for parallelized parsing\nSignificant enhancements to interactions between Embedding + Text collection databases\nImproved error exception handling in loading dynamic modules\nLatest Updates - 15 Jan 2024: llmware v0.1.15\n\nEnhancements to dual pass retrieval queries\nExpanded configuration objects and options for endpoint resources\nLatest Updates - 30 Dec 2023: llmware v0.1.14\n\nAdded support for Open Chat inference servers (compatible with OpenAI API)\nImproved capabilities for multiple embedding models and vector DB configurations\nAdded docker-compose install scripts for PGVector and Redis vector databases\nAdded 'bling-tiny-llama' to model catalog\nLatest Updates - 22 Dec 2023: llmware v0.1.13\n\nAdded 3 new vector databases - Postgres (PG Vector), Redis, and Qdrant\n\nImproved support for integrating sentence transformers directly in the model catalog\n\nImprovements in the model catalog attributes\n\nMultiple new Examples in Models & Embeddings, including GGUF, Vector database, and model catalog\n\n17 Dec 2023: llmware v0.1.12\n\ndragon-deci-7b added to catalog - RAG-finetuned model on high-performance new 7B model base from Deci\nNew GGUFGenerativeModel class for easy integration of GGUF Models\nAdding prebuilt llama_cpp \/ ctransformer shared libraries for Mac M1, Mac x86, Linux x86 and Windows\n3 DRAGON models packaged as Q4_K_M GGUF models for CPU laptop use (dragon-mistral-7b, dragon-llama-7b, dragon-yi-6b)\n4 leading open source chat models added to default catalog with Q4_K_M\n8 Dec 2023: llmware v0.1.11\n\nNew fast start examples for high volume Document Ingestion and Embeddings with Milvus.\nNew LLMWare 'Pop up' Inference Server model class and example script.\nNew Invoice Processing example for RAG.\nImproved Windows stack management to support parsing larger documents.\nEnhancing debugging log output mode options for PDF and Office parsers.\n30 Nov 2023: llmware v0.1.10\n\nWindows added as a supported operating system.\nFurther enhancements to native code for stack management.\nMinor defect fixes.\n24 Nov 2023: llmware v0.1.9\n\nMarkdown (.md) files are now parsed and treated as text files.\nPDF and Office parser stack optimizations which should avoid the need to set ulimit -s.\nNew llmware_models_fast_start.py example that allows discovery and selection of all llmware HuggingFace models.\nNative dependencies (shared libraries and dependencies) now included in repo to faciliate local development.\nUpdates to the Status class to support PDF and Office document parsing status updates.\nMinor defect fixes including image block handling in library exports.\n17 Nov 2023: llmware v0.1.8\n\nEnhanced generation performance by allowing each model to specific the trailing space parameter.\nImproved handling for eos_token_id for llama2 and mistral.\nImproved support for Hugging Face dynamic loading\nNew examples with the new llmware DRAGON models.\n14 Nov 2023: llmware v0.1.7\n\nMoved to Python Wheel package format for PyPi distribution to provide seamless installation of native dependencies on all supported platforms.\nModelCatalog enhancements:\nOpenAI update to include newly announced \u2018turbo\u2019 4 and 3.5 models.\nCohere embedding v3 update to include new Cohere embedding models.\nBLING models as out-of-the-box registered options in the catalog. They can be instantiated like any other model, even without the \u201chf=True\u201d flag.\nAbility to register new model names, within existing model classes, with the register method in ModelCatalog.\nPrompt enhancements:\n\u201cevidence_metadata\u201d added to prompt_main output dictionaries allowing prompt_main responses to be plug into the evidence and fact-checking steps without modification.\nAPI key can now be passed directly in a prompt.load_model(model_name, api_key = \u201c[my-api-key]\u201d)\nLLMWareInference Server - Initial delivery:\nNew Class for LLMWareModel which is a wrapper on a custom HF-style API-based model.\nLLMWareInferenceServer is a new class that can be instantiated on a remote (GPU) server to create a testing API-server that can be integrated into any Prompt workflow.\n03 Nov 2023: llmware v0.1.6\n\nUpdated packaging to require mongo-c-driver 1.24.4 to temporarily workaround segmentation fault with mongo-c-driver 1.25.\nUpdates in python code needed in anticipation of future Windows support.\n27 Oct 2023: llmware v0.1.5\n\nFour new example scripts focused on RAG workflows with small, fine-tuned instruct models that run on a laptop (llmware BLING models).\nExpanded options for setting temperature inside a prompt class.\nImprovement in post processing of Hugging Face model generation.\nStreamlined loading of Hugging Face generative models into prompts.\nInitial delivery of a central status class: read\/write of embedding status with a consistent interface for callers.\nEnhanced in-memory dictionary search support for multi-key queries.\nRemoved trailing space in human-bot wrapping to improve generation quality in some fine-tuned models.\nMinor defect fixes, updated test scripts, and version update for Werkzeug to address dependency security alert.\n20 Oct 2023: llmware v0.1.4\n\nGPU support for Hugging Face models.\nDefect fixes and additional test scripts.\n13 Oct 2023: llmware v0.1.3\n\nMongoDB Atlas Vector Search support.\nSupport for authentication using a MongoDB connection string.\nDocument summarization methods.\nImprovements in capturing the model context window automatically and passing changes in the expected output length.\nDataset card and description with lookup by name.\nProcessing time added to model inference usage dictionary.\nAdditional test scripts, examples, and defect fixes.\n06 Oct 2023: llmware v0.1.1\n\nAdded test scripts to the github repository for regression testing.\nMinor defect fixes and version update of Pillow to address dependency security alert.\n02 Oct 2023: llmware v0.1.0 Initial release of llmware to open source!!","query":"What kind of model is the bling-phi-3 model","ground_truth":"The bling-phi-3 model is the newest and most accurate BLING\/DRAGON model. BLING models are small CPU-based RAG-optimized, instruct-following 1B-3B parameter models. DRAGON models are production-grade RAG-optimized 6-7B parameter models - \"Delivering RAG on ...\" the leading foundation base models.","response_correct":"The bling-phi-3 is a BLING model optimized for RAG, designed for efficient CPU use with 1-3 billion parameters. It represents the latest advancement in BLING\/DRAGON models.","response_wrong":"The bling-phi-3 model is part of the DRAGON series, featuring 6-7 billion parameters and optimized for GPU deployment, making it ideal for complex enterprise applications."}
{"document_index":19,"context":"A Survey on Retrieval-Augmented Text Generation for Large Language\nModels\nYizheng Huang\nYork University\nhyz@yorku.ca\nJimmy X. Huang\nYork University\njhuang@yorku.ca\nAbstract\nRetrieval-Augmented Generation (RAG)\nmerges retrieval methods with deep learning\nadvancements to address the static limitations\nof large language models (LLMs) by enabling\nthe dynamic integration of up-to-date external\ninformation. This methodology, focusing\nprimarily on the text domain, provides a\ncost-effective solution to the generation of\nplausible but incorrect responses by LLMs,\nthereby enhancing the accuracy and reliability\nof their outputs through the use of real-world\ndata. As RAG grows in complexity and\nincorporates multiple concepts that can\ninfluence its performance, this paper organizes\nthe RAG paradigm into four categories:\npre-retrieval, retrieval, post-retrieval, and\ngeneration, offering a detailed perspective\nfrom the retrieval viewpoint. It outlines\nRAG\u2019s evolution and discusses the field\u2019s\nprogression through the analysis of significant\nstudies. Additionally, the paper introduces\nevaluation methods for RAG, addressing\nthe challenges faced and proposing future\nresearch directions. By offering an organized\nframework and categorization, the study\naims to consolidate existing research on\nRAG, clarify its technological underpinnings,\nand highlight its potential to broaden the\nadaptability and applications of LLMs.\n1 Introduction\nThe advent of ChatGPT has significantly impacted\nboth academia and industry due to its interactive\ncapabilities and widespread application, establishing itself as a leading artificial intelligence tool\n(Laskar et al., 2023; Jahan et al., 2023; Huang\nand Huang, 2024). At the core of ChatGPT is the\nlarge language model (LLM) GPT-4, as detailed by\n(OpenAI et al., 2023), which has seen numerous\nenhancements to its predecessors, showcasing exceptional abilities in a variety of Natural Language\nProcessing (NLP) tasks (Laskar et al., 2020). Despite these advancements, the adoption of LLMs\nFigure 1: An example of RAG benefits ChatGPT resolves questions that cannot be answered beyond the\nscope of the training data and generates correct results.\nhas highlighted several critical issues primarily due\nto their reliance on extensive datasets. This reliance\nrestricts their ability to incorporate new information post-training, leading to three primary challenges. First, the focus on broad and general data\nto maximize accessibility and applicability results\nin subpar performance in specialized areas. Second,\nthe rapid creation of online data, combined with the\nsignificant resources required for data annotation\nand model training, hinders LLMs\u2019 ability to stay\nupdated. Third, LLMs are susceptible to generating convincing yet inaccurate responses, known as\n\u201challucinations\u201d, which can mislead users.\nAddressing these challenges is crucial for LLMs\nto be effectively utilized across various domains. A\npromising solution is the integration of RetrievalAugmented Generation (RAG) technology, which\nsupplements models by fetching external data in\nresponse to queries, thus ensuring more accurate\nand current outputs. Figure 1 illustrates how RAG\ncan enable ChatGPT to provide precise answers\nbeyond its initial training data.\nSince its introduction by Lewis et al. (Lewis\net al., 2020b) in 2020, RAG technology has undergone significant advancements, particularly influenced by ChatGPT\u2019s success. However, there is a\nnoticeable gap in the literature regarding a thorough\nanalysis of RAG\u2019s mechanisms and the progress\nmade by subsequent studies. Furthermore, the field\nis characterized by diverse research focuses and the\nuse of ambiguous terminology for similar methods,\nleading to confusion. This paper aims to clarify these aspects by offering a structured overview of\nRAG, categorizing various methods, and delivering an in-depth understanding of this research area.\nThis survey will primarily focus on textual applications of RAG, reflecting the current emphasis of\nresearch efforts in this area.\nRAG combines retrieval methods and advanced\ndeep learning to address two main questions: effectively retrieving relevant information and generating accurate responses. The workflow of RAG\nis outlined in Section 2, categorizing the methodologies into pre-retrieval, retrieval, post-retrieval,\nand generation phases. These sections, from 3 to\n6, provide an in-depth analysis of the technologies\nwithin these phases. Section 7 offers summaries of\nthe reviewed studies, along with the retrievers and\ngenerators utilized. Section 8 details the evaluation\nmethodologies for RAG. Section 9 explores future\nresearch directions, concentrating on text-based\nstudies and extending to image and multimodal\ndata considerations. The conclusion is presented in\nSection 10.\nThe contributions of this paper are threefold:\nThis paper offers a comprehensive framework for\nunderstanding the RAG domain, identifying areas\nfor improvement and challenges for future research.\nIt provides a detailed analysis of RAG\u2019s core technologies, examining their strengths in addressing\nretrieval and generation. Additionally, it introduces\nthe evaluation methods used in RAG research, highlighting current challenges and suggesting promising directions for future studies.\n2 RAG Framework\nThe hallucinations are largely attributed to LLMs\u2019\ninability to access up-to-date information. This\nlimitation stems from the models\u2019 reliance on their\ntraining datasets. RAG proposes a solution to this\nissue by supplementing the LLM\u2019s training data\nwith current information from external sources\nthrough a retrieval model, thereby enabling the generation of accurate responses. RAG presents a more\ncost-effective alternative to the extensive training\nand fine-tuning processes typically required for\nLLMs. It allows for the dynamic incorporation\nof fresh information via traditional retrieval methods or pre-trained LMs, without the need to directly\nintegrate this new data into the LLM. This feature\nmakes RAG both flexible and scalable, facilitating its application across different LLMs for various purposes. The information retrieved through\nRAG is derived from real-world data, authored\nby humans, which not only simplifies the generation process but also increases the reliability of\nthe generated responses. Figure 2 represents the\nunified RAG framework with basic workflow and\nparadigm.\nResearch by Khandelwal et al. (Khandelwal\net al., 2020) demonstrates that accessing relevant\ninformation from the training dataset itself can significantly improve LLM performance, highlighting the effectiveness of RAG. Over time, RAG\nhas evolved from a means of providing supplementary information to enabling multiple interactions\nbetween the retrieval and generation components.\nThis involves conducting several rounds of retrieval\nto refine the accuracy of the information retrieved\nand iteratively improve the quality of the generated output. Platforms such as LangChain1\nand\nLlamaIndex2 have modularized the RAG approach,\nenhancing its adaptability and expanding its range\nof applications. Despite these platforms employing\ndiverse methodologies to tackle different aspects of\nRAG\u2014from multiple search iterations to iterative\ngeneration\u2014they maintain adherence to the fundamental RAG workflow. This consistency is crucial\nfor understanding their operation and pinpointing\nopportunities for further development.\n2.1 Basic RAG Workflow\nThe foundational workflow of RAG begins with the\ncreation of an index comprising external sources.\nThis index serves as the basis for retrieving relevant\ninformation through a retriever model based on a\nspecific query. The final step involves a generator\nmodel, which combines the retrieved information\nwith the query to produce the desired output.\n2.1.1 Indexing\nEfficient retrieval begins with comprehensive indexing, where data preparation is key. This stage\ninvolves text normalization processes such as tokenization, stemming, and the removal of stop words\nto enhance the text\u2019s suitability for indexing (Manning et al., 2008). Text segments are then organized\ninto sentences or paragraphs to facilitate more focused searches, allowing for the pinpointing of segments containing pertinent keywords. The integration of deep learning has revolutionized indexing\nthrough the use of pretrained LMs for generating\nsemantic vector representations of texts. These vectors are stored, enabling rapid and precise retrieval from extensive data collections, significantly\nenhancing retrieval efficiency.\n2.1.2 Retrieval\nWhile traditional retrieval methods, such as the\nBM25 algorithm (Hancock-Beaulieu et al., 1996),\nfocus on term frequency and presence for document\nranking, they often overlook the semantic information of queries. Current strategies leverage pretrained LMs like BERT (Devlin et al., 2019), which\ncapture the semantic essence of queries more effectively. These models improve search accuracy by\nconsidering synonyms and the structure of phrases,\nthereby refining document ranking through the detection of semantic similarities. This is typically\nachieved by measuring vector distances between\ndocuments and queries, combining traditional retrieval metrics with semantic understanding to yield\nsearch results that are both relevant and aligned\nwith user intent.\n2.1.3 Generation\nThe generation phase is tasked with producing text\nthat is both relevant to the query and reflective of\nthe information found in the retrieved documents.\nThe usual method involves concatenating the query\nwith the retrieved information, which is then fed\ninto an LLM for text generation (Li et al., 2022).\nAlthough ensuring the generated text\u2019s alignment\nand accuracy with the retrieved content presents\nchallenges, it is also essential to strike a balance between adhering closely to the source material and\ninfusing the output with creativity. The generated\ntext should accurately convey the information from\nthe retrieved documents and align with the query\u2019s\nintent, while also offering the flexibility to introduce new insights or perspectives not explicitly\ncontained within the retrieved data.\n2.2 RAG Paradigm\nThe RAG paradigm organizes research within\nthe domain, offering a straightforward yet robust\nframework to enhance LLM performance. Central to RAG is its search mechanism, crucial for\ngenerating high-quality outcomes. Therefore, this\nparadigm is structured into four main phases from\na retrieval perspective: pre-retrieval, retrieval, postretrieval, and generation. Both single-hop and\nmulti-hop retrieval approaches, encompassing iterative retrieve-generate cycles, follow this four-phase\nstructure. Figure 3 is the taxonomy tree of RAG\u2019s\ncore techniques.\n2.2.1 Pre-Retrieval\nThe pre-retrieval phase of retrieval-augmented generation lays the foundation for successful data and\nquery preparation, ensuring efficient information\nretrieval. This phase includes essential tasks to\nprepare for effective data access.\nIndexing The process starts with indexing, which\nestablishes an organized system to enable fast and\naccurate retrieval of information. The specificity\nof indexing depends on the task and data type.\nFor example, sentence-level indexing is beneficial\nfor question-answering systems to precisely locate\nanswers, while document-level indexing is more\nappropriate for summarizing documents to understand their main concepts and ideas.\nQuery Manipulation After indexing, query manipulation is performed to adjust user queries for\na better match with the indexed data. This involves query reformulation (Jansen et al., 2009;\nYu et al., 2020), which rewrites the query to align\nmore closely with the user\u2019s intention; query expansion (Huang et al., 2013), which extends the query\nto capture more relevant results through synonyms\nor related terms; and query normalization, which\nresolves differences in spelling or terminology for\nconsistent query matching.\nData Modification Data modification is also critical in enhancing retrieval efficiency. This step\nincludes preprocessing techniques like removing\nirrelevant or redundant information to improve the\nquality of results and enriching the data with additional information such as metadata to boost the\nrelevance and diversity of the retrieved content\n(Bevilacqua et al., 2022a).\n2.2.2 Retrieval\nSearch & Ranking The retrieval stage is the\ncombination of search and ranking. It focuses on\nselecting and prioritizing documents from a dataset\nto enhance the quality of the generation model\u2019s\noutputs. This stage employs search algorithms to\nnavigate through the indexed data, finding documents that match a user\u2019s query. After identifying\nrelevant documents, the process of initially ranking\nthese documents starts to sort them according to\ntheir relevance to the query.\n2.2.3 Post-Retrieval\nThe post-retrieval phase serves to refine the initially\nretrieved documents to improve the quality of text\ngeneration. This phase consists of re-ranking and\nfiltering, each aimed at optimizing the document\nselection for the final generation task.\nRe-Ranking In the re-ranking step, the documents previously retrieved are reassessed, scored,\nand reorganized. The objective is to more accurately highlight the documents most relevant to\nthe query and diminish the importance of the less\nrelevant ones. This step involves incorporating additional metrics and external knowledge sources\nto enhance precision. In this context, pre-trained\nmodels with superior accuracy but lower efficiency\ncan be effectively employed due to the limited set\nof candidate documents available (Huang and Hu,\n2009).\nFiltering Filtering aims to remove documents\nthat fail to meet specified quality or relevance\nstandards. This can be done through several approaches, such as establishing a minimum relevance score threshold to exclude documents below\na certain relevance level. Furthermore, the use of\nfeedback from users or prior relevance evaluations\nassists in adjusting the filtering process, guaranteeing that only the most relevant documents are\nretained for text generation (Khattab and Zaharia,\n2020; Huang and Huang, 2023).\n2.2.4 Generation\nThe generation stage is a crucial component of the\nRAG process, responsible for leveraging retrieved\ninformation to enhance the quality of the generated\nresponse. This stage encompasses several sub-steps\naimed at producing content that is readable, engaging, and informative.\nEnhancing At the heart of the generation phase\nis the enhancement step, where the objective is\nto merge the retrieved information with the user\u2019s\nquery to create a coherent and relevant response.\nThis includes the process of elaboration, adding\nextra details to the retrieved content to enrich it. Efforts are focused on improving the output\u2019s quality\nby increasing its clarity, coherence, and stylistic\nappeal through methods such as rephrasing and\nrestructuring. Information from various sources\nis combined to offer a comprehensive perspective,\nand verification is conducted to ensure the accuracy\nand relevance of the content.\nCustomization Customization is an optional\nstep, involving the adjustment of content to align\nwith the user\u2019s specific preferences or the context\nof the request. This tailoring includes adapting the\ncontent to meet the needs of the target audience or\nthe format in which it will be presented and condensing the information to succinctly convey the\nessence of the content. The process also entails\ncreating summaries or abstracts that emphasize the\nkey points or arguments, ensuring the output is both\ninformative and concise.\n3 Pre-Retrieval\n3.1 Indexing\nThe integration of the k-nearest neighbor (kNN)\nalgorithm with pre-trained neural LMs, as demonstrated in kNN-LMs (Khandelwal et al., 2020), represents significant progress in language modeling. This method employs a datastore created from collections of texts, enabling the dynamic retrieval of\ncontextually relevant examples to improve perplexity without necessitating additional training.\nKnown for its efficiency, FAISS (Johnson et al.,\n2021) has been adopted in many studies for indexing purposes (Khandelwal et al., 2020; Lewis et al.,\n2020b; Khattab et al., 2022). Some research integrates enhancements like the Hierarchical Navigable Small World (HNSW) approximation (Malkov\nand Yashunin, 2020) to achieve faster retrieval\n(Lewis et al., 2020b). In addition, alternative tools\nlike utilizing the Bing API 3\nfor indexing based\non actual user search histories as outlined in Webgpt (Nakano et al., 2021), illustrate the variety of\nindexing techniques under investigation.\nFurthermore, MEMWALKER (Chen et al.,\n2023a) introduces an innovative method to overcome the limitations of context window size in\nLLMs by creating a memory tree from the input\ntext. This tree is formed by initially segmenting\nthe text into smaller pieces and then summarizing these segments into a hierarchical structure of\nsummary nodes, facilitating efficient indexing and\nmanagement of large volumes of information.\n3.2 Query Manipulation\nStudies such as FiD (Izacard and Grave, 2021),\nCOK(Li et al., 2023), and Query2doc (Wang et al.,\n2023a) emphasize the significance of creating new\nqueries or refining existing ones to achieve more\n3\nhttps:\/\/www.microsoft.com\/en-us\/bing\/apis\/bing-websearch-api\npertinent retrieval results. These research efforts\nhighlight the necessity of efficiently gathering evidence from multiple passages and tailoring queries\nto suit various knowledge sources, whether structured or unstructured. Techniques ranging from the\ncreation of pseudo-documents to enhance queries\nhave shown to bolster retrieval performance across\ndiverse information retrieval datasets.\nFurther exploration into query manipulation has\nbeen conducted by Step-Back (Zheng et al., 2023)\nand PROMPTAGATOR (Dai et al., 2023), which\nfocus on abstracting high-level concepts or utilizing LLMs for prompt-based query generation.\nThese strategies strive to better align queries with\nthe retrieval system\u2019s functionality by rephrasing\ntasks into more generalized versions or crafting\ntask-specific queries from limited examples. Such\nmethodologies enhance the consistency between\nqueries and indexed data, facilitating the retrieval\nof more pertinent and insightful information.\nMoreover, KnowledGPT (Wang et al., 2023b)\nand Rewrite-Retrieve-Read (Ma et al., 2023) introduce approaches for query manipulation through\n\u201cprogram of thought\u201d prompting and innovative\nquery rewriting techniques. KnowledGPT innovates by generating code to interface with knowledge bases, converting user queries into structured\nsearch commands. In contrast, Rewrite-RetrieveRead utilizes a trainable compact LM for query\nreformulation, adjusting them to more effectively\nreflect the user\u2019s intent and context.\nLastly, FLARE (Jiang et al., 2023) presents a\nstrategy based on confidence for query formulation, which focuses on crafting queries that precisely\nreflect the information needs. This method incorporates the use of generated sentences or fragments\nthereof as a foundation for search queries. By opting to directly use sentences, obscuring tokens of\nlow confidence, or formulating explicit questions,\nthis approach aims to boost the efficiency of the\nretrieval process, ensuring that the retrieved information faithfully satisfies the requirements of the\ngeneration process.\n3.3 Data Modification\nRA-DIT (Lin et al., 2023b) and RECITE (Sun et al.,\n2023) emphasize enhancements through internal\ndata modifications. RA-DIT distinguishes between\nfine-tuning datasets for LLMs and retrievers, aiming to bolster the LLM\u2019s contextual comprehension\nand the retriever\u2019s ability to align with queries. RECITE, on the other hand, utilizes passage hints and\nsynthetic question-passage pairs to increase the\nvariety and relevance of its generated recitations\nand responses. This approach seeks to broaden the\nmodel\u2019s knowledge base and improve its response\naccuracy.\nUPRISE (Cheng et al., 2023a) and GENREAD\n(Yu et al., 2023a) target the refinement of external\ndata. UPRISE converts raw task data into a structured format and refines the selection of prompts\nto enhance retrieval outcomes. In contrast, the\nClustering-Based Prompts method employed by\nGENREAD generates documents from questions\nand clusters them to eliminate irrelevant data, enriching the input with varied contextual insights.\nThis technique aims to improve the performance of\nthe generative model by providing it with a richer\nset of information.\nFurthermore, KnowledGPT (Wang et al., 2023b)\nis dedicated to augmenting raw text data with structured, semantically rich information through entity\nlinking. This enrichment process not only structures the data more cohesively and makes it more\namenable to queries but also boosts the model\u2019s\nretrieval efficiency. It leverages precise, linked\nknowledge to enhance the model\u2019s understanding and its ability to generate relevant responses,\nthereby improving its overall performance.\n4 Retrieval\n4.1 Search & Ranking\nAtlas (Izacard et al., 2023) investigates few-shot\nlearning approaches, including Attention Distillation and Perplexity Distillation, to steer the retriever toward retrieving more relevant documents.\nIRCOT (Trivedi et al., 2023) integrates retrieval\nwith reasoning to improve the effectiveness of retrieval. SURGE (Kang et al., 2023) employs a\nsubgraph retriever to extract relevant subgraphs\nfrom a knowledge graph, while AAR (Yu et al.,\n2023b) modifies search preferences to help LLMs\nin fetching pertinent documents.\nPRCA (Yang et al., 2023a) focuses on employing domain-specific abstractive summarization to\nextract relevant and context-rich information from\ndocuments, using a supervised learning strategy\nto prioritize content crucial for accurate query responses. Meanwhile, MEMWALKER (Chen et al.,\n2023a) leverages an internal search and ranking\nmechanism in the constructed memory tree to identify pertinent information for long-context question\nanswering. Additionally, the Confidence-based Active Retrieval approach of FLARE (Jiang et al.,\n2023) dynamically triggers information retrieval\nbased on the confidence levels of generated sentences, utilizing the insight that low-confidence\ntokens signal a need for external knowledge.\n5 Post-Retrieval\n5.1 Re-Ranking\nRe2G (Glass et al., 2022) introduces a sequencepair classification approach for re-ranking, utilizing a BERT transformer to simultaneously analyze\nthe query and passage. This interaction model, employing cross-attention between sequences, offers a\ncontrast to the representation model typically used\nin initial retrieval phases. PROMPTAGATOR (Dai\net al., 2023) also employs a cross-attention model\nfor re-scoring. Its \u201cLift Yourself Up\u201d strategy iteratively selects the best candidate from a pool for\nfurther generation rounds, progressively improving\ncontent quality via self-generated content.\nRe-ranking is also a significant focus of InContext RALM (Ram et al., 2023). Two approaches to reranking are explored: zero-shot\nreranking using language models and predictive\nreranking through trained models. This step is\naimed at refining the selection of documents based\non their expected utility for improving language\nmodel performance. ITER-RETGEN (Shao et al.,\n2023), in particular, leverages knowledge distillation from the re-ranker to the dense retriever, finetuning retrieval efforts based on relevance signals\nfrom LLM outputs. This optimization of the retrieval model aims to more accurately capture query\nnuances, thereby improving document selection.\nDKS-RAC (Huang et al., 2023) presents the\nDense Knowledge Similarity (DKS) for aligning\nthe knowledge between answers and retrieved passages at the sequence level. This approach is categorized under re-ranking due to its direct impact on\npassage selection based on knowledge similarity,\nrefining the match between queries and documents.\nFiD-light (Hofst\u00e4tter et al., 2023) introduces a\nlistwise autoregressive re-ranking method that employs source pointers to optimize the ranking order.\nThis method maintains a link between the generated text and source passages, enabling a more\nstructured generation process. By incorporating\ntextual citations within the model\u2019s output as pointers to relevant information sources, this approach\nfacilitates an organized retrieval and generation\nprocess, enhancing the overall coherence and relevance of the generated content.\n5.2 Filtering\nCOK (Li et al., 2023) presents the Progressive Rationale Correction technique, aimed at iteratively\nrefining rationales with retrieved knowledge. This\nmethod constitutes a continuous optimization process, significantly enhancing the relevance and\nquality of information used in content generation.\nSelf-RAG (Asai et al., 2023) introduces a selfreflection mechanism to efficiently filter out irrelevant content. By employing critique tokens, this\napproach evaluates the relevance, supportiveness,\nand utility of retrieved passages, ensuring the integration of only high-quality information into the\ncontent generation process.\nAdditionally, FiD-TF (Berchansky et al., 2023)\nand RECOMP (Xu et al., 2023) are dedicated to the\nremoval of irrelevant or redundant tokens and information from retrieved documents. FiD-TF employs\na dynamic mechanism to identify and eliminate unnecessary tokens, enhancing the efficiency of information processing. RECOMP, on the other hand,\ncompresses documents into concise summaries, focusing on selecting only the most pertinent content\nfor the generation process. These methods streamline the content generation workflow by ensuring\nthat only relevant and supportive information is\nutilized, thereby improving the overall quality and\nrelevance of the generated content.\n6 Generation\n6.1 Enhancing\nDSP (Khattab et al., 2022) introduces a framework\ndesigned to generate multiple retrieval queries to\nsummarize and answer questions, drawing upon information aggregated from various passages. This\nframework employs CombSUM (Fox and Shaw,\n1994) to calculate a cumulative probability score\nfor passages across different retrieval lists, facilitating the compilation of a comprehensive response\nfrom multiple sources.\nPRCA (Yang et al., 2023a) outlines a RewardDriven Stage, wherein the distilled context is refined based on feedback from the generator. Utilizing reinforcement learning, this stage adjusts\nthe parameters of PRCA according to the rewards\nreceived for providing relevant context. The objective is to fine-tune the extracted context to meet\nthe specific requirements of the generator, thereby\noptimizing the generation process.\nREPLUG (Shi et al., 2023) proposes a method\nfor prepending retrieved documents to the input\ncontext before the final prediction by the black-box\nLM. It introduces an ensemble strategy to encode\nretrieved documents in parallel, overcoming the\nlimitations of LM context length and enhancing\naccuracy through the allocation of increased computational resources. This approach improves the\ngeneration process by ensuring that the LM has\naccess to a broader range of relevant information.\nRECITE (Sun et al., 2023) implements a selfconsistency technique, which involves generating\nmultiple recitations independently and employing\na plurality\/majority vote system to determine the\nmost appropriate answer. This method is designed\nto increase the reliability and accuracy of the answers, thereby improving the quality and credibility\nof the output.\n6.2 Customization\nThe PKG framework, introduced by (Luo et al.,\n2023), represents an approach to customizing the\noutput of LMs. By generating background knowledge internally using a pre-trained model, PKG\neliminates the need for traditional external retrieval\nprocesses. This method directly integrates domainor task-specific knowledge into the generation step,\nsignificantly enhancing the LM\u2019s capacity to produce responses that are specifically tailored to the\ngiven context or requirements.\nSelf-RAG (Asai et al., 2023) offers a strategy that incorporates reflection tokens within a customizable decoding algorithm. This technique permits dynamic adjustment of the model\u2019s retrieval\nand generation behaviors based on the specific task,\nfacilitating more versatile response generation. Depending on the requirements, this approach can be\ntuned for accuracy or creativity, providing flexibility in generating outputs that meet diverse needs.\nSURGE (Kang et al., 2023) achieves customization through the application of graph-text contrastive learning. This method ensures that the\ngenerated dialogue responses are in tight alignment\nwith the knowledge contained in the retrieved subgraph, yielding responses that are specific, relevant,\nand deeply rooted in the dialogue context. By maintaining consistency between the retrieved knowledge and the generated text, SURGE is capable\nof producing outputs that precisely reflect the detailed knowledge of the subgraph, enhancing the\nrelevance and specificity of the responses.\n7 Comparisons of RAG\n7.1 The Comprehensive Summary of RAG\nTable 1 presents a detailed analysis of the RAG\nstudies discussed in this paper. The analysis shows\nthat the majority of these studies have utilized external data sources to enrich the content of LLMs.\nA preference for multiple-hop over single-hop retrieval was noted, indicating that iterative search\nrounds generally yield superior results. In other\nwords, most methods employ dense retrieval to secure higher quality candidate documents. Compared to modifying datasets in the pre-retrieval\nstage, more studies focus on manipulating the query\nto improve retrieval performance. Additionally,\nthere is a significant emphasis on optimizing the\nretrieval phase, highlighting its crucial role in the\nresearch. However, there seems to be a scarcity\nof studies concentrating on customization in the\ngeneration stage, pointing to this as a potential area\nfor future exploration. Overall, while the goal of\nRAG is to enhance the response quality of LLMs,\ngreater efforts have been directed towards improving retrieval aspects.\n7.2 Retriever and Generator\nIn RAG, the retriever and the generator are the\nprimary components. Table 2 summarizes the retrievers and generators used in the studies discussed\nin this paper. It is clear from the table that while\nmost generators utilize advanced language models,\na significant number of retrievers still employ the\ntraditional BM25 due to its efficiency. The method\nof retrieval is a crucial aspect in RAG, highlighting the importance of exploring ways to enhance\nretrieval performance without compromising efficiency. Similarly, not many studies have adopted\npowerful LLMs such as LLaMA2, GPT-3.5, or\nGPT-4 as their generators. LLMs like T5 remain\npopular, yet fundamental models like BERT and\nTransformers are rarely used in 2023. Compared\nto generators, it is evident that not many IR-based\nLLMs are used in retrievers, indicating a promising\ndirection for developing such models in the future.\n8 Evaluation in RAG\nTo understand the effectiveness of LMs in generating more accurate, relevant, and robust responses\nby leveraging external knowledge, the evaluation\nof RAG systems has become a significant research\narea. With the popularity of dialogue-based interactions, recent works have been focused on assessing\nthe performance of RAG models on such downstream tasks using established metrics like Exact\nMatch (EM) and F1 scores. Furthermore, a wide\narray of datasets has been utilized for this purpose,\nincluding TriviaQA (Joshi et al., 2017), HotpotQA\n(Yang et al., 2018), FEVER (Thorne et al., 2018),\nNatural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al., 2019), and T-REX\n(ElSahar et al., 2018).\nHowever, evaluation solely from the perspective of downstream tasks falls short in addressing\nthe evolving needs of RAG development. Recent\nresearch has introduced various frameworks and\nbenchmarks that aim to evaluate these systems\nacross multiple dimensions, including the quality\nof the generated text, the relevance of retrieved\ndocuments, and the model\u2019s resilience to misinformation, as shown in Table 3. These evaluations focus on assessing specific capabilities such as noise\nrobustness, negative prompting, information integration, and counterfactual robustness, highlighting the complex challenges faced by RAG systems\nin practical applications. The continuous development of evaluation frameworks and metrics is\ncrucial for advancing the field, broadening the applicability of RAG systems, and ensuring they meet\nthe demands of a complex and evolving information landscape.\n8.1 Retrieval-based Aspect\nIn information retrieval, the quality of search results is typically evaluated using standard metrics\nsuch as Mean Average Precision (MAP), Precision,\nReciprocal Rank, and Normalized Discounted Cumulative Gain (NDCG) (Radlinski and Craswell,\n2010; Reimers and Gurevych, 2019; Nogueira et al.,\n2019). These metrics primarily assess the relevance\nof retrieved documents to a given query.\nRetrieval-based Metrics in RAG focus on the effectiveness of retrieving relevant information to\nsupport generation tasks. These include Accuracy, which measures the precision of retrieved\ndocuments in providing correct information for answering queries, and Rejection Rate (Chen et al.,\n2023b), assessing a system\u2019s ability to decline answering when no relevant information is found.\nAdditionally, Error Detection Rate (Chen et al.,\n2023b) evaluates the model\u2019s capability to identify\nand disregard incorrect or misleading information\nfrom retrieved documents. Context Relevance is\nanother essential metric, assessing the pertinence\nof the retrieved documents to the query. It\u2019s vital to\nensure the information used to generate responses\nis directly related to the query\u2019s context. Faithfulness (Shahul et al., 2023) measures the accuracy\nwith which the generated content reflects the information in the retrieved documents, ensuring that\nthe generation process with no misinformation.\n8.2 Generation-based Aspect\nEvaluating the quality of text produced by LLMs\ninvolves analyzing their performance on various\ndownstream tasks using standard metrics. These\nmetrics assess linguistic quality, coherence, accuracy, and the extent to which the generated text\nreflects ground-truth data. Linguistic quality and\ncoherence are evaluated through metrics such as\nBLEU (Papineni et al., 2002), which measures fluency and similarity to human-produced text, and\nROUGE-L (Lin, 2004), which quantifies the overlap with reference summaries to gauge the text\u2019s\ncapacity to encapsulate main ideas and phrases.\nAccuracy and overlap with ground-truth data are\ngauged using metrics like EM and F1 Score, which\nrespectively determine the percentage of answers\nthat are entirely correct and offer a balanced assessment of precision and recall in retrieving relevant\nanswers while minimizing inaccuracies.\nBeyond these standard metrics, the evaluation\nmay also incorporate task-specific criteria and\nnovel metrics tailored to particular applications.\nFor instance, in dialogue generation, perplexity\nand entropy are used to evaluate response diversity and naturalness. Additionally, metrics such as\nMisleading Rate and Mistake Reappearance Rate\n(Liu et al., 2023) gauge a model\u2019s ability to avoid\nmisinformation and inaccuracies. Other specialized metrics include Answer Relevance (Shahul\net al., 2023), assessing the precision of responses\nto queries; Kendall\u2019s tau (Saad-Falcon et al., 2023),\nfor evaluating the accuracy of RAG system rankings; Micro-F1 (Saad-Falcon et al., 2023), which\nfine-tunes accuracy evaluation in tasks with multiple correct answers; and Prediction Accuracy, directly measuring the alignment of generated answers with expected responses, thereby offering a\ndirect insight into a system\u2019s effectiveness in generating accurate content.\n9 Future Directions\n9.1 Retrieval Quality\nThe integration of RAG into LLMs faces significant\nhurdles due to the vast amounts of unreliable information on the internet, including fake news. This\npresents a challenge for accurately retrieving useful\nknowledge, leading to the unreliable generation of\nresponses by LLMs. As a result, LLMs may generate content based on incorrect information, undermining their reliability. Recent research efforts\nare directed towards enhancing retrieval methods\nto improve the efficiency, scalability, and effectiveness of LLMs in generating accurate and reliable\nresponses.\nDifferentiable Search Indices (Tay et al., 2022)\nand (Bevilacqua et al., 2022b) developed differentiable search indices that integrate the retrieval\nprocess within a Transformer model, enabling direct mapping of text queries to document identifiers.\nThese approaches offer superior performance and\npotential for more efficient and scalable retrieval.\nGenerative Models for Search GERE (Chen\net al., 2022a) can directly generate document titles\nand evidence sentences for fact-verification tasks.\nPARADE (Li et al., 2024) is a method for document\nreranking that aggregates passage representations\ninto a unified document relevance score. Both of\nthem demonstrate significant improvements in retrieval quality over traditional methods.\nFine-tuning Pre-trained Language Models\nRankT5 (Zhuang et al., 2023) is a model that finetunes the T5 framework specifically for text ranking. It leverages ranking losses to optimize performance metrics and exhibits promising zero-shot\nperformance on out-of-domain data.\nNoise Power (Cuconasu et al., 2024) provide a\ncomprehensive analysis of the impact of IR components on RAG systems, revealing that the inclusion\nof irrelevant documents can significantly improve\naccuracy. It challenges conventional retrieval strategies and underscores the potential for developing\nspecialized approaches that integrate retrieval with\nlanguage generation models.\n9.2 Multimodal RAG\nThe multimodal RAG domain has experienced significant growth, highlighting a pivotal advancement\nat the confluence of text and visual comprehension.\nThe introduction of MuRAG (Chen et al., 2022b)\nmarked a breakthrough by amalgamating textual\nand visual information for language generation, establishing a new standard for multimodal datasets.\nThis model showcased the efficacy of utilizing a\nmultimodal memory system to boost the accuracy\nin question-answering and reasoning tasks.\nAfter MuRAG, studies such as REVEAL (Hu\net al., 2023) and Re-Imagen (Chen et al., 2023c)\nhave focused on enhancing visual question answering and text-to-image generation. They achieved\nthis through the incorporation of dynamic retrieval\nmechanisms and the improvement of image fidelity,\nrespectively. These advancements laid the groundwork for further models by researchers like Sarto\net al. (Sarto et al., 2022) for image captioning,\nand Yuan et al. (Yuan et al., 2023) for text-to-audio\ngeneration, broadening the scope of RAG\u2019s application across different modalities and improving the\nquality and realism of the generated outputs. Furthermore, Re-ViLM (Yang et al., 2023b) refined\nimage captioning capabilities through a retrievalaugmented visual language model. By fine-tuning\nmodel parameters and implementing innovative filtering strategies, it has made strides in producing\nmore precise and contextually appropriate captions.\nBy tapping into external resources, these models\nhave provided significant enhancements over traditional benchmarks, highlighting the advantage of\nintegrating diverse sources of knowledge.\n10 Conclusions\nIn this paper, we have presented a comprehensive framework for understanding the RAG domain, highlighting its significance in enhancing\nthe capabilities of LLMs. Through a structured\noverview of RAG, categorizing various methods,\nand an in-depth analysis of its core technologies\nand evaluation methods, this study illuminates the\npath for future research. It identifies crucial areas\nfor improvement and outlines potential directions\nfor advancing RAG applications, especially in textual contexts. This survey aims to elucidate the\ncore concepts of the RAG field from a retrieval\nperspective, and it is intended to facilitate further\nexploration and innovation in the accurate retrieval\nand generation of information.\n11 Limitations\nThis survey comprehensively examines existing\nRAG models, summarizing their core techniques\ninto four main steps from a retrieval perspective. It\nrecognizes that some methods may encompass multiple steps and that decoupling these steps could\npotentially obscure their intrinsic connections. Nevertheless, the primary objective is to simplify the\ncomplexity of the approach, clearly delineating the\nspecific problems it addresses. This allows for a\nclearer identification of areas ripe for further optimization and improvement. Despite the thorough investigation, the rapid evolution of the field and\npage limits mean that certain aspects might not\nhave been fully analyzed and explored, or recent\ndevelopments could have been missed. While the\npaper references evaluation methods that can aid\nin the development of RAG, it also acknowledges\nmature tools like LangChain and LlamaIndex as\nuseful resources. However, the focus of this survey\nis not on detailing the evaluation pipeline or how\nthese tools are specifically used, but rather on illustrating how evaluation aspects can support the\nadvancement of RAG. This choice highlights an\narea for future work, emphasizing the importance\nof methodological clarity and the application of\nevaluation tools in refining and enhancing RAG\nmodels.\nAcknowledgements\nThis work was supported by the Natural Sciences\nand Engineering Research Council (NSERC) of\nCanada and the York Research Chairs (YRC) program.\nAcknowledgements\nThis work was supported by the Natural Sciences\nand Engineering Research Council (NSERC) of\nCanada and the York Research Chairs (YRC) program.\nReferences\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2023. Self-RAG: Learning\nto Retrieve, Generate, and Critique through SelfReflection. arXiv, abs\/2310.11511.\nMoshe Berchansky, Peter Izsak, Avi Caciularu, Ido\nDagan, and Moshe Wasserblat. 2023. Optimizing\nRetrieval-augmented Reader Models via Token Elimination. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1506\u20131524. Association for Computational\nLinguistics.\nMichele Bevilacqua, Giuseppe Ottaviano, Patrick S. H.\nLewis, Scott Yih, Sebastian Riedel, and Fabio Petroni.\n2022a. Autoregressive search engines: Generating\nsubstrings as document identifiers. In Advances in\nNeural Information Processing Systems 35: Annual\nConference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA,\nNovember 28 - December 9, 2022.\nMichele Bevilacqua, Giuseppe Ottaviano, Patrick S. H.\nLewis, Scott Yih, Sebastian Riedel, and Fabio Petroni.\n2022b. Autoregressive Search Engines: Generating Substrings as Document Identifiers. In Conference on Neural Information Processing Systems\n(NeurIPS).\nSid Black, Gao Leo, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with MeshTensorflow.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen\nSimonyan, Jack W. Rae, Erich Elsen, and Laurent\nSifre. 2022. Improving Language Models by Retrieving from Trillions of Tokens. In International Conference on Machine Learning (ICML), pages 2206\u2013\n2240.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language Models are Few-Shot Learners. In\nConference on Neural Information Processing Systems (NeurIPS), volume abs\/2005.14165.\nHoward Chen, Ramakanth Pasunuru, Jason Weston, and\nAsli Celikyilmaz. 2023a. Walking Down the Memory\nMaze: Beyond Context Limit through Interactive\nReading. arXiv, abs\/2310.05029.\nJiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan,\nand Xueqi Cheng. 2022a. Gere: Generative Evidence\nRetrieval for Fact Verification. In Proceedings of\nthe 45th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval.\nACM.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023b. Benchmarking large language models in retrieval-augmented generation. arXiv,\nabs\/2309.01431.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, and others. 2021. Evaluating\nlarge language models trained on code. arXiv,\nabs\/2107.03374.\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga,\nand William Cohen. 2022b. Murag: Multimodal\nRetrieval-Augmented Generator for Open Question\nAnswering over Images and Text. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nWenhu Chen, Hexiang Hu, Chitwan Saharia, and\nWilliam W. Cohen. 2023c. Re-Imagen: RetrievalAugmented Text-to-Image Generator. In International Conference on Learning Representations\n(ICLR).\nZhihong Chen, Feng Jiang, Junying Chen, Tiannan\nWang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao\nLiang, Chen Zhang, Zhiyi Zhang, and others. 2023d.\nPhoenix: Democratizing chatgpt across languages.\narXiv, abs\/2304.10453.\nDaixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng\nZhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu\nWei, Weiwei Deng, and Qi Zhang. 2023a. Uprise:\nUniversal Prompt Retrieval for Improving Zero-Shot\nEvaluation. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Processing, pages 12318\u201312337. Association for Computational Linguistics.\nXin Cheng, Di Luo, Xiuying Chen, Lemao Liu,\nDongyan Zhao, and Rui Yan. 2023b. Lift Yourself Up: Retrieval-augmented Text Generation with\nSelf-Memory. In Thirty-seventh Conference on\nNeural Information Processing Systems, volume\nabs\/2305.02437.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2023. Palm: Scaling Language\nModeling with Pathways. Journal of Machine Learning Research (JMLR), 24:240:1\u2013240:113.\nHyung Won Chung, Le Hou, S. Longpre, Barret\nZoph, Yi Tay, W. Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Webson, S. Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen,\nAakanksha Chowdhery, Dasha Valter, Sharan Narang,\nGaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, E. Chi, J. Dean, Jacob Devlin, Adam Roberts,\nDenny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. arXiv,\nabs\/2210.11416.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u2013\n8451. Association for Computational Linguistics.\nFlorin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle\nMaarek, Nicola Tonellotto, and Fabrizio Silvestri.\n2024. The Power of Noise: Redefining Retrieval for\nRAG Systems. arXiv, abs\/2401.14887.\nZhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B.\nHall, and Ming-Wei Chang. 2023. Promptagator:\nFew-shot Dense Retrieval From 8 Examples. In International Conference on Learning Representations\n(ICLR).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of Deep\nBidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the\nNorth, pages 4171\u20134186. Association for Computational Linguistics.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof Wikipedia: Knowledge-Powered Conversational\nAgents. In International Conference on Learning\nRepresentations (ICLR).\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). Association for\nComputational Linguistics.\nHady ElSahar, Pavlos Vougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon S. Hare, Fr\u00e9d\u00e9rique\nLaforest, and Elena Simperl. 2018. T-REx: A Large\nScale Alignment of Natural Language with Knowledge Base Triples. In International Conference on\nLanguage Resources and Evaluation (LREC).\nZhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2023. Retrieval-generation\nsynergy augmented large language models. arXiv,\nabs\/2310.05149.\nEdward A. Fox and Joseph A. Shaw. 1994. Combination of multiple searches. In TREC-2: Text retrieval\nconference, 500215, pages 105\u2013108.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple Contrastive Learning of Sentence\nEmbeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, pages 6894\u20136910. Association for Computational Linguistics.\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\nChowdhury, Ankita Naik, Pengshan Cai, and Alfio\nGliozzo. 2022. Re2g: Retrieve, Rerank, Generate.\nIn Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\npages 2701\u20132715. Association for Computational\nLinguistics.\nSimon Gottschalk and Elena Demidova. 2018. EventKG: A Multilingual Event-Centric Temporal Knowledge Graph. Springer International Publishing.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. Retrieval Augmented\nLanguage Model Pre-Training. In International Conference on Machine Learning (ICML), pages 3929\u2013\n3938.\nWilliam L. Hamilton. 2020. Graph representation learning. Springer International Publishing.\nMicheline Hancock-Beaulieu, Mike Gatford, Xiangji\nHuang, Stephen E. Robertson, Steve Walker, and\nP. W. Williams. 1996. Okapi at TREC-5. In Proceedings of The Fifth Text REtrieval Conference, TREC\n1996, Gaithersburg, Maryland, USA, November 20-\n22, 1996, volume 500-238 of NIST Special Publication. National Institute of Standards and Technology\n(NIST).\nSebastian Hofst\u00e4tter, Jiecao Chen, Karthik Raman, and\nHamed Zamani. 2023. Fid-light: Efficient and effective retrieval-augmented text generation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information\nRetrieval, pages 1437\u20131447.\nZiniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang,\nKai-Wei Chang, Yizhou Sun, Cordelia Schmid,\nDavid A. Ross, and Alireza Fathi. 2023. Reveal:\nRetrieval-Augmented Visual-Language Pre-Training\nwith Multi-Source Multimodal Knowledge Memory. In 2023 IEEE\/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 23369\u2013\n23379. IEEE.\nJie Huang, Hanyin Shao, Kevin Chen-Chuan Chang,\nJinjun Xiong, and Wen-mei Hwu. 2022. Understanding Jargon: Combining Extraction and Generation\nfor Definition Modeling. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing. Association for Computational\nLinguistics.\nJimmy Xiangji Huang, Jun Miao, and Ben He. 2013.\nHigh performance query expansion using adaptive\nco-training. Inf. Process. Manag., 49(2):441\u2013453.\nWenyu Huang, Mirella Lapata, Pavlos Vougiouklis,\nNikos Papasarantopoulos, and Jeff Z Pan. 2023. Retrieval Augmented Generation with Rich Answer Encoding. Proc. of IJCNLP-AACL, 2023.\nXiangji Huang and Qinmin Hu. 2009. A bayesian learning approach to promoting diversity in ranking for\nbiomedical information retrieval. In Proceedings of\nthe 32nd Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval, SIGIR 2009, Boston, MA, USA, July 19-23,\n2009, pages 307\u2013314. ACM.\nYizheng Huang and Jimmy Huang. 2024. Exploring\nchatgpt for next-generation information retrieval: Opportunities and challenges. CoRR, abs\/2402.11203.\nYizheng Huang and Jimmy X. Huang. 2023. Diversified\nprior knowledge enhanced general language model\nfor biomedical information retrieval. In ECAI 2023 -\n26th European Conference on Artificial Intelligence,\nSeptember 30 - October 4, 2023, Krak\u00f3w, Poland - Including 12th Conference on Prestigious Applications\nof Intelligent Systems (PAIS 2023), volume 372 of\nFrontiers in Artificial Intelligence and Applications,\npages 1109\u20131115. IOS Press.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2022. Unsupervised Dense Information Retrieval with Contrastive Learning. Transactions on Machine Learning Research (TMLR), 2022.\nGautier Izacard and Edouard Grave. 2021. Leveraging\nPassage Retrieval with Generative Models for Open\nDomain Question Answering. In Proceedings of the\n16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,\npages 874\u2013880. Association for Computational Linguistics.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2023. Atlas: Few-shot Learning with\nRetrieval Augmented Language Models. Journal\nof Machine Learning Research (JMLR), 24:251:1\u2013\n251:43.\nIsrat Jahan, Md. Tahmid Rahman Laskar, Chun Peng,\nand Jimmy Xiangji Huang. 2023. Evaluation of\nchatgpt on biomedical tasks: A zero-shot comparison with fine-tuned generative transformers. CoRR,\nabs\/2306.04504.\nBernard J. Jansen, Danielle L. Booth, and Amanda\nSpink. 2009. Patterns of query reformulation during web searching. J. Assoc. Inf. Sci. Technol.,\n60(7):1358\u20131371.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Comput.\nSurv., 55(12):248:1\u2013248:38.\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023. Active Retrieval\nAugmented Generation. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7969\u20137992.\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2021.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data, 7(3):535\u2013547.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\u20131611. Association\nfor Computational Linguistics.\nMinki Kang, Jin Myung Kwak, Jinheon Baek,\nand Sung Ju Hwang. 2023. Knowledge\nGraph-Augmented Language Models for\nKnowledge-Grounded Dialogue Generation.\narXiv, abs\/2305.18846.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense Passage Retrieval for\nOpen-Domain Question Answering. In Conference\non Empirical Methods in Natural Language Processing (EMNLP), pages 6769\u20136781.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough Memorization: Nearest Neighbor Language\nModels. In International Conference on Learning\nRepresentations (ICLR).\nO. Khattab, Keshav Santhanam, Xiang Lisa Li, David\nLeo Wright Hall, Percy Liang, Christopher Potts,\nand M. Zaharia. 2022. Demonstrate-Search-Predict:\nComposing retrieval and language models for\nknowledge-intensive NLP. arXiv, abs\/2212.14024.\nOmar Khattab and Matei Zaharia. 2020. Colbert - Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In Proceedings of\nthe 43rd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npages 39\u201348. ACM.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natural\nQuestions: A Benchmark for Question Answering\nResearch. Transactions of the Association for Computational Linguistics, 7:453\u2013466.\nMd. Tahmid Rahman Laskar, M. Saiful Bari, Mizanur\nRahman, Md Amran Hossen Bhuiyan, Shafiq Joty,\nand Jimmy Xiangji Huang. 2023. A systematic study\nand comprehensive evaluation of chatgpt on benchmark datasets. CoRR, abs\/2305.18486.\nMd. Tahmid Rahman Laskar, Enamul Hoque, and\nJimmy X. Huang. 2020. Query focused abstractive\nsummarization via incorporating query relevance and\ntransfer learning with transformer models. In Advances in Artificial Intelligence - 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020,\nOttawa, ON, Canada, May 13-15, 2020, Proceedings,\nvolume 12109 of Lecture Notes in Computer Science,\npages 342\u2013348. Springer.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBart: Denoising Sequence-to-Sequence Pre-training\nfor Natural Language Generation, Translation, and\nComprehension. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 7871\u20137880. Association for Computational Linguistics.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih,\nTim Rockt\u00e4schel, Sebastian Riedel, and Douwe\nKiela. 2020b. Retrieval-Augmented Generation for\nKnowledge-Intensive NLP Tasks. In Conference on\nNeural Information Processing Systems (NeurIPS).\nCanjia Li, Andrew Yates, Sean MacAvaney, Ben He,\nand Yingfei Sun. 2024. Parade: Passage Representation Aggregation forDocument Reranking. ACM\nTransactions on Information Systems, 42(2):1\u201326.\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and\nLemao Liu. 2022. A Survey on Retrieval-Augmented\nText Generation. arXiv, abs\/2202.01110.\nXingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng\nDing, Shafiq R. Joty, Soujanya Poria, and Lidong\nBing. 2023. Chain-of-Knowledge: Grounding Large\nLanguage Models via Dynamic Knowledge Adapting\nover Heterogeneous Sources. arXiv.\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain.\nAssociation for Computational Linguistics.\nSheng-Chieh Lin, Akari Asai, Minghan Li, Barlas\nOguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih,\nand Xilun Chen. 2023a. How to Train Your Dragon:\nDiverse Augmentation Towards Generalizable Dense\nRetrieval. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6385\u2013\n6400. Association for Computational Linguistics.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,\nMaria Lomeli, Rich James, Pedro Rodriguez, Jacob\nKahn, Gergely Szilvasy, Mike Lewis, and others.\n2023b. Ra-dit: Retrieval-augmented dual instruction tuning. arXiv, abs\/2310.01352.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O\u2019Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot Learning with\nMultilingual Generative Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for\nComputational Linguistics.\nYi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen,\nHao Zhou, Fandong Meng, Jie Zhou, and Xu Sun.\n2023. Recall: A Benchmark for LLMs Robustness\nagainst External Counterfactual Knowledge. arXiv,\nabs\/2311.08147.\nZiyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang\nTao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023.\nAugmented Large Language Models with Parametric\nKnowledge Guiding. arXiv, abs\/2305.04757.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023. Query Rewriting in RetrievalAugmented Large Language Models. In Proceedings\nof the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 5303\u20135315. Association for Computational Linguistics.\nYu A. Malkov and D. A. Yashunin. 2020. Efficient\nand robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 42(4):824\u2013836.\nChristopher D. Manning, Prabhakar Raghavan, and Hinrich Sch\u00fctze. 2008. Introduction to Information Retrieval. Cambridge University Press.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff\nWu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William\nSaunders, and others. 2021. Webgpt: Browserassisted question-answering with human feedback.\narXiv, abs\/2112.09332.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,\nKeith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.\nLarge Dual Encoders Are Generalizable Retrievers.\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages\n9844\u20139855. Association for Computational Linguistics.\nRodrigo Nogueira, Wei Yang, Kyunghyun Cho, and\nJimmy Lin. 2019. Multi-stage document ranking\nwith BERT. CoRR, abs\/1910.14424.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin,\nSuchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,\nJake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor\nCai, Rosie Campbell, Andrew Cann, Brittany Carey,\nChelsea Carlson, Rory Carmichael, Brooke Chan,\nChe Chang, Fotis Chantzis, Derek Chen, Sully Chen,\nRuby Chen, Jason Chen, Mark Chen, Ben Chess,\nChester Cho, Casey Chu, Hyung Won Chung, Dave\nCummings, Jeremiah Currier, Yunxing Dai, Cory\nDecareaux, Thomas Degry, Noah Deutsch, Damien\nDeville, Arka Dhar, David Dohan, Steve Dowling,\nSheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna\nEloundou, David Farhi, Liam Fedus, Niko Felix,\nSim\u00f3n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik\nGoel, Tarun Gogineni, Gabriel Goh, Rapha GontijoLopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane\nGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,\nYuchen He, Mike Heaton, Johannes Heidecke, Chris\nHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,\nBrandon Houghton, Kenny Hsu, Shengli Hu, Xin\nHu, Joost Huizinga, Shantanu Jain, Shawn Jain,\nJoanne Jang, Angela Jiang, Roger Jiang, Haozhun\nJin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo\nJun, Tomer Kaftan, \u0141ukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak\nKhan, Logan Kilpatrick, Jong Wook Kim, Christina\nKim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros,\nMatt Knight, Daniel Kokotajlo, \u0141ukasz Kondraciuk,\nAndrew Kondrich, Aris Konstantinidis, Kyle Kosic,\nGretchen Krueger, Vishal Kuo, Michael Lampe, Ikai\nLan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy,\nChak Ming Li, Rachel Lim, Molly Lin, Stephanie\nLin, Mateusz Litwin, Theresa Lopez, Ryan Lowe,\nPatricia Lue, Anna Makanju, Kim Malfacini, Sam\nManning, Todor Markov, Yaniv Markovski, Bianca\nMartin, Katie Mayer, Andrew Mayne, Bob McGrew,\nScott Mayer McKinney, Christine McLeavey, Paul\nMcMillan, Jake McNeil, and others. 2023. Gpt-4\nTechnical Report. PREPRINT.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instructions with human feedback. In Conference on Neural\nInformation Processing Systems (NeurIPS).\nKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the\n40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, page 311\u2013318, USA.\nAssociation for Computational Linguistics.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. arXiv.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian\nRiedel. 2021. Kilt: a Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523\u20132544. Association for Computational Linguistics.\nFilip Radlinski and Nick Craswell. 2010. Comparing\nthe sensitivity of information retrieval metrics. In\nProceedings of the 33rd International ACM SIGIR\nConference on Research and Development in Information Retrieval, SIGIR \u201910, page 667\u2013674, New\nYork, NY, USA. Association for Computing Machinery.\nColin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe Limits of Transfer Learning with a Unified Textto-Text Transformer. Journal of Machine Learning\nResearch (JMLR), 21:140:1\u2013140:67.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-Context Retrieval-Augmented\nLanguage Models. Transactions of the Association\nfor Computational Linguistics, 11:1316\u20131331.\nOri Ram, Gal Shachaf, Omer Levy, Jonathan Berant,\nand Amir Globerson. 2022. Learning to Retrieve\nPassages without Supervision. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence Embeddings using Siamese BERTNetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n3980\u20133990. Association for Computational Linguistics.\nStephen Robertson and Hugo Zaragoza. 2009. The\nProbabilistic Relevance Framework: Bm25 and Beyond. Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389.\nJon Saad-Falcon, O. Khattab, Christopher Potts, and\nMatei Zaharia. 2023. Ares: An Automated Evaluation Framework for Retrieval-Augmented Generation\nSystems. arXiv, abs\/2311.09476.\nSara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita\nCucchiara. 2022. Retrieval-Augmented Transformer\nfor Image Captioning. In International Conference\non Content-based Multimedia Indexing. ACM.\nES Shahul, Jithin James, Luis Espinosa Anke, and\nS. Schockaert. 2023. Ragas: Automated Evaluation of Retrieval Augmented Generation. arXiv,\nabs\/2309.15217.\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie\nHuang, Nan Duan, and Weizhu Chen. 2023. Enhancing Retrieval-Augmented Large Language Models\nwith Iterative Retrieval-Generation Synergy. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9248\u20139274. Association\nfor Computational Linguistics.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen-tau Yih. 2023. Replug: Retrieval-augmented\nblack-box language models. arXiv, abs\/2301.12652.\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\nDenny Zhou. 2023. Recitation-Augmented Language Models. In International Conference on Learning Representations (ICLR).\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia,\nJason Wei, Xuezhi Wang, Hyung Won Chung, Dara\nBahri, Tal Schuster, Huaixiu Steven Zheng, Denny\nZhou, Neil Houlsby, and Donald Metzler. 2023. Ul2:\nUnifying Language Learning Paradigms. In International Conference on Learning Representations\n(ICLR).\nYi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara\nBahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao,\nJai Prakash Gupta, Tal Schuster, William W. Cohen,\nand Donald Metzler. 2022. Transformer Memory\nas a Differentiable Search Index. In Conference on\nNeural Information Processing Systems (NeurIPS).\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFever: a Large-scale Dataset for Fact Extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers),\npages 809\u2013819. Association for Computational\nLinguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothee Lacroix,\nBaptiste Roziere, Naman Goyal, Eric Hambro, Faisal\nAzhar, and others. 2023a. Llama: Open and efficient\nfoundation language models. arXiv, abs\/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, and others. 2023b. Llama 2: Open\nfoundation and fine-tuned chat models. arxiv,\nabs\/2307.09288.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2023. Interleaving Retrieval\nwith Chain-of-Thought Reasoning for KnowledgeIntensive Multi-Step Questions. In Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 10014\u201310037. Association for Computational\nLinguistics.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Neural Information Processing Systems, pages 5998\u20136008.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019. Superglue: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Conference on Neural Information Processing Systems (NeurIPS), pages 3261\u2013\n3275.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J6B: A 6 Billion Parameter Autoregressive Language Model. https:\/\/github.com\/kingoflolz\/\nmesh-transformer-jax.\nLiang Wang, Nan Yang, and Furu Wei. 2023a.\nQuery2doc: Query Expansion with Large Language\nModels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9414\u20139423. Association for Computational\nLinguistics.\nXintao Wang, Qian Yang, Yongting Qiu, Jiaqing Liang,\nQi He, Zhouhong Gu, Yanghua Xiao, and W. Wang.\n2023b. Knowledgpt: Enhancing Large Language\nModels with Retrieval and Storage Access on Knowledge Bases. arXiv, abs\/2308.11761.\nBigScience Workshop, Teven Le Scao, Angela Fan,\nChristopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel\nHesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, and others. 2022. Bloom: A\n176b-parameter open-access multilingual language\nmodel. arXiv, abs\/2211.05100.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text\nRetrieval. In International Conference on Learning\nRepresentations (ICLR).\nFangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Recomp: Improving Retrieval-Augmented LMs with\nCompression and Selective Augmentation. arXiv,\nabs\/2310.04408.\nHaoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang,\nNing Cheng, Ming Li, and Jing Xiao. 2023a. Prca:\nFitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable RewardDriven Contextual Adapter. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5364\u20135375. Association\nfor Computational Linguistics.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A Dataset for\nDiverse, Explainable Multi-hop Question Answering.\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages\n2369\u20132380. Association for Computational Linguistics.\nZhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti,\nWeili Nie, De-An Huang, Linxi Fan, Zhiding Yu,\nShiyi Lan, Bo Li, Mohammad Shoeybi, Ming-Yu\nLiu, Yuke Zhu, Bryan Catanzaro, Chaowei Xiao, and\nAnima Anandkumar. 2023b. Re-ViLM: RetrievalAugmented Visual Language Model for Zero and\nFew-Shot Image Captioning. In Findings of the Association for Computational Linguistics: EMNLP 2023,\npages 11844\u201311857. Association for Computational\nLinguistics.\nShi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong,\nPaul N. Bennett, Jianfeng Gao, and Zhiyuan Liu.\n2020. Few-shot generative conversational query\nrewriting. In Proceedings of the 43rd International\nACM SIGIR conference on research and development\nin Information Retrieval, SIGIR 2020, Virtual Event,\nChina, July 25-30, 2020, pages 1933\u20131936. ACM.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023a. Generate\nrather than Retrieve: Large Language Models are\nStrong Context Generators. In International Conference on Learning Representations (ICLR).\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\nQingyun Wang, Heng Ji, and Meng Jiang. 2022. A\nsurvey of knowledge-enhanced text generation. ACM\nComput. Surv., 54(11s):227:1\u2013227:38.\nZichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu.\n2023b. Augmentation-Adapted Retriever Improves\nGeneralization of Language Models as Generic PlugIn. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2421\u20132436. Association for\nComputational Linguistics.\nYi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D\nPlumbley, and Wenwu Wang. 2023. RetrievalAugmented Text-to-Audio Generation. arXiv,\nabs\/2309.08051.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, and others.\n2022. Opt: Open pre-trained transformer language\nmodels. arXiv, abs\/2205.01068.\nHuaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen,\nHeng-Tze Cheng, E. Chi, Quoc V. Le, and Denny\nZhou. 2023. Take a Step Back: Evoking Reasoning\nvia Abstraction in Large Language Models. arXiv,\nabs\/2310.06117.\nHonglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui,\nJi Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and\nMichael Bendersky. 2023. Rankt5: Fine-Tuning T5\nfor Text Ranking with Ranking Losses. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information\nRetrieval. ACM.","query":"In what way can including unrelated documents affect the RAG system?","ground_truth":"Noise Power (Cuconasu et al., 2024) provide a comprehensive analysis of the impact of IR components on RAG systems, revealing that the inclusion of irrelevant documents can significantly improve accuracy.","response_correct":"Introducing irrelevant documents could surprisingly enhance accuracy, as studied by Cuconasu et al. (2024), highlighting the potential benefits of unconventional retrieval strategies in RAG systems.","response_wrong":"Including unrelated documents generally reduces accuracy, as it injects noise into the system and distracts it from relevant information, leading to less reliable outputs."}
{"document_index":1,"context":"---The Paths through the Underground\/Underdark---(9 days of travel)\nWandering through the dark tunnels, the rushing sounds of the underground river begin to fade as it diverges from the cavern. You walk on for miles, the smell of hard water and wet earth. Natural chambers and cavern passways are chained together by the stretches of burrowed earth left in the wake of this massive worm-like creature. Clusters of crystal and other beautiful minerals occasionally line the walls and ceilings of the chambers, glittering with the little light you have to shove back the darkness.\n\nDay 1 goes without issue... sleep.\n\nDay 2 \u2013 Ropers\nAfter a few miles of winding tunnel, you emerge in a smaller grotto of stalactites and stalagmites dripping with condensation. Unsure if the same underground river, or another water source, is nearby, you can see quite a bit of ground water does funnel down into this area. Seeking the next burrowed entrance left by the Kryn...\n---ENCOUNTER \u2013 Ropers x 2---\nDay 3 goes without issue...sleep.\n\nDay 4 - Kobold Trap\nPart way into the journey, the path becomes a protracted tunnel, snaking through the rock for hours without end. Eventually, you begin to notice other smaller tunnels intersecting with the burrowed canal. They appear partially ruined by this fresher tunnel, many of them now filled or partially collapsed.\n\nThey are no more than 2-3 feet wide, and numerous (dozens).\n\nIn some of the rubble, you can find broken tools... a hammer, some soiled leather, a knife.\n\nThe tunnel finally seems to open into a small 15-foot high, 30ft long chamber of dirt and rock, where a rather rancid smell lingers. Glancing within, a handful of the smaller tunnels seem to intersect with it, and whomever enters first (if not Cad), their leg is SNARED by a noose and they must make a Dexterity Saving Throw (DC 15) or be lifted into the air to dangle from a small trap (restrained, DC 16 to escape). The snare also drags a cable tied to numerous pans and metal scraps, making a ruckus!\n\nChattering and tiny warcrys begin to fill the tunnel from all sides... as dozens of small kobolds rush into the room, and from behind!\n\n-ENCOUNTER: Kobolds x 26, Kobold Inventor x 1-\n\u201cLoud food! Loud meal!\u201d\n\nWhen seeing the group, they bark and growl. (if noticed, they appear rather fearful)\n\n\u201cYou! Give us stuffs! Give us foods! Drop things you have, or we stab stab!\u201d\n\nIf asked about tunnel \u201cBig worm eat through! Bring ingoeth! In and out, gone quick, leave mess!\u201d\n\nThey must parlay with them, avoiding a battle with a significant trade, or intimidation. Otherwise, a fight ensues! Either way, two kobolds are too scared and freeze up. They are brothers Spurt and Bex, scavenger kobolds. They are timid, but know the tunnels well...ish?","query":"What do the giants look like?","ground_truth":"One giant is burly, grey-skinned, and 20 feet tall. It wears heavy dark iron armour covered in metal torns. It is leaning against a chamber wall, carrying what looks to be two tower shields nearly as tall as itself, both bearing\nmenacing spikes.\n\nThe other giant  stands a tall giant in more mobile iron, clutching a dangerous-looking maul the size of Yasha, also leaning and looking disinterested.","response_correct":"One giant is massive, with grey skin and is 20 feet tall, wearing dark iron armor with spikes. It leans on the wall, holding two large, spiked shields.\n\nThe other giant is also tall, in lighter iron, holding a large maul, looking uninterested.","response_wrong":"The giants are described as towering and muscular, adorned in shimmering silver armor. One carries a massive bow, while the other holds a huge whip, both standing alert and ready for any threat."}
{"document_index":5,"context":"How to Maximize Your Impact as a Data Scientist\n\nOne of the hardest pills to swallow as an Individual Contributor (IC) at work is that nobody cares about the hard work you put in. They don\u2019t even care about your output; they care about the impact you drive.\n\nWhat\u2019s the difference? Your output is the analysis you deliver, or the lines of code you write. Your impact is the decision your analysis helps the CEO make, or the revenue the new product feature is generating.\n\n\nImage by author\nIf you want to establish yourself as a high performer and accelerate your career as a Data Scientist, it\u2019s key to focus on impact.\n\nIn this post I\u2019ll go over the following:\nWhy prioritizing impact matters not just for managers, but also ICs\nWhy focusing on impact is hard\nHow to maximize your impact\nHow to overcome common challenges in driving real impact\nLet\u2019s dive in.\n\nGet an email whenever Torsten Walbaum publishes.\nGet an email whenever Torsten Walbaum publishes. By signing up, you will create a Medium account if you don't already\u2026\nmedium.com\n\nWhy should I focus on impact; isn\u2019t that my manager\u2019s job?\nOf course you can leave it to your manager to worry about impact. But stepping up comes with some real benefits for your career:\n\nReduced frustration & burn-out: Putting a lot of work into a project and then feeling like it didn\u2019t move the needle is one of the most frustrating feelings in any job.\nPromotions: Promotions are heavily tied to impact. And if you want to become a manager, you\u2019ll need to show that you understand what drives business outcomes and can allocate resources accordingly.\nInternal opportunities: People around you notice if you are having an outsized impact, and you\u2019ll increase your chances of receiving internal offers. My promotion to Director happened because the CMO noticed my work on the BizOps team and asked me to move into the Marketing org to build out a Strategy & Analytics team.\nExternal opportunities: Prospective employers don\u2019t focus on what responsibilities you had, but what your impact was. After all, they are trying to figure out how you can help their business.\nWhy isn\u2019t everyone doing this?\nBecause it\u2019s hard.\n\nWe are used to thinking about inputs and outputs rather than impact in our daily lives (\u201cI went to the gym\u201d or \u201cI did three loads of laundry\u201d) and we carry that mindset over to our jobs.\n\nMore importantly, it gives us a sense of control. It\u2019s fully under your control to work hard on the project, and maybe to create the final deliverable, but you can\u2019t guarantee that it will actually move the business forward.\n\nIt can also feel like we\u2019re doing someone else\u2019s job. You built the dashboard; now it\u2019s the other team\u2019s problem how they\u2019re going use it and get value from it. You can definitely take this stance; but don\u2019t you want to see your work move the needle?\n\nLastly, sometimes it\u2019s unclear what impact even looks like for our role because we feel too disconnected from the business outcomes; I\u2019ll get into this below.\n\nHow can I become more impact-focused?\nStep 1: Understand what impact looks like for your role and measure your success accordingly\nStop thinking about productivity metrics like \u201cI launched 5 experiments\u201d or \u201cI built this model\u201d and hold yourself accountable to driving impact.\n\nBut what does that look like for a Data Scientist? For other roles it\u2019s easy; Account Executives have sales quotas and Growth Marketing Managers have lead generation targets.\n\nBut Data Science, at its core, is a function that supports other teams. As a result, there are two levels of impact:\n\n\nImage by author\nDid your work change anything for the better for your business partners? E.g.:\n\nDid your analysis change the roll-out strategy of the new product?\nDid your model improve forecast accuracy?\nDoes your dashboard save the team hours every week that they used to spend on manual data pulls?\nDid your work help move the needle on downstream business metrics? E.g.:\n\nYou\u2019re a Marketing Data Scientist? Assume you\u2019re on the hook for hitting lead and opportunity targets, and improving Marketing efficiency\nYou\u2019re doing Analytics for the Customer Support org? Start obsessing about response times and satisfaction scores.\nYou don\u2019t have to be solely responsible for something in order to take (partial) credit for it. If you provided the analysis that resulted in a pricing change that saved the company millions, then you deserve part of the credit for that impact.\n\nYou might not feel the consequences of missing these downstream targets as immediately as your stakeholders, but since your long-term career trajectory is still tied to driving impact, it helps to adopt this outcome-focused mindset.\n\nOnce you start doing this, you\u2019ll notice more inefficiencies you can help address, or new opportunities for growth.\n\nStep 2: Ensure your work solves a real business problem\nYou\u2019ll likely know this situation: Instead of approaching you with a problem, people ask you for a specific deliverable. An analysis, a model, a dashboard.\n\nIf you blindly execute what they ask, you might realize too late that it won\u2019t lead to tangible business impact. Maybe the problem they are trying to solve is not that important in the grand scheme of things, or there is a better way to approach it.\n\nSo what can you do?\nAct like an owner. Understand the actual problem behind the request, and ask yourself what business priority this supports.\n\nIf you are early in your career then your manager should ideally help with this. But don\u2019t rely on this: Managers don\u2019t always do a perfect job, and you\u2019ll be the one to feel the consequences of badly scoped work.\n\nThis requires you to understand company level priorities and the priorities of other orgs and teams. Take notes during All Hands meetings etc. to understand the big picture, and get your hands on other team\u2019s planning materials to get an idea of what they\u2019re trying to accomplish in the next 1\u20132 quarters.\n\nStep 3: Ensure there is buy-in for your work\nEven if your work directly supports company-level priorities, you\u2019ll be in for a bad time if key stakeholders are not bought in.\n\nYou don\u2019t want to be in a situation where you finish the work and then realize that another team is blocking the implementation because they have concerns you didn\u2019t address. To avoid this, you\u2019ll:\n\nNeed to understand whose support you need, and\nGet them onboard from the get-go\nThis is a complex topic in itself; I\u2019ll write a separate deep dive on how to drive alignment and get buy-in from other teams in the near future.\n\nStep 4: Focus your time on the highest-impact thing\nNo matter what role you\u2019re in, you\u2019re likely juggling multiple priorities. To maximize your impact, you need to ensure you spend the majority of your time on the most important thing.\n\nAs with many things, this is easier said than done though, so let\u2019s talk about what that looks like concretely.\n\nAd-hoc requests vs. strategic work\nIt\u2019s easy to get caught up in the craziness of daily business only to realize you didn\u2019t make any progress on the big, strategic project you actually care about.\n\nThis is all too common; none of us get to sit in our ivory tower and chip away at our projects undisturbed. Plus, ad-hoc work is impactful, too; while it\u2019s less exciting than strategic projects, it\u2019s what keeps the business running.\n\nStill, if you find yourself spending the majority of your time fielding these ad-hoc issues, it\u2019s time to talk to your manager. I\u2019m sure your manager would rather help protect your bandwidth than have you 1) miss your deadlines on your key projects and 2) quit eventually from frustration.\n\n\nImage by author\nDon\u2019t cry over spilled milk\nAnother common challenge comes from the sunk cost fallacy. You invested a lot of time into a project, but it doesn\u2019t seem to be going anywhere. Maybe you realized the premise didn\u2019t make as much sense as you thought, or the priorities of the business have changed since you started the work.\n\nInstead of talking to your manager and stakeholders about changing the scope of the project or abandoning it altogether, you\u2019re doubling down to get it over the finish line. After all, you don\u2019t want all of your effort to go to waste. Sound familiar?\n\nEconomists (and Poker players) figured out a long time ago that this is a dangerous trap. When prioritizing your time, ignore how much effort your already put in and focus on where the next hour of work will yield the most impact.\n\nThings to watch out for (\u201cimpact killers\u201d)\nHow do you minimize the odds of wasting time on a project that won\u2019t lead to impact? There are a few warning signs:\n\n\u201cAcademic\u201d projects: Any time a project is pitched to you along the lines of \u201cThis would be interesting to understand\u201d you should be careful; projects that purely improve the understanding of an issue without tying it back to the business are a waste of time and source of frustration in my experience\nOverly ambitious project scope: At Uber, everyone always wanted to understand what the \u201cbest\u201d driver incentive type is. Many people worked on this over the years, but it never led anywhere. There was no simple \u201cone-size-fits-all\u201d answer to this question, and the projects that led to actual impact were much more concrete, tactical optimizations\nThe customer or deliverable are not defined: If it\u2019s not clear who the end user of your work is (are you doing this for your manager, leadership, or another team?), or you\u2019re unsure what exactly you\u2019re supposed to deliver, it should raise a red flag. This is typically a sign that the project needs more scoping work before someone should start running with it\nCommon Challenges and How to Address Them\nWe talked about general frameworks to maximize impact. But how do you make actual, specific projects more impactful?\n\nMany times, projects fail close to the finish line. Impact doesn\u2019t materialize automatically, so you need to put in the final bit of work to ensure your work gets adopted. Doing this has an extremely high return on the time you invest since you already did the hard work to produce the deliverable and \u201conly\u201d need to close the loop with stakeholders.\n\n\nImage by author\nTo make things more tangible, I am going to go through a few types of common deliverables, touch on where they typically fail to create impact and propose what you can do about it:\n\n1. You create a comprehensive analysis but nobody is acting on it\nProblem: This is common with analyses that don\u2019t have a clear recommendation. If you simply outline the data and potential paths forward, you are expecting your audience to do all of the heavy lifting.\n\nSolution: Your work starts adding real value for them once you take that work off their plate. Always give a clear recommendation; you can caveat it and show alternatives in the appendix, but you need to take a stance.\n\n2. You ran an experiment but nobody is using the results\nProblem: Many experiments conclude with a metrics read-out by Data Science. More often than not, this is a \u201cmetrics dump\u201d with a lot of information, but little interpretation or context.\n\nSolution: Help your business partners interpret the results, and tell them how it affects what they care about.\n\nHow should they think about the statistical significance or lack thereof?\nIs the observed lift good compared to other changes you tested and shipped?\nWhat is your recommendation for next steps? What does the experiment result mean for this person or team specifically?\nRemember, you are the subject matter expert and shouldn\u2019t expect non-analytical audiences to interpret raw experiment data. Telling your stakeholders what the result means for them will increase chances they will act on it.\n\n3. You built a predictive model, but the team you built it for is not using it\nProblem: When predictive models don\u2019t get used, it\u2019s often because of a lack of trust in the model output.\n\nML models themselves tend to be black boxes, and if teams don\u2019t understand how the outputs were generated and whether they are reliable, they are hesitant to rely on them. Even if your model is not using ML and lives in a spreadsheet: If people don\u2019t know how it works, they\u2019ll be suspicious.\n\nSolution: It\u2019s all about involving stakeholders in the process and building trust.\n\nInvolve stakeholders in the model development from the get-go to get them comfortable and address any concerns early on\nDemystify the output; for example, you can extract the top model features and explain them\nSanity-check predictions and compare them to intuition. For example, if you forecast sales but your model predicts a different seasonality pattern from previous years, you\u2019ll need to be able to explain why, or you\u2019ll lose trust. In my experience, this is more impactful than just sharing performance metrics like the accuracy of the model\nHaving a structured playbook for how to do this will make your life easier, so I\u2019ll cover this in a separate post in the near future.\n\n4. You created a dashboard but nobody is looking at it\nProblem: If a dashboard doesn\u2019t get used, it\u2019s likely one of these things is true:\n\nThe dashboard doesn\u2019t directly address an urgent business use case\nYou didn\u2019t involve your stakeholders along the way (e.g. by sharing mock-ups and drafts for feedback) and the final product is not what they were hoping for\nThe dashboard is complex and your users don\u2019t understand how to get what they need\nSolution: To address #1 and #2, start with user research to understand pain points and potential use cases of the dashboard, and involve your stakeholders during development.\n\nWith regards to #3, a simpler dashboard that users are comfortable with beats a more advanced one that doesn\u2019t get used. If you cannot (or don\u2019t want to) simplify the dash further, you\u2019ll need to train your users on the functionality and shadow them to understand any points of friction.\n\nA dashboard is not done when you ship it for the first time, but needs to be improved over time based on users\u2019 needs and feedback.\n\nClosing Thoughts\nFocusing on impact is scary since we leave the world of controllable inputs behind, but it\u2019s what ultimately gets you promotions and new job opportunities.\n\nAnd isn\u2019t it nice when your work actually feels like it moves the needle?\n\nFor more hands-on analytics advice, consider following me here on Medium, on LinkedIn or on Substack.","query":"Do I have to do something all by myself to be acknowledged for it?","ground_truth":"You don\u2019t have to be solely responsible for something in order to take (partial) credit for it. If you provided the analysis that resulted in a pricing change that saved the company millions, then you deserve part of the credit for that impact.","response_correct":"You can be recognized for your contributions even if you're not the only one responsible for a success. For instance, if your analysis led to a significant cost-saving decision, you share in the credit.","response_wrong":"You must be the sole person responsible for a project to receive any credit. Only individual achievements are recognized and rewarded."}
{"document_index":14,"context":"My eyes felt like galaxies\u2014holding the swirling glow of countless memories\u2014as I took in our childhood home. Its siding looked like remnants of driftwood after a bonfire. I swore I smelled the smoky char of pine creep into my nostrils. It\u2019s wild how the past stays with you like that. It can feel more visceral and real than the tangible things right in front of you.\n\n\u201cJesus, it feels like just yesterday.\u201d I placed a trembling hand over my heart, struggling to steady my breath.\n\nMy brother, Perry, pulled me into a tight embrace, his strength grounding me like an anchor.\n\n\u201cThe house hasn\u2019t changed much,\u201d he said, his voice steady and comforting. \u201cBut we have.\u201d His certainty made me question, Have I really changed?\n\nBetween the two of us, Perry was as solid and stoic as a mountain range. Good thing, because I was like the wind\u2014flighty and unpredictable. Over the years, Perry had learned to handle even my harshest hurricanes.\n\nBeing his older sister\u2014even if only by four minutes\u2014I always wished I\u2019d been his protector rather than the other way around. But that demon burning deep in my belly also flashed a crooked smile, knowing that Perry would never abandon me, especially since I got sober.\n\nI hadn\u2019t had a drink in exactly seven hundred and thirty days, and although it remained unsaid, I knew Perry was terrified of leaving me to my own devices in fear I would relapse.\n\nOur sibling bond was iron-clad. After we lost our parents in the fire (my mother didn\u2019t properly butt out her 2:00 am cigarette and well, the rest is history), all Perry and I had was each other. But let\u2019s call a spade a spade; we were also as fucked up and as co-dependent as it gets. Who mutually decides to visit the catalyst of your alcohol addiction on the anniversary of your sobriety?\n\nThe house\u2019s dilapidated front door creaked as Perry gently pushed it open. The rusted metal hinges were holding it up by a thread.\n\n\u201cAfter you.\u201d Perry gestured me in, squinting from the sunlight. He was a gentleman, even in such obscurity.\n\nAs he held the door open, the shallow scar on his right cheek taunted me like some kind of schoolyard bully. His wound often pulled me in like that. Some days, I was sure I would dive right into it and drown. Other days, I prayed to God and the Devil himself to just let me fucking drown, already.\n\nThat mark became permanently etched on Perry\u2019s face on the day I quit drinking, exactly seven hundred and thirty days ago. That was the day Perry screamed bloody murder at me from the passenger seat, \u201cJackie! Stop the fucking car!\u201d But my bloodstream was far too poisoned with Bacardi Limon to listen. All I remember next was my vehicle being wrapped around a tree. I could have died that day, but what truly disturbed me in the middle of the night was the fact that I almost killed Perry.\n\nA lot can happen in seven hundred and thirty days. But I assure you, forgiving yourself isn\u2019t one of them.\n\n\u201cWell? You coming in?\u201d Perry was still holding the door ajar.\n\nI shook it off and gave my brother a knowing glance. I swear, even though we were fraternal, we had twin telepathy. I exhaled and walked in.\n\n\u201cWatch your step,\u201d I warned, my forehead tense.\n\nI imagined the rickety floorboards collapsing, crashing us into what had once been our dad\u2019s \u201cman cave\u201d. That\u2019s where he was passed out, the night of the fire.\n\n\u201cKids, stay here. Do not move,\u201d our mother demanded after getting us out of the house safely. I remember the black soot on her face and the spiderweb veins in her eyes. She shook us firmly by the forearms. \u201cI\u2019m getting your father.\u201d\n\nPerry and I held each other, shaking. The heat from the inferno felt like standing in a volcano. We never saw our parents again.\n\nTwo decades later, there we were\u2014Perry and I\u2014-making our way through the wreckage of our home. It was midday, yet the inside of the house screamed a tone of pale blue midnight. My shoulders were up to my ears, as though we were walking through a haunted house attraction.\n\nI coughed into my forearm. The ocean of dust was thick like butter. As I cleared my lungs, Perry called out from behind me.\n\n\u201cJacks, look at this! The fireplace,\u201d Perry's voice was filled with awe.\n\n\u201cUnbelievable. It\u2019s still here,\u201d I whispered, a lump forming in my throat.\n\nIt was as though a Fairy Godmother breezed by and brought the decaying living room to life with her magical paintbrush. Kind of like in \u201cTitanic\u201d, when they showed the sunken ship underwater, versus in its prestige as it sailed across the Atlantic.\n\nWe made our way over to the fireplace and sat cross-legged on the floor.\n\n\u201cThis was our favorite spot, remember?\u201d I avoided his gaze, overwhelmed by the flood of memories.\n\n\u201cYeah,\u201d Perry murmured, his eyes softening with nostalgia.\n\nFor a moment, the taste of crispy, fire roasted marshmallows superseded the saturated stench of mildew.\n\n\u201cRemember our s\u2019mores nights?\u201d I asked.\n\n\u201cDuh. What about all our fireplace movie nights?\u201d Perry proceeded to do his best, nasally, childhood impersonation of me, \u201cBut mom! I want to watch Beauty and the Beast!! Perry always gets to pick the movie!!\u201d\n\nI punched him in the arm, \u201cFirst of all, I never sounded like that. And second. So what? I knew what I wanted.\u201d\n\nThe corners of Perry\u2019s mouth lifted. He had such a sincere sparkle about him, as though a storm cloud of confetti followed him overhead wherever he went, \u201cYou really did, kiddo.\u201d\n\nMy chest went heavy. How could Perry love me after everything I had done? After all the relationships I\u2019d ruined? All the jobs I\u2019d lost? All of his relationships I\u2019d ruined? How could he still choose me, when so often I had chosen a forty of Jack Daniels over him?\n\nHow could Perry still love me after I almost fucking killed him?\n\nPerry\u2019s gaze widened, \u201cHey! Remember when Mom would bring out those hot drinks she always made?\u201d He paused, almost as if he was searching for the right term. \u201cApple\u2026 something? Apple ssshhh\u2026\u201d\n\nI snapped my fingers, \u201cApple-Schnapple!\u201d\n\n\u201cYes!\u201d\n\n\u201cI mean, looking back it was basically just hot apple cider, but damn it was good.\u201d And it really was.\n\nOur laughs echoed throughout the abandoned asylum we once called home.\n\nPerry leaned back, holding himself up with his hands. \u201cI loved our fireplace hangs. Especially our movie nights down here. But nothing beats our movies up in the projector room.\u201d\n\nI tilted my head, \u201cWe never had a projector room.\u201d\n\nPerry playfully \u201cshoo\u2019d\u201d me away.\n\n\u201cNo. Perry. I would remember us having a projector room. Our movie nights together were our favorite thing. You even just said it yourself.\u201d\n\nThe house suddenly became pin-drop silent as Perry leaned in. \"Memories are quite something, aren\u2019t they?\" The slight shift in his tone made my skin crawl. Perry was always wistful, but this felt different, almost\u2026 clinical, \"We often remember things in ways that are\u2026 easier for us to digest.\"\n\nI was fidgety. \u201cGees Perry. You sound like Dr. Lasko.\u201d\n\nHe seemed to enjoy my little joke.\n\nDr. Lasko had been my therapist since the accident, and I would go out on a limb and say that he would not have approved of this self-inflicted exposure therapy I was subjecting myself to by visiting the house.\n\nPerry seemingly snapped out of his little therapist moment and went right back to being his sparkly confetti self. As I watched his amicable face scan the mantle above the fireplace, I felt a sickening uneasiness. Imagine you had actually fucking killed him.\n\n\u201cMan, I can still picture all our family photos up there,\u201d Perry\u2019s childlike wonder destroyed me.\n\nMy face went flush. I could feel the water rising in my eyes like the tides. How pathetic and infuriating was it that after everything I\u2019d done, I was still somehow the victim.\n\n\u201cHey.\u201d He took my hand.\n\n\u201cOh Perry,\u201d  I threw myself into him. \u201cI\u2019m so sorry.\u201d\n\nMy brother held me with his usual care that I didn\u2019t deserve.\n\n\u201cJacks, it\u2019s ok. I\u2019m still here. We\u2019re both still here.\u201d\n\nAs my chin rested on my brother\u2019s shoulder, I looked ahead of me at the remains of the house. Something felt off, and it wasn\u2019t just the overall unsettling environment. My brow furrowed. \u201cWasn\u2019t the doorway to the kitchen on the other side of the living room?\u201d\n\nI felt Perry shrug, \u201cI don\u2019t think so.\u201d\n\nI was staring into the out of place doorway like I was trying to pull a recollection towards me. And that\u2019s when I saw them in the kitchen: the translucent memory of mom and dad. Mom was getting our Apple-Schnapples ready. She was pacing, unlike Dad, who was sitting motionless at the table. His face was planted on its mahogany surface. His glass of Apple-Schnapple was empty, and so was the bottle of Jim Beam beside it.\n\nMom floated into the living room, our warm beverages in hand and a cigarette in her mouth, \u201cKids, your father\u2019s not feeling well. Let\u2019s have our Apple-Schnapples in here.\u201d\n\nOh my God. The bruise on her face.\n\nPerry jarred me back to reality with the gut punch of what he had to say next, \u201cYou\u2019re remembering the truth about mom and dad, aren\u2019t you?\u201d\n\nI pushed myself off and away from him. \u201cHow did you\u2026\u201d\n\nMy brother looked down, delicately tracing the floor with his finger, \u201cWe always put them on a pedestal after they died.\u201d\n\nI felt a shiver run down my spine, \u201cWhat are you talking about?\u201d\n\nAs Perry continued to speak, his words grew even more detached. \"Do you remember that first drink Dad ever gave you?\"\n\nMy eyes darted around the room as my jugular pulsed in my neck. As much as I tried to escape what Perry had just said, I did remember.\n\nI could hear my dad\u2019s slurring words of encouragement, \u201cCome on, Jackie. Just one drink. It\u2019ll be our special time, just you and me.\u201d\n\nThe bitterness of that first sip of beer made me squirm, but sharing a \u201cspecial time\u201d with my dad\u2014and the desperate yearning that maybe he did love me, afterall\u2014was the overwhelm of the full moon swallowing me whole. I was only a child, and much like how my mom turned a blind eye to my father\u2019s drinking, she did the exact same when it came to her daughter.\n\nI\u2019d used the death of my parents as the excuse for my alcoholism for so long, because admitting that they helped create the monster I would eventually become was like a knife to the heart. And knowing I had been too weak to conquer the addiction from my own volition just made the weapon twist in my chest.\n\nThe room was spinning. My face was blistering hot like the night of the fire. Or was that the warm heat from the fireplace when we were kids? The childhood fireplace memories ran through my mind, frame by frame, until\u2026..they eventually vaporized to nothing. I crouched over, thinking I might vomit.\n\n\u201cWe never had a fireplace.\u201d Perry was nodding, very matter-of-factly.\n\nMy fingernails dug into my thighs when I looked at the fireplace and: it was gone. Only a blank wall of faded, forest green wallpaper remained. Our house was once on fire, yes, but that was it. There was never a happy childhood fireplace. Ever.\n\nMy hands were cold and clammy. I fell back onto the wall behind me. \u201cPerry. Where are we?\u201d\n\nHe stood up and glided towards the staircase. One hand on the banister, his footsteps creaked, one by one, as he made his way to the second floor.\n\nMy mouth was bone dry, \u201cPerry!\u201d\n\nHe stopped and turned towards me, \u201cCome to the projector room. We love watching movies together, don\u2019t we? There\u2019s a movie I\u2019d like to show you.\u201d\n\nAs my brother disappeared from sight, I did what any classic horror movie trope would tell you to do: I went upstairs.\n\nI found Perry standing at the end of the ominous hallway. Large, empty picture frames lined the oxblood walls leading up to him. Through the doorway where Perry stood, particles in the air danced in the projector\u2019s cone-shaped light. That telltale winding of a film reel was the only sound in the deafening quiet of this house that I no longer recognized.\n\nHalf of Perry\u2019s face\u2014the one with the scar\u2014was perfectly illuminated, as though he was wearing the mask from \u201cThe Phantom of the Opera\u201d. \u201cI think you\u2019re ready to see how this movie ends, Jackie. This is the most progress you\u2019ve made since we\u2019ve been coming here.\u201d\n\nI gripped my thumbs in the palms of my hands, \u201cPerry, you\u2019re freaking the fuck out of me!\u201d\n\nI thought my knees might buckle as my brother\u2019s face glitched, like a flash of static snow on a television set. As his face settled back to normal to a deadpan gaze, he disappeared in the innocuous room. I followed, running on nothing but fumes.\n\nClutching the doorway, my mouth fell agape. Perry was gone. I darted to the middle of the room.\n\nAs I frantically searched for my brother, I shielded my eyes with the back of my hand from the projector\u2019s light. And that\u2019s when, from behind me, I heard five words that made my blood run cold, \u201cJackie! Stop the fucking car!\u201d\n\nI was convulsing yet paralyzed. Moving as slowly as cold molasses, I rotated on the spot towards my worst nightmare, shown on 35 mm. On the projector screen was Perry and me in my car, exactly seven hundred and thirty days ago, the day I almost kill\u2014\n\nOh my God.\n\nMy head pounded as fragmented memories surged. The reality of what happened began to crystallize, unrelenting.\n\nMy joints ached and my stomach churned. Clamping a hand over my mouth to stifle a scream, I stumbled down the hallway as it began caving in on itself. The picture frames were sucked into the walls. The floorboards cracked into distorted peaks and valleys. Wooden beams swung down from the ceiling like pendulums. I tried to spit out the chalky grit of disintegrating drywall that made the hallway look like a winter squall.\n\nPanting heavily, I stopped dead in my tracks at the stained glass window. My body trembled with an all-too-familiar dread. Each time I faced this, I wondered if this fall would be the one that would finally end it all.\n\nMaybe it\u2019d be better if it did.\n\nHolding my breath, I threw myself through the glass, my hands clawing the air for dear life. Free falling two stories feels like an eternity when you\u2019re watching the memory of your childhood home fall apart before your very eyes. But when the weight of my body finally made contact with the earth I\u2014\n\nI gasped. The cold air of the hospital room shocked my lungs. I sat up, ripping the suction cups from my face and body. My gown was clinging to me, soaked in sweat. Medical equipment beeped all around me like a metronome.\n\nDr. Lasko, my therapist since the accident, sat across the stark white room, sighing as he rubbed his forehead. He, too, was connected to a plethora of suction cups and wires. He looked a little worn out in the fluorescent overhead lighting. Ever since I was hospitalized and later incarcerated, Dr. Lasko had been helping me delve into my memories, namely the ones that were too excruciating for me to face. And as such, Dr. Lasko had been appearing in the simulations as my brother Perry, the love of my life who died in the car crash, seven hundred and thirty days prior.\n\nDisoriented, I blinked rapidly, the vividness of the memory contrasting sharply with the sterile, geometric ceiling tiles above me.\n\n\u201cI don\u2019t ever want to do that again!\u201d I was venomous.\n\n\u201cJackie,\u201d Dr. Lasko started.\n\n\u201cDon\u2019t start,\u201d I pulled up four fingers for air quotes, \u201c\u2018Jackie, don\u2019t give up. This was the closest you\u2019ve ever come to facing the truth.\u201d\n\nAs the initial burst of adrenal and cortisol left my body, I fell back on my pillow. I was depleted. Quiet rivers flowed down my cheeks.\n\nRemoving his own suction cups, Dr. Lasko approached my bedside and took a seat. He treaded lightly. \u201cJackie, I understand how challenging this is for you, but you did an incredible job today. If we continue making progress like this, there's a real possibility you'll gain your freedom sooner.\u201d\n\nI looked at the well-meaning doctor, but all I could see was Perry. Multicolored confetti fell softly around him like that first November snow. His face was the sun. His eyes reflected the whole world back to me.\n\nPerry.\n\nWith a weak grip, I took Dr. Lasko\u2019s hand. My vocal cords were like sandpaper. \u201cI\u2019ll leave this place one day, doc.\u201d A single tear dripped from my chin onto my collarbone. \u201cBut I\u2019m not sure if I\u2019ll ever be free.\u201d\n\nDr. Lasko didn\u2019t say a word, but I felt him squeeze my hand, just a little bit tighter.\n\nI licked the cracks on my lips as my eyes closed shut, imagining the oaky comfort of bourbon on my tongue. I felt myself drift, and good thing, because I needed the rest. Dr. Lasko and I would be delving into my memories again the following day.\n\nNo matter how masochistic it felt, I vowed to keep showing up for the simulations. Even if I never forgave myself for what I did, at least in my memories, I got to see Perry.","query":"What things did Jackie remember differently to what she saw?","ground_truth":"In her and Perry's childhood home, Jackie didn't remember having a projector room and she remembered the doorway to the kitchen being on the other side of the living room. She also remembered that she had almost killed her brother, Perry, seven hundred and thirty days ago in a car accident, when she actually did kill her brother in that accident.","response_correct":"Jackie misremembered having a projector room and thought the kitchen doorway was in a different location. She mistakenly believed she almost killed Perry in the car accident, but he actually died in it.","response_wrong":"Jackie clearly remembered having a large library in their childhood home and thought she nearly hit a cyclist in the accident, rather than harming Perry."}
{"document_index":17,"context":"Alan Wake 2\nWhy the hell did you kill Casey? What the hell were you thinking, man?\nThis article or a section of this article will contain full, or partial plot spoilers of an Alan Wake game or any other piece of media related to the franchise.\nYou have been warned...\n\n\"This story... is a monster. And monsters wear many faces.\"\n\u2015 Alan Wake\n\nAlan Wake 2 (stylized as Alan Wake II) is a survival-horror video game that released on 27 October 2023 on digital storefronts for the PlayStation 5, Xbox Series X|S and PC through the Epic Games Store. It is the sequel to 2010's Alan Wake. The game had an expansion DLC released on June 8, 2024 called Night Springs and a second expansion DLC, The Lake House, is slated to be released sometime later. The game has four difficulties: Story, Normal, Hard, and Nightmare, the fourth of which is only available in \"The Final Draft\" mode. The Final Draft is a New Game Plus mode for Alan Wake 2, featuring new Manuscript pages, other new collectibles, a new expanded ending, and other changes to the game's story. \"The Final Draft\" was released on December 11, 2023.\n\nPatch notes for Alan Wake 2 updates can be found here.\n\n\nContents\n1\tSynopsis\n2\tPlot\n3\tChapter List\n3.1\tThe Final Draft\n3.2\tExpansion 1: Night Springs\n4\tGameplay\n5\tDevelopment\n5.1\tInitial development\n5.2\tRemedy Connected Universe\n5.3\tOfficial development\n6\tReception\n7\tTrivia\n8\tGallery\n8.1\tOfficial Images\n8.2\tConcept art\n8.3\tConcept art (earlier iteration)\n9\tVideos\n10\tSources\nSynopsis\nQuote1 A string of ritualistic murders threatens Bright Falls, a small-town community surrounded by Pacific Northwest wilderness. Saga Anderson, an accomplished FBI agent with a reputation for solving impossible cases arrives to investigate the murders. Anderson\u2019s case spirals into a nightmare when she discovers pages of a horror story that starts to come true around her.\n\nAlan Wake, a lost writer trapped in a nightmare beyond our world, writes a dark story in an attempt to shape the reality around him and escape his prison. With a dark horror hunting him, Wake is trying to retain his sanity and beat the devil at his own game.\n\nAnderson and Wake are two heroes on two desperate journeys in two separate realities, connected at heart in ways neither of them can understand: reflecting each other, echoing each other, and affecting the worlds around them.\n\nFueled by the horror story, supernatural darkness invades Bright Falls, corrupting the locals and threatening the loved ones of both Anderson and Wake. Light is their weapon\u2014and their safe haven \u2014 against the darkness they face. Trapped in a sinister horror story where there are only victims and monsters, can they break out to be the heroes they need to be? Quote2\n\u2015 Epic Games Store page description\nPlot\nSee also: Alan Wake, Alan Wake's American Nightmare, and Control\nIn 2023, thirteen years after the events of Alan Wake, a naked man emerges from the dark shores of Cauldron Lake, experiencing visions of a man violently screaming before he is found and has his heart cut out by a group of men wearing deer masks.\n\nFBI Special Agent Saga Anderson and her partner Alex Casey are dispatched to the town of Bright Falls, Washington to investigate a series of ritualistic murders. They investigate the latest victim, the naked man, who is revealed as former FBI Agent Robert Nightingale. It is believed he was murdered by a group calling themselves the \"Cult of the Tree\". In addition to Nightingale's corpse, Saga finds a mysterious manuscript page that seems to predict the future. While in the town, they encounter Sheriff Tim Breaker, who promises to assist their investigation, as well as waitress Rose Marigold, who recognizes Saga and acts like Saga was a local of the area, despite this being her first visit to the town. She also claims Saga's daughter Logan drowned several years ago, despite being alive in Virginia with her father. At the morgue, Saga then performs an autopsy on Nightingale's corpse, but it suddenly reanimates, in search of the \"Clicker\", and escapes the morgue after Breaker mysteriously vanishes.\n\nAs they pursue Nightingale to Cauldron Lake, Saga uses her psychic powers to investigate the scene. Casey reveals he had investigated a murder cult in New York City dedicated to bringing back the missing author Alan Wake by reenacting the murders described in his books, and that Nightingale came to Bright Falls to pursue Alan thirteen years prior. Saga encounters Nightingale, now converted into what the manuscript called a \"Taken\", and is forced to kill him. Saga then finds a very-shaken Alan Wake washed up on the shore of the lake and takes him into custody while also discovering evidence that an organization called the Federal Bureau of Control has a presence in Bright Falls. Saga and Casey take Alan back to the Elderwood Palace Lodge in their field office where he recounts how he escaped from a dream-like dimension called the Dark Place.\n\nWhile trapped in the Dark Place, Alan continually tried to find a way to escape. After appearing in a mysterious talk show called \"In Between With Mr. Door\", Alan found an Angel Lamp once belonging to Thomas Zane and connected to the Clicker that, in combination with his writing ability, allowed him to manipulate the Dark Place to better navigate it. Alan began navigating a dark, twisted version of New York City, following the trail of Alex Casey's investigation of the \"Cult of the Word\" led by Alan's evil doppelganger Scratch, who survived his erasure from existence, which brought him through the various scenes of the murders committed by the cult. Along the way, Alan also encountered Breaker, who was searching for the talk show host, Warlin Door, believing him to be responsible for trapping him in the Dark Place. Alan also occasionally made brief contact with Saga, the two of them trading vital information when they could. After believing he had killed Tom Zane, who had gone insane from his captivity inside the Dark Place, Alan was contacted by an alternative version of himself, explaining that his repeated attempts to escape the Dark Place are causing him to experience time loops.\n\nBack in the present, Alan explains that he wrote a new novel, \"Return\", that helped him escape the Dark Place. However, Scratch re-edited the manuscript into a horror story now taking effect in reality. Alan warns that Scratch is searching for the Clicker, which is the key he needs to free the malevolent Dark Presence completely, but also the key to permanently defeating it. When one of the manuscript pages mentioning the Cult of the Tree possesses the Clicker, Saga decides to follow its trail in Watery and realizes that \"Return\" has rewritten her past so that her daughter Logan drowned in an accident. Angered that Alan wrote her and Logan into his story, Saga recovers the Clicker, but before she can get it to Alan, agents from the FBC led by Agent Kiran Estevez arrive and apprehend him as well as the Cult leaders, revealed to be town entrepreneurs Ilmo and Jaakko Koskela.\n\nWith no other options, Saga goes to the Valhalla Nursing Home and tracks down Odin and Tor Anderson, who know more about the Clicker. As she communicates with them and rescues Tor from a Taken Cynthia Weaver, she learns that Odin and Tor are actually her grand-uncle and grandfather, respectively, and she inherited their seer abilities, allowing her to discern the changes to reality \"Return\" is causing. Odin and Tor also explain that the Clicker does not do anything by itself, but instead dramatically enhances the creative power of the person using it, which is why it is important to Alan and Scratch. Saga then heads back to Bright Falls to get the Clicker to Alan, but finds out that the Alan who escaped Cauldron Lake was actually Scratch. With his identity now exposed, Scratch kills Jaakko and escapes from captivity, attempting to take the Clicker from Saga. He is temporarily thwarted and banished by the FBC.\n\nIlmo reveals that he and Jakko formed the cult in order to scare townsfolk away from the dangers of the lake, while they perform their ritual killings on whatever Taken emerges from it, such as Nightingale. Realizing that Alan is still trapped in the Dark Place, Saga enlists the help of Casey, Odin, Tor, and Estevez to carry out a ritual to summon him to the real world. Meanwhile, Alan continues to try and find a way out of the Dark Place, eventually making his way to his old apartment. There, he discovers his wife Alice had been tormented with visions of Scratch, eventually leading to her apparent suicide. In anger, Alan kills who he believes is Scratch, but is actually a version of himself attempting to fix Scratch's ending of \"Return\", perpetuating the loop.\n\nBack in the real world, the summoning ritual fails to summon Alan in the current time and instead was responsible for his initial appearance earlier in the story. Both Saga and Alan come to the realization that Alan and Scratch were always the same person, with Scratch being Alan, who is possessed by the Dark Presence at the end of every loop, and thus indirectly responsible for Alice's death. Scratch arrives at the summoning site and Saga is able to banish him from Alan's body, only for Scratch to possess Casey instead, steal the Clicker, and throw Saga into the Dark Place.\n\nWith Scratch in possession of the Clicker, Alan concludes that the only way to stop him now is to find the manuscript and write a completely new ending for \"Return\". He returns to his writing room and tries to work out how to write a perfect ending that saves everybody while staying consistent with the horror genre. Meanwhile, trapped in her mind place, Saga fights off the Dark Presence's attempt to overwhelm her with her own negative emotions and self-doubt. Now in the Dark Place proper, she reunites with Breaker, who remains there in his search for Door, heavily implied to be Saga's missing father. She then receives aid from an anonymous woman who directs her to the Clicker and a Bullet of Light. Saga takes the two items and escapes back to the real world by mimicking Door's actions on a manuscript page given to her by Breaker. With the Clicker, Alan is able to banish Scratch from Casey's body. Scratch returns to possessing Alan, and Saga shoots him with the Bullet of Light, seemingly killing him and Scratch. As Casey recovers from his possession, Saga tries to call Logan to confirm her safety, but the scene cuts short before any answer can be heard.\n\nIn a post-credits scene, a recording left behind by Alice reveals that she had managed to fully regain her memories of what happened to Alan after consulting with the FBC at the Oldest House. She had tricked Alan into thinking she committed suicide by choosing to return the Dark Place as part of a plan to help him eventually escape, explaining that the only way he can escape the loops is through \"ascension.\" Alan then revives from his gunshot wound and says, \"It's not a loop, it's a spiral.\"\n\nIn the \"Final Draft\" ending, Alan realizes that the Bullet of Light was meant to cleanse a part of himself the Dark Presence used to create Scratch, finally killing his doppelg\u00e4nger for good while finishing the spiral and freeing Alan from the loop. Saga's call connects, confirming Logan's survival, and Alan revives soon after to the surprise of her and Casey, declaring himself the master of not just two worlds, but many.\n\nChapter List\nThe chapters\/parts listed here are played in the following order:\nPrologue: The Cult\nReturn 1: Invitation\nReturn 2: The Heart\nInitiation 1: Late Night\nInitiation 2: Casey\nInitiation 3: Haunting\nPlayers have the choice to play the following of Alan and Saga's chronological chapters\/parts in any order they wish:\nReturn 3: Local Girl\nReturn 4: No Chance\nReturn 5: Old Gods\nReturn 6: Scratch\nInitiation 4: We Sing\nInitiation 5: Room 665\nInitiation 6: Return\nInitiation 7: Masks\nInitiation 8: Zane's Film\nThe chapters\/parts listed here are past the point of no return and are played in the following order:\nReturn 7: Summoning\nInitiation 9: Gone\nReturn 8: Deerfest\nReturn 9: Come Home\nThe Final Draft\nRemedy released a New Game Plus update to the game on December 11th, 2023 named \"The Final Draft\".\n\nExpansion 1: Night Springs\nSet as \"episodes\" of the Night Springs TV show, these can be played in any order the player wishes.\n\nNumber One Fan\nNorth Star\nTime Breaker\nGameplay\nSimilar to the original, the game is a third-person shooter, with players using light to burn away the darkness of the enemy and various weapons to fight against the dark forces as well as various cult members. Unlike the first game, which was an action-adventure game, Alan Wake 2 emphasizes survival-horror gameplay. Players have an inventory with limited space, where they will need to conserve their ammo and healing items. Health items themselves consist of med kits and painkillers, with Safe Havens only healing the player to take them out of a \"critical state.\" Players also encounter various puzzles and locked boxes that they'll need to solve in order to obtain upgraded gear.\n\nThe game features two distinct campaigns, one where you play as the titular Alan Wake, and one where you play a new protagonist, Saga Anderson. The two campaigns blend together, eventually allowing you to switch between the two at various points, using a dark puddle found in certain Break Rooms if the player wishes, with the two stories both foreshadowing and echoing each other. Eventually though, the player will be required to complete both stories before progressing past a point of no return.\n\nSaga\u2019s story takes place in the Pacific Northwest, consisting of three hubs: the small town of Bright Falls, the stunning primordial forest surrounding the mysterious Cauldron Lake, and the run-down town of Watery. Alan's story takes place solely within the Dark Place in a warped version of New York City, with his reality shifting and looping. Similarly to the episodic nature of the original, the narrative is divided into chapters, which each having their own accompanying end song. Between chapters, players are returned to the hubs and are allowed to explore or move to the next chapter at their own pace.\n\nWhilst Saga's story involves a lot of talking to various townsfolk around the area in order to progress in her environment, Alan uses an Angel Lamp to allow him to change a scene by transferring light from one area of New York to another. Both characters have their own \"mind palaces\" to help progress their stories, with Alan having his Writer's Room to help change the story and Saga having her Mind Place to help her with her investigations, both of which can be accessed at the press of a button.\n\nDevelopment\nInitial development\nThroughout 2011 and 2012, several hints had been dropped of a potential sequel to Alan Wake. These hints include when \"Alan Wake 2\" was shown on the Artist Althea Suarez Gata's CV, only for it to be removed on the very same day. Whilst Alan Wake's American Nightmare (though the name was unknown at the time) was revealed not long after this hiccup, fans had been trying to find more evidence of a direct sequel. In Alan Wake's American Nightmare, the credits to the game also then indicated that \"Alan Wake's journey through the night will continue\".\n\nAlso in 2012, Sam Lake tweeted a link to a \"Blogspot\" called \"This House of Dreams.\" At first, the site seemed like a mere blog for someone refurbishing a house, but one such blog talks about a dream the blogger had about a man \"wearing a funny jacket with old-fashioned elbow patches\" and that he wanted \"to turn all the lights on.\" This tweet came just days before E3 2012, a gaming expo that reveals and shows brand new content of upcoming releases. People also found that the domain \"alanwake2.com\" was also reserved by Remedy Entertainment.\n\nAt the Xbox Event on May 21, 2013, Remedy revealed Quantum Break, which would be an exclusive to the newest Xbox console at the time, Xbox One. On February 2016, it was confirmed that it would also come to PC. Fans were disappointed that Alan Wake 2 wasn't revealed, but some still supported Remedy in their new franchise. A day after the Quantum Break announcement, Sam Lake took to YouTube to talk to the fans about why Alan Wake 2 wasn't announced. He revealed that the time just wasn't right yet, but mentioned he had not given up on the franchise.\n\nIn April 2015, Remedy announced that Alan Wake 2 had previously gone into the early stage of development shortly after Alan Wake was released, where a video of prototype demo was also released. While in the stage of showing this demo to publishers, Remedy went into conversations with Microsoft and the project was put on hold indefinitely while Quantum Break went into development. The sequel would have introduced new features, many of which were used in Alan Wake's American Nightmare. When asked about the possibility of an Alan Wake sequel being released in the future, Sam Lake replied, \"It's hard to guess,\" but that he would \"love to do that\".\n\nRemedy Connected Universe\nEaster eggs in Remedy's 2019 game Control revealed that the game shared the same universe as Alan Wake. Two typewritten pages found within the game (and which were said to have been written by Alan Wake himself) heavily imply that almost ten years had passed since Alan's experience in Bright Falls, and that it was not an isolated event, but rather what the Federal Bureau of Control classifies as an \"Altered World Event\" (AWE). In this way, Control revealed additional information about the events involving the Dark Presence, especially given the nature of its own primary enemy, the Hiss.\n\nFurthermore, the second DLC for Control, AWE, was released in August 2020. While the letters can simply stand for \"Altered World Event,\" they also resemble Alan's own name. In addition to this, the top half of the cover art depicted a flashlight-wielding person in a dark forest, while the bottom half depicted a second figure surrounded by red, which figures prominently with the Hiss in Control.\n\nIn an interview given on May 14, 2020 with VG24\/7, Sam Lake commented on this, explaining, \"Through AWE you\u2019ll learn more about the Bureau\u2019s involvement, research, and interest in Alan Wake and the events that took place 10 years ago,\u201d Lake said. \u201cWe\u2019re really happy to give fans of the original game a bit more of Alan Wake-related content.\"[1] This suggested a deeper connection, possibly even a future crossover, between the events of Control and Alan Wake.\n\nThe plot of AWE confirmed this, tying up some loose ends from the previous Alan Wake entries while also setting up details which would come in a sequel work. In the ending of AWE, an alarm sounds in the Federal Bureau of Control that another AWE appears to be happening again in Bright Falls. This, along with Alan's narration, seemed to indicate Remedy's next game could be a follow up to Alan Wake.\n\nOfficial development\nIn March 2020, Remedy signed a deal with Epic Games to publish two single-player games under Epic's new publishing initiative. The games were said to be part of \"the same franchise\". In March 2021, Remedy CEO Tero Virtala confirmed that they were indeed working on two unannounced project with Epic Games that are part of the same franchise, one smaller scale and one AAA in size. In April 2021, GamesBeat reporter Jeff Grubb told viewers that \"I've heard that Remedy is going to be making Alan Wake 2,\" as part of the Remedy-Epic deal. Grubb went on to say that the game \"should kind of be the follow-up that fans of that series want,\" but that there's little extra information and no word on an official announcement.\n\nOn the 7 September 2021, Alan Wake Remastered was revealed by Sam Lake, of which some people believed to have been the smaller project mentioned in March 2021, and the other AAA from the same franchise was to be Alan Wake 2. With the reveal of Alan Wake Remastered, which indicated Remedy are trying to generate interest in the franchise, a sequel was looking more and more likely.\n\nAt The Game Awards 2021, Alan Wake 2 was officially announced for 2023 and would be Remedy's first survival-horror video game. Like Quantum Break and Control, it was announced that Alan Wake 2 would use the Northlight engine and be a third-person game. On a later interview with IGN, Sam Lake revealed that the game would have a Mature 17+ age rating.\n\nOn May 24, 2023, Sony hosted a PlayStation Showcase livestream, in which a gameplay trailer for Alan Wake 2 was revealed, with a release date of October 17, 2023. It was revealed that players would control two protagonists within their own campaigns, Alan Wake himself and a new protagonist, Saga Anderson. At Summer Game Fest on June 8, 2023, Sam Lake shared further information on the ways Alan and Saga's campaigns would be connected to each other. Additionally, more gameplay and combat mechanics were shown.\n\nOn August 17, 2023, the game was delayed by 10 days to October 27, 2023 to make way for various other games also releasing in October. At Gamescom 2023, Alan Wake's side of the game in the Dark Place was focused on in a new trailer, which was later followed by a lengthy Alan-centric gameplay featured behind closed doors.\n\nReception\nAlan Wake 2 received near universal acclaim from video game critics. Itr received five nominations at the 2023 Golden Joystick Awards and won the Critic's Choice Award. It was nominated for eight awards at The Game Awards 2023, and won for Best Game Direction, Best Narrative, and Best Art Direction.\n\nThe game went on to win more awards in Finland and around the world.\n\nTrivia\nIlkka Villi and Matthew Porretta both reprise their role as Alan Wake in the sequel. James McCaffrey reprises his role as Alex Casey, who he previously voiced in the manuscript pages of The Sudden Stop.\nBrett Madden, who played Alice Wake in the first game, sadly passed away in September 2020. She was memorialized in the credits of the game. Alice was recast in the sequel, now played by Christina Cole.\nWhile the first game was an action-adventure game that implemented horror elements, Alan Wake 2 is a full-blown survival-horror game.\nUnlike the first game that was published by Microsoft Studios, the sequel is published by Epic Games Publishing.\nThe game was initially a digital-only release due to various shifts towards a digital-only market. This was also done to keep the price of the game down at launch. However despite initially saying there were no plans for a physical release, in June 2024 along with a trailer and release date announcement of the first DLC it was revealed that a physical version would be released that October, a year after the digital release. Both a physical version of the digital deluxe version (inclusive of the DLC) and a limited run collector's edition were announced.\nThe second entry in Remedy's Max Payne series, Max Payne 2: The Fall of Max Payne, also had two playable protagonists (Max Payne and Mona Sax).","query":"What is Alan tricked into believing by his wife?","ground_truth":"Alan believes that while being possessed by the Dark Presence, he torments Alice with visions of his alter ego, Scratch, which eventually leading to her suicide. Alice had actually tricked Alan into thinking she had committing suicide by choosing to return the Dark Place as part of a plan to help him eventually escape, explaining that the only way he can escape the loops is through \"ascension.\"","response_correct":"Alan is led to think his wife, Alice, has taken her own life. However, she faked her death and returned to the Dark Place to help him escape.","response_wrong":"Alan is convinced by Alice that she has gone to the Dark Place to rescue him directly, leaving clues for him to follow."}
