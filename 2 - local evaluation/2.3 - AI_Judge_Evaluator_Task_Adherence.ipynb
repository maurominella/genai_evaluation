{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Adherence Evaluator\n",
    "\n",
    "# Intent Resolution Evaluator\n",
    "\n",
    "## Objective\n",
    "This sample demonstrates to how to use task adherence evaluator on agent data. The supported input formats include:\n",
    "- simple data such as strings;\n",
    "- user-agent conversations in the form of list of agent messages. \n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend about 10 minutes running this notebook. \n",
    "\n",
    "## Before you begin\n",
    "For quality evaluation, you need to deploy a `gpt` model supporting JSON mode. We recommend a model `gpt-4o` or `gpt-4o-mini` for their strong reasoning capabilities.    \n",
    "\n",
    "### Prerequisite\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity azure-ai-evaluation\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **PROJECT_CONNECTION_STRING** - The project connection string, as found in the overview page of your Azure AI Foundry project.\n",
    "2) **MODEL_DEPLOYMENT_NAME** - The deployment name of the model for this AI-assisted evaluator, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n",
    "3) **AZURE_OPENAI_ENDPOINT** - Azure Open AI Endpoint to be used for evaluation.\n",
    "4) **AZURE_OPENAI_API_KEY** - Azure Open AI Key to be used for evaluation.\n",
    "5) **AZURE_OPENAI_API_VERSION** - Azure Open AI Api version to be used for evaluation.\n",
    "6) **AZURE_SUBSCRIPTION_ID** - Azure Subscription Id of Azure AI Project\n",
    "7) **PROJECT_NAME** - Azure AI Project Name\n",
    "8) **RESOURCE_GROUP_NAME** - Azure AI Project Resource Group Name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "This sample demonstrates how to use Task Adherence Evaluator\n",
    "Before running the sample:\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity azure-ai-evaluation\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **PROJECT_CONNECTION_STRING** - The project connection string, as found in the overview page of your Azure AI Foundry project.\n",
    "2) **MODEL_DEPLOYMENT_NAME** - The deployment name of the AI model, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n",
    "3) **AZURE_OPENAI_ENDPOINT** - Azure Open AI Endpoint to be used for evaluation.\n",
    "4) **AZURE_OPENAI_API_KEY** - Azure Open AI Key to be used for evaluation.\n",
    "5) **AZURE_OPENAI_API_VERSION** - Azure Open AI Api version to be used for evaluation.\n",
    "6) **AZURE_SUBSCRIPTION_ID** - Azure Subscription Id of Azure AI Project\n",
    "7) **PROJECT_NAME** - Azure AI Project Name\n",
    "8) **RESOURCE_GROUP_NAME** - Azure AI Project Resource Group Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Task Adherence evaluator measures how well the agent adheres to their assigned tasks or predefined goal.\n",
    "\n",
    "The scoring is on a 1-5 integer scale and is as follows:\n",
    "\n",
    "  - Score 1: Fully Inadherent\n",
    "  - Score 2: Barely Adherent\n",
    "  - Score 3: Moderately Adherent\n",
    "  - Score 4: Mostly Adherent\n",
    "  - Score 5: Fully Adherent\n",
    "\n",
    "The evaluation requires the following inputs:\n",
    "\n",
    "  - Query    : The user query. Either a string with a user request or a list of messages with previous requests from the user and responses from the assistant, potentially including a system message.\n",
    "  - Response : The response to be evaluated. Either a string or a message with the response from the agent to the last user query.\n",
    "\n",
    "There is a third optional parameter:\n",
    "  - ToolDefinitions : The list of tool definitions the agent can call. This may be useful for the evaluator to better assess if the right tool was called to adhere to user intent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Task Adherence Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai endpoint: <https://mmoaiswc-01.openai.azure.com/>\n",
      "azure deployment name: <gpt-4o>\n",
      "openai api version: <2025-04-01-preview>\n"
     ]
    },
    {
     "ename": "EvaluationException",
     "evalue": "(UserError) Model config validation failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ProgramData\\miniconda3\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:187\u001b[39m, in \u001b[36mvalidate_model_config\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_typed_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAzureOpenAIModelConfiguration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ProgramData\\miniconda3\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:292\u001b[39m, in \u001b[36m_validate_typed_dict\u001b[39m\u001b[34m(o, t)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m o.items():\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     \u001b[43mvalidate_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(T_TypedDict, o)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ProgramData\\miniconda3\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:279\u001b[39m, in \u001b[36m_validate_typed_dict.<locals>.validate_annotation\u001b[39m\u001b[34m(v, annotation)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(origin \u001b[38;5;129;01mis\u001b[39;00m g \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m (NotRequired, Required)):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[43mvalidate_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ProgramData\\miniconda3\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:277\u001b[39m, in \u001b[36m_validate_typed_dict.<locals>.validate_annotation\u001b[39m\u001b[34m(v, annotation)\u001b[39m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mtype\u001b[39m(literal) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(v) \u001b[38;5;129;01mand\u001b[39;00m literal == v \u001b[38;5;28;01mfor\u001b[39;00m literal \u001b[38;5;129;01min\u001b[39;00m literal_args):\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected value to be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(literal_args)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m. Received type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(origin \u001b[38;5;129;01mis\u001b[39;00m g \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m (NotRequired, Required)):\n",
      "\u001b[31mTypeError\u001b[39m: Expected value to be one of ['azure_openai']. Received type <class 'str'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ProgramData\\miniconda3\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:190\u001b[39m, in \u001b[36mvalidate_model_config\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_typed_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOpenAIModelConfiguration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ProgramData\\miniconda3\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:229\u001b[39m, in \u001b[36m_validate_typed_dict\u001b[39m\u001b[34m(o, t)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unknown_keys:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdict contains unknown keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(unknown_keys)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m required_keys = {\n\u001b[32m    232\u001b[39m     k\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m annotations\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (is_total \u001b[38;5;129;01mand\u001b[39;00m get_origin(annotations[k]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotRequired)\n\u001b[32m    235\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m is_total \u001b[38;5;129;01mand\u001b[39;00m get_origin(annotations[k]) \u001b[38;5;129;01mis\u001b[39;00m Required)\n\u001b[32m    236\u001b[39m }\n",
      "\u001b[31mTypeError\u001b[39m: dict contains unknown keys: ['azure_deployment', 'azure_endpoint', 'api_version']",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mEvaluationException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mazure deployment name: <\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_config[\u001b[33m\"\u001b[39m\u001b[33mazure_deployment\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m>\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mopenai api version: <\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_config[\u001b[33m\"\u001b[39m\u001b[33mapi_version\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m>\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m task_adherence = \u001b[43mTaskAdherenceEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ProgramData\\miniconda3\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\_experimental.py:80\u001b[39m, in \u001b[36m_add_class_docstring.<locals>._add_class_warning.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_skip_warning() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_warning_cached(message):\n\u001b[32m     79\u001b[39m     module_logger.warning(message)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ProgramData\\miniconda3\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluators\\_task_adherence\\_task_adherence.py:73\u001b[39m, in \u001b[36mTaskAdherenceEvaluator.__init__\u001b[39m\u001b[34m(self, model_config, threshold, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m prompty_path = os.path.join(current_dir, \u001b[38;5;28mself\u001b[39m._PROMPTY_FILE)\n\u001b[32m     72\u001b[39m \u001b[38;5;28mself\u001b[39m.threshold = threshold\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompty_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompty_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_key\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_RESULT_KEY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ProgramData\\miniconda3\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluators\\_common\\_base_prompty_eval.py:79\u001b[39m, in \u001b[36mPromptyEvaluatorBase.__init__\u001b[39m\u001b[34m(self, result_key, prompty_file, model_config, eval_last_turn, threshold, _higher_is_better, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m subclass_name = \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\n\u001b[32m     77\u001b[39m user_agent = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mUserAgentSingleton().value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (type=evaluator subtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m prompty_model_config = construct_prompty_model_config(\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[43mvalidate_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     80\u001b[39m     \u001b[38;5;28mself\u001b[39m._DEFAULT_OPEN_API_VERSION,\n\u001b[32m     81\u001b[39m     user_agent,\n\u001b[32m     82\u001b[39m )\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m._flow = AsyncPrompty.load(\n\u001b[32m     85\u001b[39m     source=\u001b[38;5;28mself\u001b[39m._prompty_file, model=prompty_model_config, is_reasoning_model=\u001b[38;5;28mself\u001b[39m._is_reasoning_model\n\u001b[32m     86\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ProgramData\\miniconda3\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:193\u001b[39m, in \u001b[36mvalidate_model_config\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    192\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mModel config validation failed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[32m    194\u001b[39m         message=msg, internal_message=msg, category=ErrorCategory.MISSING_FIELD, blame=ErrorBlame.USER_ERROR\n\u001b[32m    195\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mEvaluationException\u001b[39m: (UserError) Model config validation failed."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.evaluation import TaskAdherenceEvaluator, AzureOpenAIModelConfiguration\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv # requires python-dotenv\n",
    "\n",
    "if not load_dotenv(\"./../../config/credentials_my.env\"):\n",
    "    print(\"Environment variables not loaded, cell execution stopped\")\n",
    "    sys.exit()\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = os.environ[\"OPENAI_API_VERSION\"]\n",
    "\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "task_adherence_evaluator = TaskAdherenceEvaluator(model_config)\n",
    "\n",
    "print(f'openai endpoint: <{model_config[\"azure_endpoint\"]}>')\n",
    "print(f\"azure deployment name: <{model_config[\"azure_deployment\"]}>\")\n",
    "print(f\"openai api version: <{model_config[\"api_version\"]}>\")\n",
    "\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating query and response as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_adherence': 2.0,\n",
      " 'task_adherence_reason': 'The response is barely adherent as it only '\n",
      "                          'partially aligns with the query by mentioning '\n",
      "                          'watering and trimming but lacks critical details '\n",
      "                          'and other important practices for maintaining a '\n",
      "                          'healthy rose garden.',\n",
      " 'task_adherence_result': 'fail',\n",
      " 'task_adherence_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "# Failure example, there's only a vague adherence to the task\n",
    "result = task_adherence_evaluator(\n",
    "    query=\"What are the best practices for maintaining a healthy rose garden during the summer?\",\n",
    "    response=\"Make sure to water your roses regularly and trim them occasionally.\",\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_adherence': 4.0,\n",
      " 'task_adherence_reason': 'The response is mostly adherent to the query, '\n",
      "                          'providing a comprehensive and structured set of '\n",
      "                          'best practices for summer rose garden care. It '\n",
      "                          'includes specific actions and considerations, '\n",
      "                          'making it a useful and accurate answer.',\n",
      " 'task_adherence_result': 'pass',\n",
      " 'task_adherence_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "# Success example, full adherence to the task\n",
    "result = task_adherence_evaluator(\n",
    "    query=\"What are the best practices for maintaining a healthy rose garden during the summer?\",\n",
    "    response=\"For optimal summer care of your rose garden, start by watering deeply early in the morning to ensure the roots are well-hydrated without encouraging fungal growth. Apply a 2-3 inch layer of organic mulch around the base of the plants to conserve moisture and regulate soil temperature. Fertilize with a balanced rose fertilizer every 4 to 6 weeks to support healthy growth. Prune away any dead or diseased wood to promote good air circulation, and inspect regularly for pests such as aphids or spider mites, treating them promptly with an appropriate organic insecticidal soap. Finally, ensure that your roses receive at least 6 hours of direct sunlight daily for robust flowering.\",\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating query and response as list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_adherence': 5.0,\n",
      " 'task_adherence_reason': 'The response is clear, accurate, and aligns with '\n",
      "                          'the instructions. It provides a specific book '\n",
      "                          'recommendation, including the title, author, and a '\n",
      "                          \"brief summary, which fully addresses the user's \"\n",
      "                          'query. The assistant also offers to provide more '\n",
      "                          'details or another suggestion, enhancing the user '\n",
      "                          'experience.',\n",
      " 'task_adherence_result': 'pass',\n",
      " 'task_adherence_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "query = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in literature and at provid can provide book recommendations.\"},\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:00Z\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"I love historical fiction. Can you recommend a good book from that genre?\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "response = [\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:05Z\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Let me fetch a recommendation for historical fiction.\"}],\n",
    "    },\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:10Z\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_call\",\n",
    "                \"tool_call_id\": \"tool_call_20250314_001\",\n",
    "                \"name\": \"get_book\",\n",
    "                \"arguments\": {\"genre\": \"historical fiction\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:15Z\",\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": \"tool_call_20250314_001\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_result\": '{ \"book\": { \"title\": \"The Pillars of the Earth\", \"author\": \"Ken Follett\", \"summary\": \"A captivating tale set in medieval England that weaves historical events with personal drama.\" } }',\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:20Z\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Based on our records, I recommend 'The Pillars of the Earth' by Ken Follett. This novel is an excellent example of historical fiction with a rich narrative and well-developed characters. Would you like more details or another suggestion?\",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "tool_definitions = [\n",
    "    {\n",
    "        \"name\": \"get_book\",\n",
    "        \"description\": \"Retrieve a book recommendation for a specified genre.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"genre\": {\"type\": \"string\", \"description\": \"The genre for which a book recommendation is requested.\"}\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "result = task_adherence_evaluator(\n",
    "    query=query,\n",
    "    response=response,\n",
    "    tool_definitions=tool_definitions,\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch evaluate and visualize results on Azure AI Foundry\n",
    "Batch evaluate to leverage asynchronous evaluation on a dataset. \n",
    "\n",
    "Optionally, you can go to AI Foundry URL for rich Azure AI Foundry data visualization. You can inspect the evaluation scores and reasoning to quickly identify bugs and issues of your agent to fix and improve. Make sure to authenticate to Azure using `az login` in your terminal before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-08 17:46:37 +0200][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-08 17:46:37 +0200][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_task_adherence_20250708_174637_240826, log path: C:\\Users\\mauromi\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_task_adherence_20250708_174637_240826\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-08 17:46:38 +0200    4124 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-07-08 17:46:42 +0200    4124 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-07-08 17:46:42 +0200    4124 execution.bulk     INFO     Average execution time for completed lines: 4.04 seconds. Estimated time for incomplete lines: 16.16 seconds.\n",
      "2025-07-08 17:46:43 +0200    4124 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-07-08 17:46:43 +0200    4124 execution.bulk     INFO     Average execution time for completed lines: 2.62 seconds. Estimated time for incomplete lines: 7.86 seconds.\n",
      "2025-07-08 17:46:44 +0200    4124 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-07-08 17:46:44 +0200    4124 execution.bulk     INFO     Average execution time for completed lines: 2.05 seconds. Estimated time for incomplete lines: 4.1 seconds.\n",
      "2025-07-08 17:46:45 +0200    4124 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-07-08 17:46:45 +0200    4124 execution.bulk     INFO     Average execution time for completed lines: 1.8 seconds. Estimated time for incomplete lines: 1.8 seconds.\n",
      "2025-07-08 17:46:46 +0200    4124 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-07-08 17:46:46 +0200    4124 execution.bulk     INFO     Average execution time for completed lines: 1.63 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_task_adherence_20250708_174637_240826\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-07-08 17:46:37.264406+02:00\"\n",
      "Duration: \"0:00:09.497929\"\n",
      "Output path: \"C:\\Users\\mauromi\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_task_adherence_20250708_174637_240826\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"task_adherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:09.497929\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"C:\\\\Users\\\\mauromi\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_task_adherence_20250708_174637_240826\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "('AI Foundary URL: '\n",
      " 'https://ai.azure.com/build/evaluation/46708b28-6a25-4254-9ccc-8bf8e898bcbc?wsid=/subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/aihub01_projects_rg/providers/Microsoft.MachineLearningServices/workspaces/aihub01_project01')\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# This sample files contains the evaluation data in JSONL format. Where each line is a run from agent.\n",
    "# This was saved using agent thread and converter.\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "\n",
    "response = evaluate(\n",
    "    data=file_name,\n",
    "    evaluation_name=\"Task Adherence Evaluation\",\n",
    "    evaluators={\n",
    "        \"task_adherence\": task_adherence_evaluator,\n",
    "    },\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"AZURE_HUB_PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"AZURE_HUB_PROJECTS_GROUP_NAME\"],\n",
    "    },\n",
    ")\n",
    "pprint(f'AI Foundary URL: {response.get(\"studio_url\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative AI evaluation",
   "language": "python",
   "name": "genai_evaluation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
