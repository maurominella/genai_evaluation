{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Adherence Evaluator\n",
    "\n",
    "# Intent Resolution Evaluator\n",
    "\n",
    "## Objective\n",
    "This sample demonstrates to how to use task adherence evaluator on agent data. The supported input formats include:\n",
    "- simple data such as strings;\n",
    "- user-agent conversations in the form of list of agent messages. \n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend about 10 minutes running this notebook. \n",
    "\n",
    "## Before you begin\n",
    "For quality evaluation, you need to deploy a `gpt` model supporting JSON mode. We recommend a model `gpt-4o` or `gpt-4o-mini` for their strong reasoning capabilities.    \n",
    "\n",
    "### Prerequisite\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity azure-ai-evaluation\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **PROJECT_CONNECTION_STRING** - The project connection string, as found in the overview page of your Azure AI Foundry project.\n",
    "2) **MODEL_DEPLOYMENT_NAME** - The deployment name of the model for this AI-assisted evaluator, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n",
    "3) **AZURE_OPENAI_ENDPOINT** - Azure Open AI Endpoint to be used for evaluation.\n",
    "4) **AZURE_OPENAI_API_KEY** - Azure Open AI Key to be used for evaluation.\n",
    "5) **AZURE_OPENAI_API_VERSION** - Azure Open AI Api version to be used for evaluation.\n",
    "6) **AZURE_SUBSCRIPTION_ID** - Azure Subscription Id of Azure AI Project\n",
    "7) **PROJECT_NAME** - Azure AI Project Name\n",
    "8) **RESOURCE_GROUP_NAME** - Azure AI Project Resource Group Name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "This sample demonstrates how to use Task Adherence Evaluator\n",
    "Before running the sample:\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity azure-ai-evaluation\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **PROJECT_CONNECTION_STRING** - The project connection string, as found in the overview page of your Azure AI Foundry project.\n",
    "2) **MODEL_DEPLOYMENT_NAME** - The deployment name of the AI model, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n",
    "3) **AZURE_OPENAI_ENDPOINT** - Azure Open AI Endpoint to be used for evaluation.\n",
    "4) **AZURE_OPENAI_API_KEY** - Azure Open AI Key to be used for evaluation.\n",
    "5) **AZURE_OPENAI_API_VERSION** - Azure Open AI Api version to be used for evaluation.\n",
    "6) **AZURE_SUBSCRIPTION_ID** - Azure Subscription Id of Azure AI Project\n",
    "7) **PROJECT_NAME** - Azure AI Project Name\n",
    "8) **RESOURCE_GROUP_NAME** - Azure AI Project Resource Group Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Task Adherence evaluator measures how well the agent adheres to their assigned tasks or predefined goal.\n",
    "\n",
    "The scoring is on a 1-5 integer scale and is as follows:\n",
    "\n",
    "  - Score 1: Fully Inadherent\n",
    "  - Score 2: Barely Adherent\n",
    "  - Score 3: Moderately Adherent\n",
    "  - Score 4: Mostly Adherent\n",
    "  - Score 5: Fully Adherent\n",
    "\n",
    "The evaluation requires the following inputs:\n",
    "\n",
    "  - Query    : The user query. Either a string with a user request or a list of messages with previous requests from the user and responses from the assistant, potentially including a system message.\n",
    "  - Response : The response to be evaluated. Either a string or a message with the response from the agent to the last user query.\n",
    "\n",
    "There is a third optional parameter:\n",
    "  - ToolDefinitions : The list of tool definitions the agent can call. This may be useful for the evaluator to better assess if the right tool was called to adhere to user intent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Task Adherence Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class TaskAdherenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai endpoint: <https://aiservicesiyva.openai.azure.com/>\n",
      "azure deployment name: <gpt-4o>\n",
      "openai api version: <2025-04-01-preview>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.evaluation import TaskAdherenceEvaluator, AzureOpenAIModelConfiguration\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv # requires python-dotenv\n",
    "\n",
    "if not load_dotenv(\"./../../config/credentials_my.env\"):\n",
    "    print(\"Environment variables not loaded, cell execution stopped\")\n",
    "    sys.exit()\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = os.environ[\"OPENAI_API_VERSION\"]\n",
    "\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "task_adherence_evaluator = TaskAdherenceEvaluator(model_config)\n",
    "\n",
    "print(f'openai endpoint: <{model_config[\"azure_endpoint\"]}>')\n",
    "print(f\"azure deployment name: <{model_config[\"azure_deployment\"]}>\")\n",
    "print(f\"openai api version: <{model_config[\"api_version\"]}>\")\n",
    "\n",
    "task_adherence = TaskAdherenceEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating query and response as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_adherence': 2.0,\n",
      " 'task_adherence_reason': 'The response provides some relevant actions but '\n",
      "                          'lacks detail and omits other important practices '\n",
      "                          'for summer rose garden maintenance, resulting in '\n",
      "                          'critical gaps.',\n",
      " 'task_adherence_result': 'fail',\n",
      " 'task_adherence_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "# Failure example, there's only a vague adherence to the task\n",
    "result = task_adherence_evaluator(\n",
    "    query=\"What are the best practices for maintaining a healthy rose garden during the summer?\",\n",
    "    response=\"Make sure to water your roses regularly and trim them occasionally.\",\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_adherence': 4.0,\n",
      " 'task_adherence_reason': 'The response is clear, accurate, and aligns with '\n",
      "                          'the instructions, providing a comprehensive guide '\n",
      "                          'to summer rose garden maintenance with minor room '\n",
      "                          'for additional details.',\n",
      " 'task_adherence_result': 'pass',\n",
      " 'task_adherence_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "# Success example, full adherence to the task\n",
    "result = task_adherence_evaluator(\n",
    "    query=\"What are the best practices for maintaining a healthy rose garden during the summer?\",\n",
    "    response=\"For optimal summer care of your rose garden, start by watering deeply early in the morning to ensure the roots are well-hydrated without encouraging fungal growth. Apply a 2-3 inch layer of organic mulch around the base of the plants to conserve moisture and regulate soil temperature. Fertilize with a balanced rose fertilizer every 4 to 6 weeks to support healthy growth. Prune away any dead or diseased wood to promote good air circulation, and inspect regularly for pests such as aphids or spider mites, treating them promptly with an appropriate organic insecticidal soap. Finally, ensure that your roses receive at least 6 hours of direct sunlight daily for robust flowering.\",\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating query and response as list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_adherence': 5.0,\n",
      " 'task_adherence_reason': 'The response effectively fulfills the query by '\n",
      "                          'providing a specific book recommendation in the '\n",
      "                          'historical fiction genre, along with a brief '\n",
      "                          \"description of the book's content. It also offers \"\n",
      "                          'further assistance, making it a well-rounded and '\n",
      "                          'useful response.',\n",
      " 'task_adherence_result': 'pass',\n",
      " 'task_adherence_threshold': 3}\n"
     ]
    }
   ],
   "source": [
    "query = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert in literature and at provid can provide book recommendations.\"},\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:00Z\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"I love historical fiction. Can you recommend a good book from that genre?\"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "response = [\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:05Z\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"Let me fetch a recommendation for historical fiction.\"}],\n",
    "    },\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:10Z\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_call\",\n",
    "                \"tool_call_id\": \"tool_call_20250314_001\",\n",
    "                \"name\": \"get_book\",\n",
    "                \"arguments\": {\"genre\": \"historical fiction\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:15Z\",\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": \"tool_call_20250314_001\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_result\": '{ \"book\": { \"title\": \"The Pillars of the Earth\", \"author\": \"Ken Follett\", \"summary\": \"A captivating tale set in medieval England that weaves historical events with personal drama.\" } }',\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"createdAt\": \"2025-03-14T08:00:20Z\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Based on our records, I recommend 'The Pillars of the Earth' by Ken Follett. This novel is an excellent example of historical fiction with a rich narrative and well-developed characters. Would you like more details or another suggestion?\",\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "tool_definitions = [\n",
    "    {\n",
    "        \"name\": \"get_book\",\n",
    "        \"description\": \"Retrieve a book recommendation for a specified genre.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"genre\": {\"type\": \"string\", \"description\": \"The genre for which a book recommendation is requested.\"}\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "result = task_adherence_evaluator(\n",
    "    query=query,\n",
    "    response=response,\n",
    "    tool_definitions=tool_definitions,\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch evaluate and visualize results on Azure AI Foundry\n",
    "Batch evaluate to leverage asynchronous evaluation on a dataset. \n",
    "\n",
    "Optionally, you can go to AI Foundry URL for rich Azure AI Foundry data visualization. You can inspect the evaluation scores and reasoning to quickly identify bugs and issues of your agent to fix and improve. Make sure to authenticate to Azure using `az login` in your terminal before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-09 15:25:10 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-09 15:25:10 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_task_adherence_20250609_152510_400838, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_task_adherence_20250609_152510_400838/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-09 15:25:10 +0000  890824 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-06-09 15:25:13 +0000  890824 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-06-09 15:25:13 +0000  890824 execution.bulk     INFO     Average execution time for completed lines: 3.36 seconds. Estimated time for incomplete lines: 13.44 seconds.\n",
      "2025-06-09 15:25:14 +0000  890824 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-06-09 15:25:14 +0000  890824 execution.bulk     INFO     Average execution time for completed lines: 1.81 seconds. Estimated time for incomplete lines: 5.43 seconds.\n",
      "2025-06-09 15:25:14 +0000  890824 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-06-09 15:25:14 +0000  890824 execution.bulk     INFO     Average execution time for completed lines: 1.21 seconds. Estimated time for incomplete lines: 2.42 seconds.\n",
      "2025-06-09 15:25:14 +0000  890824 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-06-09 15:25:14 +0000  890824 execution.bulk     INFO     Average execution time for completed lines: 1.0 seconds. Estimated time for incomplete lines: 1.0 seconds.\n",
      "2025-06-09 15:25:15 +0000  890824 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-06-09 15:25:15 +0000  890824 execution.bulk     INFO     Average execution time for completed lines: 0.98 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_task_adherence_20250609_152510_400838\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-06-09 15:25:10.412378+00:00\"\n",
      "Duration: \"0:00:05.142919\"\n",
      "Output path: \"/home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_task_adherence_20250609_152510_400838\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"task_adherence\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:05.142919\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_task_adherence_20250609_152510_400838\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n"
     ]
    },
    {
     "ename": "EvaluationException",
     "evalue": "(InternalError) The get 'mmai-swc-hub01-prj01' workspace request failed with HTTP 403 - (AuthorizationFailed) The client 'c891dd02-f27f-4bd8-b2c8-fcfc7e6a8e33' with object id 'c891dd02-f27f-4bd8-b2c8-fcfc7e6a8e33' does not have authorization to perform action 'Microsoft.MachineLearningServices/workspaces/read' over scope '/subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/mmai-swc-hub01-grp/providers/Microsoft.MachineLearningServices/workspaces/mmai-swc-hub01-prj01' or the scope is invalid. If access was recently granted, please refresh your credentials.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEvaluationException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# This sample files contains the evaluation data in JSONL format. Where each line is a run from agent.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This was saved using agent thread and converter.\u001b[39;00m\n\u001b[32m      5\u001b[39m file_name = \u001b[33m\"\u001b[39m\u001b[33mevaluation_data.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m response = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTask Adherence Evaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtask_adherence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_adherence_evaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msubscription_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAZURE_SUBSCRIPTION_ID\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproject_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPROJECT_NAME\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresource_group_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRESOURCE_GROUP_NAME\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m pprint(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAI Foundary URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.get(\u001b[33m\"\u001b[39m\u001b[33mstudio_url\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/genai_evaluation/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:796\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(data, evaluators, evaluation_name, target, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, **kwargs)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EvaluationException):\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[32m    790\u001b[39m         message=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    791\u001b[39m         target=ErrorTarget.EVALUATE,\n\u001b[32m    792\u001b[39m         category=ErrorCategory.FAILED_EXECUTION,\n\u001b[32m    793\u001b[39m         blame=ErrorBlame.SYSTEM_ERROR,\n\u001b[32m    794\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/genai_evaluation/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:754\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(data, evaluators, evaluation_name, target, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Evaluates target or data with built-in or custom evaluators. If both target and data are provided,\u001b[39;00m\n\u001b[32m    703\u001b[39m \u001b[33;03m    data will be run through target function and then results will be evaluated.\u001b[39;00m\n\u001b[32m    704\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    751\u001b[39m \u001b[33;03m            https://{resource_name}.services.ai.azure.com/api/projects/{project_name}\u001b[39;00m\n\u001b[32m    752\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluators_and_graders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m=\u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfail_on_evaluator_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfail_on_evaluator_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# Handle multiprocess bootstrap error\u001b[39;00m\n\u001b[32m    767\u001b[39m     bootstrap_error = (\n\u001b[32m    768\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn attempt has been made to start a new process before the\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m        \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    769\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcurrent process has finished its bootstrapping phase.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    770\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/genai_evaluation/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:936\u001b[39m, in \u001b[36m_evaluate\u001b[39m\u001b[34m(evaluators_and_graders, evaluation_name, target, data, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, **kwargs)\u001b[39m\n\u001b[32m    934\u001b[39m     studio_url = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    935\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trace_destination:\n\u001b[32m--> \u001b[39m\u001b[32m936\u001b[39m         studio_url = \u001b[43m_log_metrics_and_instance_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_destination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    940\u001b[39m result_df_dict = results_df.to_dict(\u001b[33m\"\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    941\u001b[39m result: EvaluationResult = {\u001b[33m\"\u001b[39m\u001b[33mrows\u001b[39m\u001b[33m\"\u001b[39m: result_df_dict, \u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m: metrics, \u001b[33m\"\u001b[39m\u001b[33mstudio_url\u001b[39m\u001b[33m\"\u001b[39m: studio_url}  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/genai_evaluation/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_utils.py:230\u001b[39m, in \u001b[36m_log_metrics_and_instance_results\u001b[39m\u001b[34m(metrics, instance_results, trace_destination, run, evaluation_name, name_map, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m ws_triad = extract_workspace_triad_from_trace_provider(trace_destination)\n\u001b[32m    223\u001b[39m management_client = LiteMLClient(\n\u001b[32m    224\u001b[39m     subscription_id=ws_triad.subscription_id,\n\u001b[32m    225\u001b[39m     resource_group=ws_triad.resource_group_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# let the client automatically determine the credentials to use\u001b[39;00m\n\u001b[32m    229\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m tracking_uri = \u001b[43mmanagement_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkspace_get_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mws_triad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[43m)\u001b[49m.ml_flow_tracking_uri\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# Adding line_number as index column this is needed by UI to form link to individual instance run\u001b[39;00m\n\u001b[32m    233\u001b[39m instance_results[\u001b[33m\"\u001b[39m\u001b[33mline_number\u001b[39m\u001b[33m\"\u001b[39m] = instance_results.index.values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/genai_evaluation/lib/python3.12/site-packages/azure/ai/evaluation/_azure/_clients.py:159\u001b[39m, in \u001b[36mLiteMLClient.workspace_get_info\u001b[39m\u001b[34m(self, workspace_name)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mworkspace_get_info\u001b[39m(\u001b[38;5;28mself\u001b[39m, workspace_name: \u001b[38;5;28mstr\u001b[39m) -> Workspace:\n\u001b[32m    151\u001b[39m     \u001b[38;5;66;03m# https://learn.microsoft.com/rest/api/azureml/workspaces/get?view=rest-azureml-2024-10-01\u001b[39;00m\n\u001b[32m    152\u001b[39m     workspace_response = \u001b[38;5;28mself\u001b[39m._http_client.request(\n\u001b[32m    153\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGET\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    154\u001b[39m         \u001b[38;5;28mself\u001b[39m._generate_path(*PATH_ML_WORKSPACES, workspace_name),\n\u001b[32m    155\u001b[39m         params={QUERY_KEY_API_VERSION: \u001b[38;5;28mself\u001b[39m._api_version},\n\u001b[32m    156\u001b[39m         headers=\u001b[38;5;28mself\u001b[39m._get_headers(),\n\u001b[32m    157\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_throw_on_http_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m workspace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m     workspace = Workspace.deserialize(workspace_response)\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m workspace\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/genai_evaluation/lib/python3.12/site-packages/azure/ai/evaluation/_azure/_clients.py:191\u001b[39m, in \u001b[36mLiteMLClient._throw_on_http_error\u001b[39m\u001b[34m(response, description, valid_status)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (JSONDecodeError, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m):\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[32m    192\u001b[39m     message=message,\n\u001b[32m    193\u001b[39m     target=ErrorTarget.EVALUATE,\n\u001b[32m    194\u001b[39m     category=ErrorCategory.FAILED_EXECUTION,\n\u001b[32m    195\u001b[39m     blame=ErrorBlame.SYSTEM_ERROR,\n\u001b[32m    196\u001b[39m )\n",
      "\u001b[31mEvaluationException\u001b[39m: (InternalError) The get 'mmai-swc-hub01-prj01' workspace request failed with HTTP 403 - (AuthorizationFailed) The client 'c891dd02-f27f-4bd8-b2c8-fcfc7e6a8e33' with object id 'c891dd02-f27f-4bd8-b2c8-fcfc7e6a8e33' does not have authorization to perform action 'Microsoft.MachineLearningServices/workspaces/read' over scope '/subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/mmai-swc-hub01-grp/providers/Microsoft.MachineLearningServices/workspaces/mmai-swc-hub01-prj01' or the scope is invalid. If access was recently granted, please refresh your credentials."
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# This sample files contains the evaluation data in JSONL format. Where each line is a run from agent.\n",
    "# This was saved using agent thread and converter.\n",
    "file_name = \"evaluation_data.jsonl\"\n",
    "\n",
    "response = evaluate(\n",
    "    data=file_name,\n",
    "    evaluation_name=\"Task Adherence Evaluation\",\n",
    "    evaluators={\n",
    "        \"task_adherence\": task_adherence_evaluator,\n",
    "    },\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "        \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "        \"resource_group_name\": os.environ[\"RESOURCE_GROUP_NAME\"],\n",
    "    },\n",
    ")\n",
    "pprint(f'AI Foundary URL: {response.get(\"studio_url\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative AI evaluation",
   "language": "python",
   "name": "genai_evaluation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
