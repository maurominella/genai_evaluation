{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Completeness Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "This sample demonstrates how to use Response Completeness Evaluator on agent's response when ground truth is provided. This evaluator is helpful when you have ground truth to assess the quality of the agent's final response. \n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend about 20 minutes running this notebook. \n",
    "\n",
    "## Before you begin\n",
    "For quality evaluation, you need to deploy a `gpt` model supporting JSON mode. We recommend a model `gpt-4o` or `gpt-4o-mini` for their strong reasoning capabilities.    \n",
    "\n",
    "### Prerequisite\n",
    "```bash\n",
    "pip install azure-ai-projects azure-identity azure-ai-evaluation\n",
    "```\n",
    "Set these environment variables with your own values:\n",
    "1) **PROJECT_CONNECTION_STRING** - The project connection string, as found in the overview page of your Azure AI Foundry project.\n",
    "2) **MODEL_DEPLOYMENT_NAME** - The deployment name of the model for this AI-assisted evaluator, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n",
    "3) **AZURE_OPENAI_ENDPOINT** - Azure Open AI Endpoint to be used for evaluation.\n",
    "4) **AZURE_OPENAI_API_KEY** - Azure Open AI Key to be used for evaluation.\n",
    "5) **AZURE_OPENAI_API_VERSION** - Azure Open AI Api version to be used for evaluation.\n",
    "6) **AZURE_SUBSCRIPTION_ID** - Azure Subscription Id of Azure AI Project\n",
    "7) **PROJECT_NAME** - Azure AI Project Name\n",
    "8) **RESOURCE_GROUP_NAME** - Azure AI Project Resource Group Name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Response Completeness evaluator assesses the quality of an agent response by examining how well it aligns with the provided ground truth. The evaluation is based on the following scoring system:\n",
    "\n",
    "<pre>\n",
    "Score 1: Fully incomplete: The response misses all necessary and relevant information compared to the ground truth.\n",
    "Score 2: Barely complete: The response contains only a small percentage of the necessary information.\n",
    "Score 3: Moderately complete: The response includes about half of the necessary information.\n",
    "Score 4: Mostly complete: The response contains most of the necessary information, with only minor omissions.\n",
    "Score 5: Fully complete: The response perfectly matches all necessary and relevant information from the ground truth.\n",
    "</pre>\n",
    "\n",
    "The evaluation requires the following inputs:\n",
    "\n",
    "- Response: The response to be evaluated. (string)\n",
    "- Ground Truth: The correct and complete information against which the response is compared. (string)\n",
    "\n",
    "The evaluator uses these inputs to determine the completeness score, ensuring that the response meaningfully addresses the query while adhering to the provided definitions and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Completeness Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai endpoint: <https://aiservicesiyva.openai.azure.com/>\n",
      "azure deployment name: <gpt-4o>\n",
      "openai api version: <2025-04-01-preview>\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import ResponseCompletenessEvaluator, AzureOpenAIModelConfiguration\n",
    "from pprint import pprint\n",
    "import os\n",
    "from dotenv import load_dotenv # requires python-dotenv\n",
    "\n",
    "if not load_dotenv(\"./../../config/credentials_my.env\"):\n",
    "    print(\"Environment variables not loaded, cell execution stopped\")\n",
    "    sys.exit()\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = os.environ[\"OPENAI_API_VERSION\"]\n",
    "\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "\n",
    "print(f'openai endpoint: <{model_config[\"azure_endpoint\"]}>')\n",
    "print(f\"azure deployment name: <{model_config[\"azure_deployment\"]}>\")\n",
    "print(f\"openai api version: <{model_config[\"api_version\"]}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ResponseCompletenessEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "\n",
    "response_completeness_evaluator = ResponseCompletenessEvaluator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating for a ground_truth and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_completeness': 5,\n",
       " 'response_completeness_result': 'pass',\n",
       " 'response_completeness_threshold': 3,\n",
       " 'response_completeness_reason': 'The Response accurately and thoroughly represents the information provided in the Ground Truth, covering all necessary details for both days of the itinerary.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent response is complete\n",
    "result = response_completeness_evaluator(\n",
    "    response=\"Itinery: Day 1 check out the downtown district of the city on train; for Day 2, we can rest in hotel.\",\n",
    "    ground_truth=\"Itinery: Day 1 take a train to visit the downtown area for city sightseeing; Day 2 rests in hotel.\",\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_completeness': 5,\n",
       " 'response_completeness_result': 'pass',\n",
       " 'response_completeness_threshold': 3,\n",
       " 'response_completeness_reason': 'The response accurately includes all the necessary information from the ground truth about the order with ID 124. The additional information about another order does not affect the completeness of the response concerning the ground truth.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent response is incomplete\n",
    "result = response_completeness_evaluator(\n",
    "    response=\"The order with ID 123 has been shipped and is expected to be delivered on March 15, 2025. However, the order with ID 124 is delayed and should now arrive by March 20, 2025.\",\n",
    "    ground_truth=\"The order with ID 124 is delayed and should now arrive by March 20, 2025.\",\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare ground truth for agent response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"response\": \"The temperature of Seattle now is 70 degrees. Based on the temperature, having an outdoor office party is recommended.\",\n",
    "        \"ground_truth\": \"The temperature of Seattle now is 50 degrees. It will be recommended to bring a jacket in the evening.\",\n",
    "    },\n",
    "    {\n",
    "        \"response\": 'The email draft \"Project Plan\" is attached. Please review and provide feedback.',\n",
    "        \"ground_truth\": 'The email draft \"Project Plan\" is attached. Please review and provide feedback by EOD.',\n",
    "    },\n",
    "    {\n",
    "        \"response\": \"Based on the retrieved documents, the shareholder meeting discussed the operational efficiency of the company and financing options.\",\n",
    "        \"ground_truth\": \"The shareholder meeting discussed the compensation package of the company CEO.\",\n",
    "    },\n",
    "    {\n",
    "        \"response\": \"The calendar API returns an error code 500. Please check the server logs for more details.\",\n",
    "        \"ground_truth\": \"The meeting is scheduled for 2 PM tomorrow. Please confirm your availability by EOD.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "file_path = \"response_completeness_data.jsonl\"\n",
    "\n",
    "with Path.open(file_path, \"w\") as file:\n",
    "    for line in data:\n",
    "        file.write(json.dumps(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch evaluate and visualize results on Azure AI Foundry\n",
    "Batch evaluate to leverage asynchronous evaluation on a dataset. \n",
    "\n",
    "Optionally, you can go to AI Foundry URL for rich Azure AI Foundry data visualization. You can inspect the evaluation scores and reasoning to quickly identify bugs and issues of your agent to fix and improve. Make sure to authenticate to Azure using `az login` in your terminal before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-09 15:27:37 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-06-09 15:27:37 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_response_completeness_20250609_152737_506060, log path: /home/azureuser/.promptflow/.runs/azure_ai_evaluation_evaluators_response_completeness_20250609_152737_506060/logs.txt\n"
     ]
    },
    {
     "ename": "EvaluationException",
     "evalue": "(InternalError) The get 'mmai-swc-hub01-prj01' workspace request failed with HTTP 403 - (AuthorizationFailed) The client 'c891dd02-f27f-4bd8-b2c8-fcfc7e6a8e33' with object id 'c891dd02-f27f-4bd8-b2c8-fcfc7e6a8e33' does not have authorization to perform action 'Microsoft.MachineLearningServices/workspaces/read' over scope '/subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/mmai-swc-hub01-grp/providers/Microsoft.MachineLearningServices/workspaces/mmai-swc-hub01-prj01' or the scope is invalid. If access was recently granted, please refresh your credentials.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEvaluationException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[32m      3\u001b[39m azure_ai_project = {\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msubscription_id\u001b[39m\u001b[33m\"\u001b[39m: os.environ[\u001b[33m\"\u001b[39m\u001b[33mAZURE_SUBSCRIPTION_ID\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproject_name\u001b[39m\u001b[33m\"\u001b[39m: os.environ[\u001b[33m\"\u001b[39m\u001b[33mPROJECT_NAME\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresource_group_name\u001b[39m\u001b[33m\"\u001b[39m: os.environ[\u001b[33m\"\u001b[39m\u001b[33mRESOURCE_GROUP_NAME\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      7\u001b[39m }\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m response = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_completeness\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_completeness_evaluator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m=\u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m pprint(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAI Foundry URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.get(\u001b[33m\"\u001b[39m\u001b[33mstudio_url\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/genai_evaluation/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:796\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(data, evaluators, evaluation_name, target, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, **kwargs)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EvaluationException):\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[32m    790\u001b[39m         message=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    791\u001b[39m         target=ErrorTarget.EVALUATE,\n\u001b[32m    792\u001b[39m         category=ErrorCategory.FAILED_EXECUTION,\n\u001b[32m    793\u001b[39m         blame=ErrorBlame.SYSTEM_ERROR,\n\u001b[32m    794\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/genai_evaluation/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:754\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(data, evaluators, evaluation_name, target, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Evaluates target or data with built-in or custom evaluators. If both target and data are provided,\u001b[39;00m\n\u001b[32m    703\u001b[39m \u001b[33;03m    data will be run through target function and then results will be evaluated.\u001b[39;00m\n\u001b[32m    704\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    751\u001b[39m \u001b[33;03m            https://{resource_name}.services.ai.azure.com/api/projects/{project_name}\u001b[39;00m\n\u001b[32m    752\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m754\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluators_and_graders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m=\u001b[49m\u001b[43mazure_ai_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfail_on_evaluator_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfail_on_evaluator_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    766\u001b[39m     \u001b[38;5;66;03m# Handle multiprocess bootstrap error\u001b[39;00m\n\u001b[32m    767\u001b[39m     bootstrap_error = (\n\u001b[32m    768\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn attempt has been made to start a new process before the\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m        \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    769\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcurrent process has finished its bootstrapping phase.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    770\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/genai_evaluation/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_evaluate.py:936\u001b[39m, in \u001b[36m_evaluate\u001b[39m\u001b[34m(evaluators_and_graders, evaluation_name, target, data, evaluator_config, azure_ai_project, output_path, fail_on_evaluator_errors, **kwargs)\u001b[39m\n\u001b[32m    934\u001b[39m     studio_url = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    935\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trace_destination:\n\u001b[32m--> \u001b[39m\u001b[32m936\u001b[39m         studio_url = \u001b[43m_log_metrics_and_instance_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_destination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    940\u001b[39m result_df_dict = results_df.to_dict(\u001b[33m\"\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    941\u001b[39m result: EvaluationResult = {\u001b[33m\"\u001b[39m\u001b[33mrows\u001b[39m\u001b[33m\"\u001b[39m: result_df_dict, \u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m: metrics, \u001b[33m\"\u001b[39m\u001b[33mstudio_url\u001b[39m\u001b[33m\"\u001b[39m: studio_url}  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/genai_evaluation/lib/python3.12/site-packages/azure/ai/evaluation/_evaluate/_utils.py:230\u001b[39m, in \u001b[36m_log_metrics_and_instance_results\u001b[39m\u001b[34m(metrics, instance_results, trace_destination, run, evaluation_name, name_map, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m ws_triad = extract_workspace_triad_from_trace_provider(trace_destination)\n\u001b[32m    223\u001b[39m management_client = LiteMLClient(\n\u001b[32m    224\u001b[39m     subscription_id=ws_triad.subscription_id,\n\u001b[32m    225\u001b[39m     resource_group=ws_triad.resource_group_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# let the client automatically determine the credentials to use\u001b[39;00m\n\u001b[32m    229\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m tracking_uri = \u001b[43mmanagement_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkspace_get_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mws_triad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[43m)\u001b[49m.ml_flow_tracking_uri\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# Adding line_number as index column this is needed by UI to form link to individual instance run\u001b[39;00m\n\u001b[32m    233\u001b[39m instance_results[\u001b[33m\"\u001b[39m\u001b[33mline_number\u001b[39m\u001b[33m\"\u001b[39m] = instance_results.index.values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/genai_evaluation/lib/python3.12/site-packages/azure/ai/evaluation/_azure/_clients.py:159\u001b[39m, in \u001b[36mLiteMLClient.workspace_get_info\u001b[39m\u001b[34m(self, workspace_name)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mworkspace_get_info\u001b[39m(\u001b[38;5;28mself\u001b[39m, workspace_name: \u001b[38;5;28mstr\u001b[39m) -> Workspace:\n\u001b[32m    151\u001b[39m     \u001b[38;5;66;03m# https://learn.microsoft.com/rest/api/azureml/workspaces/get?view=rest-azureml-2024-10-01\u001b[39;00m\n\u001b[32m    152\u001b[39m     workspace_response = \u001b[38;5;28mself\u001b[39m._http_client.request(\n\u001b[32m    153\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGET\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    154\u001b[39m         \u001b[38;5;28mself\u001b[39m._generate_path(*PATH_ML_WORKSPACES, workspace_name),\n\u001b[32m    155\u001b[39m         params={QUERY_KEY_API_VERSION: \u001b[38;5;28mself\u001b[39m._api_version},\n\u001b[32m    156\u001b[39m         headers=\u001b[38;5;28mself\u001b[39m._get_headers(),\n\u001b[32m    157\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_throw_on_http_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkspace_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m workspace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m     workspace = Workspace.deserialize(workspace_response)\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m workspace\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/anaconda/envs/genai_evaluation/lib/python3.12/site-packages/azure/ai/evaluation/_azure/_clients.py:191\u001b[39m, in \u001b[36mLiteMLClient._throw_on_http_error\u001b[39m\u001b[34m(response, description, valid_status)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (JSONDecodeError, \u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m):\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[32m    192\u001b[39m     message=message,\n\u001b[32m    193\u001b[39m     target=ErrorTarget.EVALUATE,\n\u001b[32m    194\u001b[39m     category=ErrorCategory.FAILED_EXECUTION,\n\u001b[32m    195\u001b[39m     blame=ErrorBlame.SYSTEM_ERROR,\n\u001b[32m    196\u001b[39m )\n",
      "\u001b[31mEvaluationException\u001b[39m: (InternalError) The get 'mmai-swc-hub01-prj01' workspace request failed with HTTP 403 - (AuthorizationFailed) The client 'c891dd02-f27f-4bd8-b2c8-fcfc7e6a8e33' with object id 'c891dd02-f27f-4bd8-b2c8-fcfc7e6a8e33' does not have authorization to perform action 'Microsoft.MachineLearningServices/workspaces/read' over scope '/subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/mmai-swc-hub01-grp/providers/Microsoft.MachineLearningServices/workspaces/mmai-swc-hub01-prj01' or the scope is invalid. If access was recently granted, please refresh your credentials."
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "    \"project_name\": os.environ[\"PROJECT_NAME\"],\n",
    "    \"resource_group_name\": os.environ[\"RESOURCE_GROUP_NAME\"],\n",
    "}\n",
    "\n",
    "response = evaluate(\n",
    "    data=file_path,\n",
    "    evaluators={\n",
    "        \"response_completeness\": response_completeness_evaluator,\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project,\n",
    ")\n",
    "\n",
    "pprint(f'AI Foundry URL: {response.get(\"studio_url\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative AI evaluation",
   "language": "python",
   "name": "genai_evaluation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
