{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c479b344-f6ae-454d-a96c-85f9137f16dc",
   "metadata": {},
   "source": [
    "# [Generate synthetic and simulated data for evaluation](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/simulator-interaction-data)\n",
    "**Azure AI Evaluation SDK's** `Simulator` provides an end-to-end synthetic data generation capability to help developers test their application's response to typical user queries in the absence of production data. AI developers can use an index or text-based query generator and fully customizable simulator to create robust test datasets around non-adversarial tasks specific to their application. The `Simulator` class is a powerful tool designed to generate synthetic conversations and simulate task-based interactions. This capability is useful for:\n",
    "- **Testing Conversational Applications**: Ensure your chatbots and virtual assistants respond accurately under various scenarios.\n",
    "- **Training AI Models**: Generate diverse datasets to train and fine-tune machine learning models.\n",
    "- **Generating Datasets**: Create extensive conversation logs for analysis and development purposes.\n",
    "By automating the creation of synthetic data, the Simulator class helps streamline the development and testing processes, ensuring your applications are robust and reliable.\n",
    "<br/>\n",
    "By automating the creation of synthetic data, the `Simulator` class helps streamline the development and testing processes, ensuring your applications are robust and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f1ba86-e977-4636-8887-04394b833f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8ac0ff-c633-4603-b3bf-c63ef79b1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Libraries\n",
    "import os, json\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider #requires azure-identity\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv # requires python-dotenv\n",
    "\n",
    "if not load_dotenv(\"./../../config/credentials_my.env\"):\n",
    "    print(\"Environment variables not loaded, cell execution stopped\")\n",
    "    sys.exit()\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = os.environ[\"OPENAI_API_VERSION\"]\n",
    "\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4edd78d8-45bd-46ec-aca1-26f28afa8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure AI project and Azure OpenAI connection\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    \"project_name\": os.environ.get(\"PROJECT_NAME\"),\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"type\": os.environ.get(\"GLOBAL_LLM_SERVICE\"), # \"AzureOpenAI\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94618b92-d099-4eaf-a6f1-191c9a2dc721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation.simulator import Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfa9ff-ecb3-47a2-bf9f-798e3ca079bd",
   "metadata": {},
   "source": [
    "## Generate text or index-based synthetic data as input\n",
    "In the first part, we prepare the text for generating the input to our simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57bf56fd-42e7-4aac-837c-e4a505181885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leonardo di ser Piero da Vinci (15 April 1452 – 2 May 1519) was an Italian polymath of the High Rena...\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import wikipedia\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "# Prepare the text to send to the simulator\n",
    "wiki_search_term = \"Leonardo da Vinci\"\n",
    "wiki_title = wikipedia.search(wiki_search_term)[0]\n",
    "wiki_page = wikipedia.page(wiki_title)\n",
    "text = wiki_page.summary[:5000]\n",
    "print(f\"{text[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa8dc5-c6fc-4306-a3cd-422cb332d808",
   "metadata": {},
   "source": [
    "## Specify application Prompty\n",
    "The following `application.prompty` file specifies how a chat application behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca529b72-ba77-4db9-87a9-21696f4b6d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./eval_assets/application.prompty\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./eval_assets/application.prompty\n",
    "---\n",
    "name: ApplicationPrompty\n",
    "description: Chat RAG application\n",
    "model:\n",
    "  api: chat\n",
    "  parameters:\n",
    "    temperature: 0.0\n",
    "    top_p: 1.0\n",
    "    presence_penalty: 0\n",
    "    frequency_penalty: 0\n",
    "    response_format:\n",
    "      type: text\n",
    " \n",
    "inputs:\n",
    "  conversation_history:\n",
    "    type: dict\n",
    "  context:\n",
    "    type: string\n",
    "  query:\n",
    "    type: string\n",
    " \n",
    "---\n",
    "system:\n",
    "You are a helpful assistant and you're helping with the user's query. \n",
    "Keep the conversation engaging and interesting.\n",
    "\n",
    "Keep your conversation grounded in the provided context: \n",
    "{{ context }}\n",
    "\n",
    "Output with a string that continues the conversation, responding to \n",
    "the latest message from the user query:\n",
    "{{ query }}\n",
    "\n",
    "given the conversation history:\n",
    "{{ conversation_history }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6da11cff-7a95-4173-a2ce-3148f7d1aecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Making pizza at home is a fun and delicious project! Here’s a simple way to do it:\\n\\n1. Make or buy pizza dough. You can find ready-made dough at most grocery stores, or make your own with flour, yeast, water, salt, and a bit of olive oil.\\n2. Preheat your oven to its highest setting (usually around 475°F/245°C).\\n3. Roll out the dough on a floured surface to your desired thickness.\\n4. Place the dough on a baking sheet or pizza stone.\\n5. Spread a thin layer of pizza sauce (store-bought or homemade) over the dough.\\n6. Add your favorite toppings—cheese, pepperoni, veggies, mushrooms, etc.\\n7. Bake for 10-15 minutes, or until the crust is golden and the cheese is bubbly.\\n8. Let it cool for a couple of minutes, slice, and enjoy!\\n\\nWould you like a specific recipe for dough or sauce, or ideas for unique toppings?'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptflow.client import load_flow\n",
    "\n",
    "prompty_path = \"eval_assets/application.prompty\"\n",
    "flow = load_flow(source=prompty_path, model={\"configuration\": model_config})\n",
    "\n",
    "flow(conversation_history={}, context=\"\", query=\"how to make a pizza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a7e0c-ae71-43f8-8422-3d62a6a10e70",
   "metadata": {},
   "source": [
    "## Specify target callback to simulate against\n",
    "You can bring any application endpoint to simulate against by specifying a target callback function such as the following given an application that is an LLM with a Prompty file like `application.prompty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e4e2868-cd07-403e-a475-6cd468102065",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def callback(\n",
    "    messages: Dict,\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,  # noqa: ANN401\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    # Get the last message\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "    context = latest_message.get(\"context\", None) # looks for context, default None\n",
    "    # Call your endpoint or AI application here\n",
    "    current_dir = os.path.dirname(__file__)\n",
    "    prompty_path = os.path.join(current_dir, \"application.prompty\")\n",
    "    _flow = load_flow(source=prompty_path, model={\"configuration\": azure_ai_project})\n",
    "    response = _flow(query=query, context=context, conversation_history=messages_list)\n",
    "    # Format the response to follow the OpenAI chat protocol\n",
    "    formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": context,\n",
    "    }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\n",
    "        \"messages\": messages[\"messages\"],\n",
    "        \"stream\": stream,\n",
    "        \"session_state\": session_state,\n",
    "        \"context\": context\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative AI evaluation",
   "language": "python",
   "name": "genai_evaluation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
