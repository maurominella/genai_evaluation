{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c479b344-f6ae-454d-a96c-85f9137f16dc",
   "metadata": {},
   "source": [
    "# [Groundedness and Groundedness PRO Evaluators](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/evaluate-sdk#built-in-evaluators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f1ba86-e977-4636-8887-04394b833f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8ac0ff-c633-4603-b3bf-c63ef79b1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Libraries\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider #requires azure-identity\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv # requires python-dotenv\n",
    "\n",
    "if not load_dotenv(\"./../../config/credentials_my.env\"):\n",
    "    print(\"Environment variables not loaded, cell execution stopped\")\n",
    "    sys.exit()\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = os.environ[\"OPENAI_API_VERSION\"]\n",
    "\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4edd78d8-45bd-46ec-aca1-26f28afa8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure AI project and Azure OpenAI connection\n",
    "azure_ai_project_config = {\n",
    "    \"subscription_id\": os.environ.get(\"AZURE_SUBSCRIPTION_ID\"),\n",
    "    \"resource_group_name\": os.environ.get(\"RESOURCE_GROUP_NAME\"),\n",
    "    \"project_name\": os.environ.get(\"PROJECT_NAME\"),\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87e4c0b-e712-41ae-b3bd-0e1857e1deae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class GroundednessProEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "# Initializing Groundedness and Groundedness Pro evaluators\n",
    "from azure.ai.evaluation import GroundednessEvaluator, GroundednessProEvaluator\n",
    "\n",
    "groundedness_eval = GroundednessEvaluator(model_config)\n",
    "groundedness_pro_eval = GroundednessProEvaluator(azure_ai_project=azure_ai_project_config, credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c357db2-92a9-472c-8af1-7d8529aa5290",
   "metadata": {},
   "outputs": [
    {
     "ename": "EvaluationException",
     "evalue": "(UserError) Model config validation failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:176\u001b[39m, in \u001b[36mvalidate_model_config\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_typed_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAzureOpenAIModelConfiguration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:281\u001b[39m, in \u001b[36m_validate_typed_dict\u001b[39m\u001b[34m(o, t)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m o.items():\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[43mvalidate_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(T_TypedDict, o)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:268\u001b[39m, in \u001b[36m_validate_typed_dict.<locals>.validate_annotation\u001b[39m\u001b[34m(v, annotation)\u001b[39m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(origin \u001b[38;5;129;01mis\u001b[39;00m g \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m (NotRequired, Required)):\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[43mvalidate_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:266\u001b[39m, in \u001b[36m_validate_typed_dict.<locals>.validate_annotation\u001b[39m\u001b[34m(v, annotation)\u001b[39m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mtype\u001b[39m(literal) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(v) \u001b[38;5;129;01mand\u001b[39;00m literal == v \u001b[38;5;28;01mfor\u001b[39;00m literal \u001b[38;5;129;01min\u001b[39;00m literal_args):\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected value to be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(literal_args)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m. Received type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(origin \u001b[38;5;129;01mis\u001b[39;00m g \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m (NotRequired, Required)):\n",
      "\u001b[31mTypeError\u001b[39m: Expected value to be one of ['azure_openai']. Received type <class 'str'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:179\u001b[39m, in \u001b[36mvalidate_model_config\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_typed_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOpenAIModelConfiguration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:218\u001b[39m, in \u001b[36m_validate_typed_dict\u001b[39m\u001b[34m(o, t)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unknown_keys:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdict contains unknown keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(unknown_keys)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    220\u001b[39m required_keys = {\n\u001b[32m    221\u001b[39m     k\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m annotations\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (is_total \u001b[38;5;129;01mand\u001b[39;00m get_origin(annotations[k]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m NotRequired)\n\u001b[32m    224\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m is_total \u001b[38;5;129;01mand\u001b[39;00m get_origin(annotations[k]) \u001b[38;5;129;01mis\u001b[39;00m Required)\n\u001b[32m    225\u001b[39m }\n",
      "\u001b[31mTypeError\u001b[39m: dict contains unknown keys: ['api_version', 'azure_deployment', 'azure_endpoint']",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mEvaluationException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m query_response = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m      2\u001b[39m     query=\u001b[33m\"\u001b[39m\u001b[33mWhich tent is the most waterproof?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     context=\u001b[33m\"\u001b[39m\u001b[33mThe Alpine Explorer Tent is the second most water-proof of all tents available.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     response=\u001b[33m\"\u001b[39m\u001b[33mThe Alpine Explorer Tent is the most waterproof.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Running Groundedness PRO Evaluator on a query and response pair\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m groundedness_score = \u001b[43mgroundedness_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mquery_response\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m+++++ groundedness_score +++++\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m pprint(groundedness_score)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_evaluators\\_groundedness\\_groundedness.py:156\u001b[39m, in \u001b[36mGroundednessEvaluator.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m     prompty_path = os.path.join(current_dir, \u001b[38;5;28mself\u001b[39m._PROMPTY_FILE_WITH_QUERY)\n\u001b[32m    154\u001b[39m     \u001b[38;5;28mself\u001b[39m._prompty_file = prompty_path\n\u001b[32m    155\u001b[39m     prompty_model_config = construct_prompty_model_config(\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m         \u001b[43mvalidate_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_config\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    157\u001b[39m         \u001b[38;5;28mself\u001b[39m._DEFAULT_OPEN_API_VERSION,\n\u001b[32m    158\u001b[39m         USER_AGENT,\n\u001b[32m    159\u001b[39m     )\n\u001b[32m    160\u001b[39m     \u001b[38;5;28mself\u001b[39m._flow = AsyncPrompty.load(source=\u001b[38;5;28mself\u001b[39m._prompty_file, model=prompty_model_config)\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\evaluation\\_common\\utils.py:182\u001b[39m, in \u001b[36mvalidate_model_config\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mModel config validation failed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EvaluationException(\n\u001b[32m    183\u001b[39m         message=msg, internal_message=msg, category=ErrorCategory.MISSING_FIELD, blame=ErrorBlame.USER_ERROR\n\u001b[32m    184\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mEvaluationException\u001b[39m: (UserError) Model config validation failed."
     ]
    }
   ],
   "source": [
    "query_response = dict(\n",
    "    query=\"Which tent is the most waterproof?\",\n",
    "    context=\"The Alpine Explorer Tent is the second most water-proof of all tents available.\",\n",
    "    response=\"The Alpine Explorer Tent is the most waterproof.\"\n",
    ")\n",
    "\n",
    "# Running Groundedness PRO Evaluator on a query and response pair\n",
    "groundedness_score = groundedness_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(\"+++++ groundedness_score +++++\")\n",
    "pprint(groundedness_score)\n",
    "\n",
    "# Running Groundedness Evaluator on the same query\n",
    "groundedness_pro_score = groundedness_pro_eval(\n",
    "    **query_response\n",
    ")\n",
    "print(\"+++++ groundedness_PRO_score +++++\")\n",
    "pprint(groundedness_pro_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative AI evaluation",
   "language": "python",
   "name": "genai_evaluation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
