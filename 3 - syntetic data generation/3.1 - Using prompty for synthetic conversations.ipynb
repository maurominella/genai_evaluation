{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c479b344-f6ae-454d-a96c-85f9137f16dc",
   "metadata": {},
   "source": [
    "# [Generate synthetic and simulated data for evaluation](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/simulator-interaction-data)\n",
    "**Azure AI Evaluation SDK's** `Simulator` provides an end-to-end synthetic data generation capability to help developers test their application's response to typical user queries in the absence of production data. AI developers can use an index or text-based query generator and fully customizable simulator to create robust test datasets around non-adversarial tasks specific to their application. The `Simulator` class is a powerful tool designed to generate synthetic conversations and simulate task-based interactions. This capability is useful for:\n",
    "- **Testing Conversational Applications**: Ensure your chatbots and virtual assistants respond accurately under various scenarios.\n",
    "- **Training AI Models**: Generate diverse datasets to train and fine-tune machine learning models.\n",
    "- **Generating Datasets**: Create extensive conversation logs for analysis and development purposes.\n",
    "By automating the creation of synthetic data, the Simulator class helps streamline the development and testing processes, ensuring your applications are robust and reliable.\n",
    "<br/>\n",
    "By automating the creation of synthetic data, the `Simulator` class helps streamline the development and testing processes, ensuring your applications are robust and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1ba86-e977-4636-8887-04394b833f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ac0ff-c633-4603-b3bf-c63ef79b1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Libraries\n",
    "import os, json\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider #requires azure-identity\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv # requires python-dotenv\n",
    "from typing import List, Dict, Any, Optional\n",
    "from promptflow.client import load_flow\n",
    "from pprint import pprint\n",
    "\n",
    "if not load_dotenv(\"./../../config/credentials_my.env\"):\n",
    "    print(\"Environment variables not loaded, cell execution stopped\")\n",
    "    sys.exit()\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = os.environ[\"OPENAI_API_VERSION\"]\n",
    "\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd78d8-45bd-46ec-aca1-26f28afa8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI connection\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"MODEL_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    \"type\": \"AzureOpenAI\" # NEEDED FOR \\Lib\\site-packages\\promptflow\\core\\_prompty_utils.py\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa8dc5-c6fc-4306-a3cd-422cb332d808",
   "metadata": {},
   "source": [
    "## Specify application Prompty\n",
    "The following `application.prompty` file specifies how a chat application behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca529b72-ba77-4db9-87a9-21696f4b6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./eval_assets/application.prompty\n",
    "---\n",
    "name: ApplicationPrompty\n",
    "description: Chat RAG application\n",
    "model:\n",
    "    api: chat\n",
    "    parameters:\n",
    "        temperature: 0.0\n",
    "        top_p: 1.0\n",
    "        presence_penalty: 0\n",
    "        frequency_penalty: 0\n",
    "        response_format:\n",
    "            type: text\n",
    " \n",
    "inputs:\n",
    "    context:\n",
    "        type: string\n",
    "    query:\n",
    "        type: string\n",
    "    conversation_history:\n",
    "        type: dict\n",
    "---\n",
    "system:\n",
    "You are a helpful assistant and you're helping with the user's query. \n",
    "Keep the conversation engaging and interesting.\n",
    "\n",
    "Keep your conversation grounded in the provided context: \n",
    "{{ context }}\n",
    "\n",
    "Output with a string that continues the conversation, responding to \n",
    "the latest message from the user query using it same language:\n",
    "{{ query }}\n",
    "\n",
    "given the conversation history:\n",
    "{{ conversation_history }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da11cff-7a95-4173-a2ce-3148f7d1aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompty_path = \"eval_assets/application.prompty\"\n",
    "flow = load_flow(source=prompty_path, model={\"configuration\": model_config})\n",
    "\n",
    "pprint(flow(context=\"\", query=\"Come preparare una pizza fatta in casa\", conversation_history=[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811aad51-cd89-4f42-85da-ec1238162f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Conversazione amichevole fra due persone\"\n",
    "\n",
    "conversation_history = [\n",
    "    {\n",
    "        \"content\": \"Cosa fai di bello questa sera?\",\n",
    "        \"role\": \"user\",\n",
    "        \"context\": \"Domanda amichevole\",\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Vado al cinema\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": \"Esposizione di un'attività ricreativa divertente\",\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Ah fantastico! Che cosa vai a vedere?\",\n",
    "        \"role\": \"user\",\n",
    "        \"context\": \"Ulteriore domanda amichevole a dimostrare interesse verso l'interlocutore\",\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Lo SQUALO di Steven Spielberg\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": \"Informazione specifica di un'attività divertente che si intende compiere\",\n",
    "    },\n",
    "]\n",
    "\n",
    "flow(context=conversation_history[-1][\"context\"], query=conversation_history[-1][\"content\"], conversation_history=conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e7524-1305-4844-b5a6-d2f7a3372f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for ch in conversation_history:\n",
    "    print(f'Message {i} - {ch[\"role\"]}: {ch[\"content\"]}\\n')\n",
    "    i += 1\n",
    "    \n",
    "response = flow(context=conversation_history[-1][\"context\"], query=conversation_history[-1][\"content\"], conversation_history=conversation_history)\n",
    "\n",
    "conversation_history.append({\"content\": response, \"role\": \"user\" if conversation_history[-1][\"role\"]==\"assistant\" else \"assistant\", \"context\": \"\"})\n",
    "print(f'Message {i} - {conversation_history[-1][\"role\"]}: {conversation_history[-1][\"content\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a7e0c-ae71-43f8-8422-3d62a6a10e70",
   "metadata": {},
   "source": [
    "## Specify target callback to simulate against\n",
    "You can bring any application endpoint to simulate against by specifying a target callback function such as the following given an application that is an LLM with a Prompty file like `application.prompty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e2868-cd07-403e-a475-6cd468102065",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def callback(\n",
    "    messages: Dict,\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,  # noqa: ANN401\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    "    subfolder: str = \"eval_assets\",\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    latest_message = messages_list[-1] # Get the last message\n",
    "    query = latest_message[\"content\"]\n",
    "    context = latest_message.get(\"context\", None) # looks for context, default None\n",
    "    # Call your endpoint or AI application here\n",
    "    prompty_path = os.path.join(os.getcwd(), subfolder, \"application.prompty\") \n",
    "    _flow = load_flow(source=prompty_path, model={\"configuration\": model_config})\n",
    "    response = _flow(query=query, context=context, conversation_history=messages_list)\n",
    "    # Format the response to follow the OpenAI chat protocol\n",
    "    formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"user\" if conversation_history[-1][\"role\"]==\"assistant\" else \"assistant\",\n",
    "        \"context\": context,\n",
    "    }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\n",
    "        \"messages\": messages[\"messages\"],\n",
    "        \"stream\": stream,\n",
    "        \"session_state\": session_state,\n",
    "        \"context\": context\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f52fa10-fdff-4c9f-96fe-5b0a768dff29",
   "metadata": {},
   "source": [
    "## Generate text or index-based synthetic data as input\n",
    "In the first part, we prepare the text for generating the input to our simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e05b1-2fa7-4447-84c7-b2340ec980b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import wikipedia\n",
    "import os\n",
    "# Prepare the text to send to the simulator\n",
    "wiki_search_term = \"Leonardo da Vinci\"\n",
    "wiki_title = wikipedia.search(wiki_search_term)[0]\n",
    "wiki_page = wikipedia.page(wiki_title)\n",
    "wiki_text = wiki_page.summary[:5000]\n",
    "print(f\"{wiki_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73503b-a94b-4002-9672-5ee34a6011ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation.simulator import Simulator\n",
    "simulator = Simulator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a38f46a-f767-4449-8fe9-af8ab99233d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = await simulator(\n",
    "    num_queries = 1,  # Number of queries\n",
    "    text        = wiki_text,\n",
    "    target      = callback   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c66be-ba7a-4cee-8e58-b4b89347708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590db99a-12b5-4abe-92ea-ac12b4e2bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for ch in outputs[0][\"messages\"]:\n",
    "    print(f'Message {i} - {ch[\"role\"]}: {ch[\"content\"]}\\n')\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative AI evaluation",
   "language": "python",
   "name": "genai_evaluation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
