{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c479b344-f6ae-454d-a96c-85f9137f16dc",
   "metadata": {},
   "source": [
    "# [Cloud Evaluation](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/cloud-evaluation#cloud-evaluation-preview-with-azure-ai-projects-sdk)\n",
    "While Azure AI Evaluation SDK client supports running evaluations locally on your own machine, you might want to delegate the job remotely to the cloud. For example, after you ran local evaluations on small test data to help assess your generative AI application prototypes, now you move into pre-deployment testing and need run evaluations on a large dataset. Cloud evaluation frees you from managing your local compute infrastructure, and enables you to integrate evaluations as tests into your CI/CD pipelines. After deployment, you might want to continuously evaluate your applications for post-deployment monitoring.\n",
    "\n",
    "In this article, you learn how to run cloud evaluation (preview) in pre-deployment testing on a test dataset. Using the Azure AI Projects SDK, you'll have evaluation results automatically logged into your Azure AI project for better observability. This feature supports all Microsoft curated built-in evaluators and your own custom evaluators which can be located in the Evaluator library and have the same project-scope RBAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2d673-f357-4d0f-b8e5-729a34fce542",
   "metadata": {},
   "source": [
    "## Environment prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f1ba86-e977-4636-8887-04394b833f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8ac0ff-c633-4603-b3bf-c63ef79b1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Libraries\n",
    "import os, json\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider #requires azure-identity\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv # requires python-dotenv\n",
    "\n",
    "if not load_dotenv(\"./../../config/credentials_my.env\"):\n",
    "    print(\"Environment variables not loaded, cell execution stopped\")\n",
    "    sys.exit()\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = os.environ[\"OPENAI_API_VERSION\"]\n",
    "\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4edd78d8-45bd-46ec-aca1-26f28afa8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI connection\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"MODEL_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9aaf73-250e-4d6c-9876-98d3a47f22e9",
   "metadata": {},
   "source": [
    "## [Create an Azure AI Client from a connection string](https://learn.microsoft.com/en-us/azure/ai-services/agents/quickstart?pivots=programming-language-python-azure). \n",
    "- available on Azure AI project Overview page.\n",
    "- format: `<HostName>;<AzureSubscriptionId>;<ResourceGroup>;<ProjectName>`\n",
    "- command: `az ml workspace show -n mmai-swc-hub01-prj01 --resource-group mmai-swc-hub01-grp --query discovery_url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "951a94cf-1764-4841-a668-3ff358cced3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbcf8e7-6a97-4e04-b77b-5031cfd49f67",
   "metadata": {},
   "source": [
    "## Uploading evaluation data\n",
    "We provide two ways to register your data in Azure AI project required for evaluations in the cloud:\n",
    "- From SDK: Upload new data from your local directory to your Azure AI project in the SDK, and fetch the dataset ID as a result\n",
    "- Given existing datasets uploaded to your Project..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0393656-b27d-4d78-a3d0-8262e1625a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id, _ = project_client.upload_file(\"./synthetic_dataset_cloud.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0110f2-7654-4b74-8f91-8af4f4e9d46a",
   "metadata": {},
   "source": [
    "## Specifying built-in evaluators from Evaluator library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6feab645-45b6-43a5-b3f5-a525965715f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import F1ScoreEvaluator, RelevanceEvaluator, ViolenceEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c4818e-c791-4114-9dee-1417fa9f5ad3",
   "metadata": {},
   "source": [
    "## Specifying custom evaluators\n",
    "Note: they must be already registered as done in 2.4 Custom Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3caacc18-2bf1-448b-81bb-e843a4d0af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n",
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "# Define ml_client to register custom evaluator\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "ml_client = MLClient(\n",
    "       subscription_id=os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "       resource_group_name=os.environ[\"RESOURCE_GROUP_NAME\"],\n",
    "       workspace_name=os.environ[\"PROJECT_NAME\"],\n",
    "       credential=credential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ee10ad8-c6f5-4533-b477-5c263a2a3c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Method evaluators: This is an experimental method, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered evaluator: creation_context:\n",
      "  created_at: '2025-04-20T23:33:23.686456+00:00'\n",
      "  created_by: Mauro Minella\n",
      "  created_by_type: User\n",
      "  last_modified_at: '2025-04-20T23:33:23.686456+00:00'\n",
      "  last_modified_by: Mauro Minella\n",
      "  last_modified_by_type: User\n",
      "description: prompt-based evaluator measuring response friendliness.\n",
      "id: azureml:/subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/mmai-swc-hub01-grp/providers/Microsoft.MachineLearningServices/workspaces/mmai-swc-hub01-prj01/models/FriendlinessEvaluator/versions/3\n",
      "name: FriendlinessEvaluator\n",
      "path: azureml://subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/mmai-swc-hub01-grp/workspaces/mmai-swc-hub01-prj01/datastores/workspaceblobstore/paths/LocalUpload/e8ccca8c8f07c002cc62dce8635918f0/friendliness_local\n",
      "properties:\n",
      "  is-evaluator: 'true'\n",
      "  is-promptflow: 'true'\n",
      "stage: Development\n",
      "tags: {}\n",
      "type: custom_model\n",
      "version: '3'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify evaluator name as it appears in the Evaluator library\n",
    "evaluator_name = \"FriendlinessEvaluator\"\n",
    "registered_evaluator = ml_client.evaluators.get(evaluator_name, version=3)\n",
    "print(\"Registered evaluator:\", registered_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3db3a-040b-43b9-ac6a-08d71febd73e",
   "metadata": {},
   "source": [
    "## Create an evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08a514b0-bb39-4845-8431-cff66e583917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'displayName': 'Cloud evaluation', 'description': 'Evaluation of dataset', 'data': {'type': 'dataset', 'id': '/subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/mmai-swc-hub01-grp/providers/Microsoft.MachineLearningServices/workspaces/mmai-swc-hub01-prj01/data/902bf50f-c87c-4320-ab89-9bf64be8be3b/versions/1'}, 'evaluators': {'f1_score': {'id': 'azureml://registries/azureml/models/F1Score-Evaluator/versions/3'}, 'relevance': {'id': 'azureml://registries/azureml/models/Relevance-Evaluator/versions/4', 'initParams': {'model_config': {'azure_endpoint': 'https://mmai-swc-hub01-oais581696736083.openai.azure.com/', 'api_key': '9W7MYkTJhnsTiY4eSyH8zFlol3SEoj7hbYUSyXkJuvIcpUBCvwQnJQQJ99BDACfhMk5XJ3w3AAAAACOGKaCA', 'azure_deployment': 'gpt-4.1', 'api_version': '2025-03-01-preview'}}}, 'violence': {'id': 'azureml://registries/azureml/models/Violent-Content-Evaluator/versions/3', 'initParams': {'azure_ai_project': {'subscription_id': 'eca2eddb-0f0c-4351-a634-52751499eeea', 'resource_group_name': 'mmai-swc-hub01-grp', 'project_name': 'mmai-swc-hub01-prj01'}}}, 'friendliness': {'id': 'azureml://subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/mmai-swc-hub01-grp/workspaces/mmai-swc-hub01-prj01/datastores/workspaceblobstore/paths/LocalUpload/e8ccca8c8f07c002cc62dce8635918f0/friendliness_local', 'initParams': {'model_config': {'azure_endpoint': 'https://mmai-swc-hub01-oais581696736083.openai.azure.com/', 'api_key': '9W7MYkTJhnsTiY4eSyH8zFlol3SEoj7hbYUSyXkJuvIcpUBCvwQnJQQJ99BDACfhMk5XJ3w3AAAAACOGKaCA', 'azure_deployment': 'gpt-4.1', 'api_version': '2025-03-01-preview'}}}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.projects.models import Evaluation, Dataset, EvaluatorConfiguration\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Cloud evaluation\",\n",
    "    description=\"Evaluation of dataset\",\n",
    "    data=Dataset(id=data_id),\n",
    "    evaluators={\n",
    "        # Note the evaluator configuration key must follow a naming convention\n",
    "        # the string must start with a letter with only alphanumeric characters \n",
    "        # and underscores. Take \"f1_score\" as example: \"f1score\" or \"f1_evaluator\" \n",
    "        # will also be acceptable, but \"f1-score-eval\" or \"1score\" will result in errors.\n",
    "        \"f1_score\": EvaluatorConfiguration(\n",
    "            id=F1ScoreEvaluator.id,\n",
    "        ),\n",
    "        \"relevance\": EvaluatorConfiguration(\n",
    "            id=RelevanceEvaluator.id,\n",
    "            init_params={\n",
    "                \"model_config\": model_config\n",
    "            },\n",
    "        ),\n",
    "        \"violence\": EvaluatorConfiguration(\n",
    "            id=ViolenceEvaluator.id,\n",
    "            init_params={\n",
    "                \"azure_ai_project\": project_client.scope\n",
    "            },\n",
    "        ),\n",
    "        \"friendliness\": EvaluatorConfiguration(\n",
    "            id=registered_evaluator.path,\n",
    "            init_params={\n",
    "                \"model_config\": model_config\n",
    "            }\n",
    "        )\n",
    "    },\n",
    ")\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0648368-4f24-4fd7-820c-0b08eb72bd00",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(UserError) Evaluation evaluator ID invalid asset or open AI grader format. Provided evaluator id is azureml://subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/mmai-swc-hub01-grp/workspaces/mmai-swc-hub01-prj01/datastores/workspaceblobstore/paths/LocalUpload/e8ccca8c8f07c002cc62dce8635918f0/friendliness_local is invalid\nCode: UserError\nMessage: Evaluation evaluator ID invalid asset or open AI grader format. Provided evaluator id is azureml://subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/mmai-swc-hub01-grp/workspaces/mmai-swc-hub01-prj01/datastores/workspaceblobstore/paths/LocalUpload/e8ccca8c8f07c002cc62dce8635918f0/friendliness_local is invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHttpResponseError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create evaluation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m evaluation_response = \u001b[43mproject_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:138\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes.items():\n\u001b[32m    137\u001b[39m                 span.add_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[32m    141\u001b[39m     config = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\genai_evaluation\\Lib\\site-packages\\azure\\ai\\projects\\operations\\_operations.py:7342\u001b[39m, in \u001b[36mEvaluationsOperations.create\u001b[39m\u001b[34m(self, evaluation, **kwargs)\u001b[39m\n\u001b[32m   7340\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   7341\u001b[39m     map_error(status_code=response.status_code, response=response, error_map=error_map)\n\u001b[32m-> \u001b[39m\u001b[32m7342\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response=response)\n\u001b[32m   7344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _stream:\n\u001b[32m   7345\u001b[39m     deserialized = response.iter_bytes()\n",
      "\u001b[31mHttpResponseError\u001b[39m: (UserError) Evaluation evaluator ID invalid asset or open AI grader format. Provided evaluator id is azureml://subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/mmai-swc-hub01-grp/workspaces/mmai-swc-hub01-prj01/datastores/workspaceblobstore/paths/LocalUpload/e8ccca8c8f07c002cc62dce8635918f0/friendliness_local is invalid\nCode: UserError\nMessage: Evaluation evaluator ID invalid asset or open AI grader format. Provided evaluator id is azureml://subscriptions/eca2eddb-0f0c-4351-a634-52751499eeea/resourceGroups/mmai-swc-hub01-grp/workspaces/mmai-swc-hub01-prj01/datastores/workspaceblobstore/paths/LocalUpload/e8ccca8c8f07c002cc62dce8635918f0/friendliness_local is invalid"
     ]
    }
   ],
   "source": [
    "# Create evaluation\n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation=evaluation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b19814d-f029-4b75-bf4f-766435e3e082",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get evaluation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m evaluation_response = project_client.evaluations.get(\u001b[43mevaluation_response\u001b[49m.id)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m----------------------------------------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreated evaluation, evaluation ID: \u001b[39m\u001b[33m\"\u001b[39m, get_evaluation_response.id)\n",
      "\u001b[31mNameError\u001b[39m: name 'evaluation_response' is not defined"
     ]
    }
   ],
   "source": [
    "# Get evaluation\n",
    "evaluation_response = project_client.evaluations.get(evaluation_response.id)\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Created evaluation, evaluation ID: \", get_evaluation_response.id)\n",
    "print(\"Evaluation status: \", get_evaluation_response.status)\n",
    "print(\"AI project URI: \", get_evaluation_response.properties[\"AiStudioEvaluationUri\"])\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80fd894-6586-4c0d-a4e7-eba2c8184ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative AI evaluation",
   "language": "python",
   "name": "genai_evaluation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
