[
{
"metric_name": "Intent Resolution",
"metric_definition": "Intent Resolution assesses the quality of the response given in relation to a query from a user, specifically focusing on the agent’s ability to understand and resolve the user intent expressed in the query. There's also a field for tool definitions describing the functions, if any, that are accessible to the agent and that the agent might invoke in the response if necessary","metric_features": "Score range 1: to 5 where 1 is the lowest quality and 5 is the highest quality.What is this metric? Intent Resolution measures how well an agent identifies a user’s request, including how well it scopes the user’s intent, asks clarifying questions, and reminds end users of its scope of capabilities. How does it work? The metric is calculated by instructing a language model to follow the definition (in the description) and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). See the following definition and grading rubric.When to use it? The recommended scenario is evaluating agent’s ability to identify user intents from agent interactions. What does it need as input? Query, Response, Tool Definitions (optional)"
},
{
"metric_name": "Tool Call Accuracy",
"metric_definition": "Tool Call Accuracy returns the correctness of a single tool call, or the passing rate of the correct tool calls among multiple ones. A correct tool call considers relevance and potential usefulness, including syntactic and semantic correctness of a proposed tool call from an intelligent system. The judgment for each tool call is based on the following provided criteria, user query, and the tool definitions available to the agent.",
"metric_features": "Score range 1 to 5 where 1 is the lowest quality and 5 is the highest quality. What is this metric? Tool Call Accuracy measures an agent’s ability to select appropriate tools, extract, and process correct parameters from previous steps of the agentic workflow. It detects whether each tool call made is accurate (binary) and reports back the average scores, which can be interpreted as a passing rate across tool calls made. How does it work? The metric is calculated by instructing a language model to follow the definition (in the description) and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). See the following definition and grading rubric. When to use it? The recommended scenario is evaluating agent’s ability to select the right tools and parameters from agentic interactions. What does it need as input? Query, Response, or Tool Calls, Tool Definitions"
},
{
"metric_name": "Task Adherence",
"metric_definition": "Task Adherence assesses the quality of the response given in relation to a query from a user, specifically focusing on the agent’s ability to understand and resolve the user intent expressed in the query. There's also a field for tool definitions describing the functions, if any, that are accessible to the agent and that the agent might invoke in the response if necessary.",
"metric_features": "Score range 1 to 5 where 1 is the lowest quality and 5 is the highest quality. What is this metric? Task Adherence measures how well an agent’s response adheres to their assigned tasks, according to their task instruction (extracted from system message and user query), and available tools. How does it work? The metric is calculated by instructing a language model to follow the definition (in the description) and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). See the following definition and grading rubric. When to use it? The recommended scenario is evaluating agent’s ability to adhere to assigned tasks. What does it need as input? Query, Response, Tool Definitions (optional)"
},
{
"metric_name": "Response Completeness",
"metric_definition": "Response Completeness refers to how accurately and thoroughly a response represents the information provided in the ground truth. It considers both the inclusion of all relevant statements and the correctness of those statements. Each statement in the ground truth should be evaluated individually to determine if it is accurately reflected in the response",
"metric_features": "Score range 1 to 5 where 1 is the lowest quality and 5 is the highest quality. What is this metric? Response Completeness measures how comprehensive an agent’s response is when compared with the ground truth provided. How does it work? The metric is calculated by instructing a language model to follow the definition (in the description) and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). See the following definition and grading rubric. When to use it? The recommended scenario is evaluating agent’s final response to be comprehensive with respect to the ground truth provided. What does it need as input? Response, Ground Truth."
},
{
"metric_name": "Groundedness (prompt-based)",
"metric_definition": "Prompt-based groundedness using your own model deployment to output a score and an explanation for the score is currently supported in all regions.",
"metric_features": "Score range 1 to 5 where 1 is the lowest quality and 5 is the highest quality. What is this metric? Groundedness measures how well the generated response aligns with the given context in a retrieval-augmented generation scenario, focusing on its relevance and accuracy with respect to the context. If a query is present in the input, the recommended scenario is question and answering. Otherwise, the recommended scenario is summarization. How does it work? The groundedness metric is calculated by instructing a language model to follow the definition and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). See the following definition and grading rubrics. When to use it? The recommended scenario is retrieval-augmented generation (RAG) scenarios, including question and answering and summarization. Use the groundedness metric when you need to verify that AI-generated responses align with and are validated by the provided context. It's essential for applications where contextual accuracy is key, like information retrieval, question and answering, and summarization. This metric ensures that the AI-generated answers are well-supported by the context. What does it need as input? Query (optional), Context, Response."
},
{
"metric_name": "Groundedness Pro",
"metric_definition": "Groundedness Pro evaluator leverages Azure AI Content Safety Service (AACS) via integration into the Azure AI Foundry evaluations. No deployment is required, as a back-end service provides the models for you to output a score and reasoning. Groundedness Pro is currently supported in the East US 2 and Sweden Central regions. Prompt-based groundedness using your own model deployment to output a score and an explanation for the score is currently supported in all regions.",
"metric_features": "Score range 1 to 5 where 1 is the lowest quality and 5 is the highest quality. What is this metric? It detects whether the generated text response is consistent or accurate with respect to the given context in a retrieval-augmented generation question and answering scenario. It checks whether the response adheres closely to the context in order to answer the query, avoiding speculation or fabrication, and outputs a true/false label. How does it work? It leverages an Azure AI Content Safety Service custom language model fine-tuned to a natural language processing task called Natural Language Inference (NLI), which evaluates claims in response to a query as being entailed or not entailed by the given context. When to use it? The recommended scenario is retrieval-augmented generation question and answering (RAG QA). Use the Groundedness Pro metric when you need to verify that AI-generated responses align with and are validated by the provided context. It's essential for applications where contextual accuracy is key, like information retrieval and question and answering. This metric ensures that the AI-generated answers are well-supported by the context.. What does it need as input? Question, Context, Response."
},
{
"metric_name": "Retrieval",
"metric_definition": "Retrieval refers to measuring how relevant the context chunks are to address a query and how the most relevant context chunks are surfaced at the top of the list. It emphasizes the extraction and ranking of the most relevant information at the top, without introducing bias from external knowledge and ignoring factual correctness. It assesses the relevance and effectiveness of the retrieved context chunks with respect to the query.",
"metric_features": "Score range 1 to 5 where 1 is the lowest quality and 5 is the highest quality. What is this metric? Retrieval measures the quality of search without ground truth. It focuses on how relevant the context chunks (encoded as a string) are to address a query and how the most relevant context chunks are surfaced at the top of the list. How does it work? The retrieval metric is calculated by instructing a language model to follow the definition (in the description) and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). See the following definition and grading rubrics. When to use it? The recommended scenario is the quality of search in information retrieval and retrieval augmented generation, when you don't have ground truth for chunk retrieval rankings. Use the retrieval score when you want to assess to what extent the context chunks retrieved are highly relevant and ranked at the top for answering your users' queries. What does it need as input? Query, Context."
},
{
"metric_name": "Relevance",
"metric_definition": "Relevance refers to how effectively a response addresses a question. It assesses the accuracy, completeness, and direct relevance of the response based solely on the given information.",
"metric_features": "Score range 1 to 5 where 1 is the lowest quality and 5 is the highest quality. What is this metric? The relevance metric is calculated by instructing a language model to follow the definition (in the description) and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). See the following definition and grading rubric. How does it work? The metric is calculated by instructing a language model to follow the definition (in the description) and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). See the following definition and grading rubric. When to use it? The recommended scenario is evaluating the quality of responses in question and answering, without reference to any context. Use the metric when you want to understand the overall quality of responses when context isn't available. What does it need as input? Query, Response."
},
{
"metric_name": "Coherence",
"metric_definition": "Coherence refers to the logical and orderly presentation of ideas in a response, allowing the reader to easily follow and understand the writer's train of thought. A coherent answer directly addresses the question with clear connections between sentences and paragraphs, using appropriate transitions and a logical sequence of ideas",
"metric_features": "Score range 1 to 5 where 1 is the lowest quality and 5 is the highest quality. What is this metric? Coherence measures the logical and orderly presentation of ideas in a response, allowing the reader to easily follow and understand the writer's train of thought. A coherent response directly addresses the question with clear connections between sentences and paragraphs, using appropriate transitions and a logical sequence of ideas. How does it work? The coherence metric is calculated by instructing a language model to follow the definition (in the description) and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). See the following definition and grading rubrics. When to use it? The recommended scenario is generative business writing such as summarizing meeting notes, creating marketing materials, and drafting email. What does it need as input? Query, Response."
},
{
"metric_name": "Fluency",
"metric_definition": "Fluency refers to the effectiveness and clarity of written communication, focusing on grammatical accuracy, vocabulary range, sentence complexity, coherence, and overall readability. It assesses how smoothly ideas are conveyed and how easily the text can be understood by the reader",
"metric_features": "Score range 1 to 5 where 1 is the lowest quality and 5 is the highest quality. What is this metric? Fluency measures the effectiveness and clarity of written communication, focusing on grammatical accuracy, vocabulary range, sentence complexity, coherence, and overall readability. It assesses how smoothly ideas are conveyed and how easily the text can be understood by the reader. How does it work? The fluency metric is calculated by instructing a language model to follow the definition (in the description) and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). See the following definition and grading rubrics. When to use it? The recommended scenario is generative business writing such as summarizing meeting notes, creating marketing materials, and drafting email. What does it need as input? Response."
},
{
"metric_name": "Similarity",
"metric_definition": "Similarity measures the degrees of similarity between the generated text and its ground truth with respect to a query.",
"metric_features": "Score range 1 to 5 where 1 is the lowest quality and 5 is the highest quality. What is this metric? Similarity measures the degrees of similarity between the generated text and its ground truth with respect to a query. How does it work? The similarity metric is calculated by instructing a language model to follow the definition (in the description) and a set of grading rubrics, evaluate the user inputs, and output a score on a 5-point scale (higher means better quality). See the following definition and grading rubrics. When to use it? The recommended scenario is NLP tasks with a user query. Use it when you want an objective evaluation of an AI model's performance, particularly in text generation tasks where you have access to ground truth responses. Similarity enables you to assess the generated text's semantic alignment with the desired content, helping to gauge the model's quality and accuracy. What does it need as input? Query, Response."
},
{
"metric_name": "F1 Score",
"metric_definition": "Traditional Machine Learning metric",
"metric_features": "What is this metric? F1 score measures the similarity by shared tokens between the generated text and the ground truth, focusing on both precision and recall.How does it work? The F1-score computes the ratio of the number of shared words between the model generation and the ground truth. Ratio is computed over the individual words in the generated response against those in the ground truth answer. The number of shared words between the generation and the truth is the basis of the F1 score: precision is the ratio of the number of shared words to the total number of words in the generation, and recall is the ratio of the number of shared words to the total number of words in the ground truth. When to use it? The recommended scenario is Natural Language Processing (NLP) tasks. Use the F1 score when you want a single comprehensive metric that combines both recall and precision in your model's responses. It provides a balanced evaluation of your model's performance in terms of capturing accurate information in the response. What does it need as input? Response, Ground Truth"
},
{
"metric_name": "BLEU Score",
"metric_definition": "Traditional Machine Learning metric",
"metric_features": "Score range Float [0-1] (higher means better quality) What is this metric? BLEU (Bilingual Evaluation Understudy) score is commonly used in natural language processing (NLP) and machine translation. It measures how closely the generated text matches the reference text. When to use it? The recommended scenario is Natural Language Processing (NLP) tasks. It's widely used in text summarization and text generation use cases. What does it need as input? Response, Ground Truth"
},
{
"metric_name": "ROUGE Score",
"metric_definition": "Traditional Machine Learning metric",
"metric_features": "Score range Float [0-1] (higher means better quality) What is this metric? ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate automatic summarization and machine translation. It measures the overlap between generated text and reference summaries. ROUGE focuses on recall-oriented measures to assess how well the generated text covers the reference text. The ROUGE score is composed of precision, recall, and F1 score. When to use it? The recommended scenario is Natural Language Processing (NLP) tasks. Text summarization and document comparison are among the recommended use cases for ROUGE, particularly in scenarios where text coherence and relevance are critical. What does it need as input? Response, Ground Truth"
},
{
"metric_name": "METEOR Score",
"metric_definition": "Traditional Machine Learning metric",
"metric_features": "Score range Float [0-1] (higher means better quality) What is this metric? METEOR score measures the similarity by shared n-grams between the generated text and the ground truth, similar to the BLEU score, focusing on precision and recall. But it addresses limitations of other metrics like the BLEU score by considering synonyms, stemming, and paraphrasing for content alignment. When to use it? The recommended scenario is Natural Language Processing (NLP) tasks. It addresses limitations of other metrics like BLEU by considering synonyms, stemming, and paraphrasing. METEOR score considers synonyms and word stems to more accurately capture meaning and language variations. In addition to machine translation and text summarization, paraphrase detection is a recommended use case for the METEOR score. What does it need as input? Response, Ground Truth"
}
]