{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c479b344-f6ae-454d-a96c-85f9137f16dc",
   "metadata": {},
   "source": [
    "# [Cloud Evaluation](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/cloud-evaluation#cloud-evaluation-preview-with-azure-ai-projects-sdk)\n",
    "While Azure AI Evaluation SDK client supports running evaluations locally on your own machine, you might want to delegate the job remotely to the cloud. For example, after you ran local evaluations on small test data to help assess your generative AI application prototypes, now you move into pre-deployment testing and need run evaluations on a large dataset. Cloud evaluation frees you from managing your local compute infrastructure, and enables you to integrate evaluations as tests into your CI/CD pipelines. After deployment, you might want to continuously evaluate your applications for post-deployment monitoring.\n",
    "\n",
    "In this article, you learn how to run cloud evaluation (preview) in pre-deployment testing on a test dataset. Using the Azure AI Projects SDK, you'll have evaluation results automatically logged into your Azure AI project for better observability. This feature supports all Microsoft curated built-in evaluators and your own custom evaluators which can be located in the Evaluator library and have the same project-scope RBAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2d673-f357-4d0f-b8e5-729a34fce542",
   "metadata": {},
   "source": [
    "## Environment prepration\n",
    "\n",
    "Needed libararies: `pip install azure-identity azure-ai-projects azure-ai-ml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f1ba86-e977-4636-8887-04394b833f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8ac0ff-c633-4603-b3bf-c63ef79b1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Libraries\n",
    "import os, json\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider #requires azure-identity\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv # requires python-dotenv\n",
    "\n",
    "if not load_dotenv(\"./../../config/credentials_my.env\"):\n",
    "    print(\"Environment variables not loaded, cell execution stopped\")\n",
    "    sys.exit()\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = os.environ[\"OPENAI_API_VERSION\"]\n",
    "\n",
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4edd78d8-45bd-46ec-aca1-26f28afa8b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI connection\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"MODEL_DEPLOYMENT_NAME\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9aaf73-250e-4d6c-9876-98d3a47f22e9",
   "metadata": {},
   "source": [
    "## [Create an Azure AI Client from a connection string](https://learn.microsoft.com/en-us/azure/ai-services/agents/quickstart?pivots=programming-language-python-azure). \n",
    "- available on Azure AI project Overview page.\n",
    "- format: `<HostName>;<AzureSubscriptionId>;<ResourceGroup>;<ProjectName>`\n",
    "- command: `az ml workspace show -n mmai-swc-hub01-prj01 --resource-group mmai-swc-hub01-grp --query discovery_url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128534f-3964-4fc4-9a25-56eb0fc9d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new version, to be fixed\n",
    "\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "project_client = AIProjectClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    endpoint=os.environ[\"PROJECT_ENDPOINT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951a94cf-1764-4841-a668-3ff358cced3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AIProjectClient.__init__() missing 1 required positional argument: 'endpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprojects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AIProjectClient\n\u001b[32m      3\u001b[39m azure_ai_project_config = {\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msubscription_id\u001b[39m\u001b[33m\"\u001b[39m: os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mAZURE_SUBSCRIPTION_ID\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresource_group_name\u001b[39m\u001b[33m\"\u001b[39m: os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mAZURE_HUB_PROJECTS_GROUP_NAME\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproject_name\u001b[39m\u001b[33m\"\u001b[39m: os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mAZURE_HUB_PROJECT_NAME\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      7\u001b[39m }\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m project_client = \u001b[43mAIProjectClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mazure_ai_project_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredential\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDefaultAzureCredential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m project_client = AIProjectClient.from_connection_string(\n\u001b[32m     15\u001b[39m     credential=DefaultAzureCredential(),\n\u001b[32m     16\u001b[39m     conn_str=os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPROJECT_CONNECTION_STRING\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: AIProjectClient.__init__() missing 1 required positional argument: 'endpoint'"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbcf8e7-6a97-4e04-b77b-5031cfd49f67",
   "metadata": {},
   "source": [
    "## Uploading evaluation data\n",
    "We provide two ways to register your data in Azure AI project required for evaluations in the cloud:\n",
    "- From SDK: Upload new data from your local directory to your Azure AI project in the SDK, and fetch the dataset ID as a result\n",
    "- Given existing datasets uploaded to your Project..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0393656-b27d-4d78-a3d0-8262e1625a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id, _ = project_client.upload_file(\"./synthetic_dataset_cloud.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d0bbb-4713-4c0c-86fd-f1c462e46587",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0110f2-7654-4b74-8f91-8af4f4e9d46a",
   "metadata": {},
   "source": [
    "## Specifying built-in evaluators from Evaluator library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feab645-45b6-43a5-b3f5-a525965715f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import F1ScoreEvaluator, RelevanceEvaluator, ViolenceEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c4818e-c791-4114-9dee-1417fa9f5ad3",
   "metadata": {},
   "source": [
    "## Specifying custom evaluators\n",
    "Note: they must be already registered as done in 2.4 Custom Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caacc18-2bf1-448b-81bb-e843a4d0af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ml_client to register custom evaluator\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "ml_client = MLClient(\n",
    "       subscription_id=os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "       resource_group_name=os.environ[\"RESOURCE_GROUP_NAME\"],\n",
    "       workspace_name=os.environ[\"PROJECT_NAME\"],\n",
    "       credential=credential\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee10ad8-c6f5-4533-b477-5c263a2a3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify evaluator name as it appears in the Evaluator library\n",
    "evaluator_name = \"FriendlinessEvaluator\"\n",
    "registered_evaluator = ml_client.evaluators.get(evaluator_name, version=9)\n",
    "print(\"Registered evaluator:\", registered_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e5890-585d-4561-b173-a43ceb40fa9e",
   "metadata": {},
   "source": [
    "## Create the evaluators list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa76b842-fadb-4938-b95a-097a3a661515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import EvaluatorConfiguration\n",
    "\n",
    "evaluators={\n",
    "    \"f1_score\": EvaluatorConfiguration(\n",
    "        id=F1ScoreEvaluator.id,\n",
    "    ),\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=RelevanceEvaluator.id,\n",
    "        init_params={\n",
    "            \"model_config\": model_config\n",
    "        },\n",
    "    ),\n",
    "    \"violence\": EvaluatorConfiguration(\n",
    "        id=ViolenceEvaluator.id,\n",
    "        init_params={\n",
    "            \"azure_ai_project\": project_client.scope\n",
    "        },\n",
    "    ),\n",
    "    \"friendliness\": EvaluatorConfiguration(\n",
    "        id=registered_evaluator.path,\n",
    "        init_params={\n",
    "            \"model_config\": model_config\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "# evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3db3a-040b-43b9-ac6a-08d71febd73e",
   "metadata": {},
   "source": [
    "## Create the evaluation object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a514b0-bb39-4845-8431-cff66e583917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import Evaluation, Dataset, EvaluatorConfiguration\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Cloud evaluation\",\n",
    "    description=\"Evaluation of dataset\",\n",
    "    data=Dataset(id=data_id),\n",
    "    evaluators=evaluators,\n",
    ")\n",
    "\n",
    "# evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba826a3a-d049-4a7e-9454-ee880c6330cb",
   "metadata": {},
   "source": [
    "## Create the evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0648368-4f24-4fd7-820c-0b08eb72bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation\n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation=evaluation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b19814d-f029-4b75-bf4f-766435e3e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation\n",
    "evaluation_response = project_client.evaluations.get(evaluation_response.id)\n",
    "\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Created evaluation, evaluation ID: \", evaluation_response.id)\n",
    "print(\"Evaluation status: \", evaluation_response.status)\n",
    "print(\"AI project URI: \", evaluation_response.properties[\"AiStudioEvaluationUri\"])\n",
    "print(\"----------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative AI evaluation",
   "language": "python",
   "name": "genai_evaluation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
